{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "\n",
    "import numpy as np\n",
    "import tqdm\n",
    "from torch import nn, optim\n",
    "import sys\n",
    "sys.path.append('/home/anuj/Desktop/Work/TU_Delft/research/implement/learning_to_meta-learn')\n",
    "sys.path\n",
    "\n",
    "from src.zoo.delpo_utils import setup, inner_adapt_delpo\n",
    "from src.utils2 import Profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--dataset DATASET] [--root ROOT]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: --ip=127.0.0.1 --stdin=9013 --control=9011 --hb=9010 --Session.signature_scheme=\"hmac-sha256\" --Session.key=b\"716c0b2c-0527-431d-876f-d1f2057d9694\" --shell=9012 --transport=\"tcp\" --iopub=9014 --f=/tmp/tmp-16469fl3VhWWaQO43.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    }
   ],
   "source": [
    "\n",
    "##############\n",
    "# Parameters #\n",
    "##############\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--dataset', type=str, default='mini_imagenet')\n",
    "parser.add_argument('--root', type=str, default='../../data')\n",
    "# parser.add_argument('--n-ways', type=int)\n",
    "# parser.add_argument('--k-shots', type=int)\n",
    "# parser.add_argument('--q-shots', type=int)\n",
    "# parser.add_argument('--inner-adapt-steps-train', type=int)\n",
    "# parser.add_argument('--inner-adapt-steps-test', type=int)\n",
    "# parser.add_argument('--inner-lr', type=float)\n",
    "# parser.add_argument('--meta-lr', type=float)\n",
    "# parser.add_argument('--meta-batch-size', type=int)\n",
    "# parser.add_argument('--iterations', type=int)\n",
    "# parser.add_argument('--order', type=str)\n",
    "# parser.add_argument('--device', type=str)\n",
    "# parser.add_argument('--download', type=str)\n",
    "\n",
    "args = parser.parse_args()\n",
    "# if args.order == 'True':\n",
    "#     args.order = True\n",
    "# elif args.order == 'False':\n",
    "#     args.order = False\n",
    "\n",
    "# if args.download == 'True':\n",
    "#     args.download = True\n",
    "# elif args.download == 'False':\n",
    "#     args.download = False\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'1': 'a', '2': 'b', '3': 'c', '4': 'd', 'abc': 1}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_dict = {1:'a', 2:'b', 3:'c', 4:'d'}\n",
    "\n",
    "a = dict({f'{k}': j for _, (k,j) in enumerate(example_dict.items())}, **{'abc': 1})\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = bool(10%5==0)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Generating Tasks, initializing learners, loss, meta - optimizer\n",
    "train_tasks, valid_tasks, test_tasks, learner = setup(\n",
    "    args.dataset, args.root, args.n_ways, args.k_shots, args.q_shots, args.order, args.inner_lr, args.device, download=args.download)\n",
    "opt = optim.Adam(learner.parameters(), args.meta_lr)\n",
    "reconst_loss = nn.MSELoss(reduction='none')\n",
    "if args.order == False:\n",
    "    profiler = Profiler('DELPO_{}_{}-way_{}-shot_{}-queries'.format(args.dataset,\n",
    "                        args.n_ways, args.k_shots, args.q_shots))\n",
    "    prof_test = Profiler('DELPO_test_{}_{}-way_{}-shot_{}-queries'.format(\n",
    "        args.dataset, args.n_ways, args.k_shots, args.q_shots))\n",
    "elif args.order == True:\n",
    "    profiler = Profiler('FO-DELPO_{}_{}-way_{}-shot_{}-queries'.format(\n",
    "        args.dataset, args.n_ways, args.k_shots, args.q_shots))\n",
    "    prof_test = Profiler('FO-DELPO_test_{}_{}-way_{}-shot_{}-queries'.format(\n",
    "        args.dataset, args.n_ways, args.k_shots, args.q_shots))\n",
    "\n",
    "\n",
    "## Training ##\n",
    "for iter in tqdm.tqdm(range(args.iterations)):\n",
    "    opt.zero_grad()\n",
    "    meta_train_losses = []\n",
    "    meta_valid_losses = []\n",
    "    meta_train_acc = []\n",
    "    meta_valid_acc = []\n",
    "\n",
    "    for batch in range(args.meta_batch_size):\n",
    "        ttask = train_tasks.sample()\n",
    "        model = learner.clone()\n",
    "        evaluation_loss, evaluation_accuracy = inner_adapt_delpo(\n",
    "            ttask, reconst_loss, model, args.n_ways, args.k_shots, args.q_shots, args.inner_adapt_steps_train, args.device)\n",
    "        evaluation_loss[0].backward()\n",
    "        meta_train_losses.append([l.item() for l in evaluation_loss])\n",
    "        meta_train_acc.append(evaluation_accuracy.item())\n",
    "\n",
    "    vtask = valid_tasks.sample()\n",
    "    model = learner.clone()\n",
    "    validation_loss, validation_accuracy = inner_adapt_delpo(\n",
    "        vtask, reconst_loss, model, args.n_ways, args.k_shots, args.q_shots, args.inner_adapt_steps_train, args.device)\n",
    "    meta_valid_losses.append([l.item() for l in validation_loss])\n",
    "    meta_valid_acc.append(validation_accuracy.item())\n",
    "\n",
    "    #Logging\n",
    "    meta_train_losses = np.hstack(meta_train_losses)\n",
    "    meta_valid_losses = np.hstack(meta_valid_losses)\n",
    "    meta_train_acc = np.array(meta_train_acc)\n",
    "    meta_valid_acc = np.array(meta_valid_acc)\n",
    "    profiler.log([meta_train_acc.mean(), meta_train_losses[::5].mean(), meta_train_losses[1::5].mean(),\n",
    "                  meta_train_losses[2::5].mean(), meta_train_losses[3::5].mean(\n",
    "    ), meta_train_losses[4::5].mean(),\n",
    "        meta_train_acc.std(), meta_train_losses[::5].std(\n",
    "    ), meta_train_losses[1::5].std(),\n",
    "        meta_train_losses[2::5].std(), meta_train_losses[3::5].std(\n",
    "    ), meta_train_losses[4::5].std(),\n",
    "        meta_valid_acc.mean(), meta_valid_losses[::5].mean(\n",
    "    ), meta_valid_losses[1::5].mean(),\n",
    "        meta_valid_losses[2::5].mean(), meta_valid_losses[3::5].mean(), meta_valid_losses[4::5].mean()])\n",
    "\n",
    "    # if (iter % 500 == 0):\n",
    "    #     print('Meta Train Accuracy: {:.4f} +- {:.4f}'.format(\n",
    "    #         np.array(meta_train_acc).mean(), np.array(meta_train_acc).std()))\n",
    "    #     print('Meta Valid Accuracy: {:.4f} +- {:.4f}'.format(\n",
    "    #         np.array(meta_valid_acc).mean(), np.array(meta_valid_acc).std()))\n",
    "\n",
    "    for p in learner.parameters():\n",
    "        p.grad.data.mul_(1.0 / args.meta_batch_size)\n",
    "    opt.step()\n",
    "\n",
    "#torch.save(learner, f='../repro')\n",
    "\n",
    "## Testing ##\n",
    "print('Testing on held out classes')\n",
    "\n",
    "for i, tetask in enumerate(test_tasks):\n",
    "    meta_test_acc = []\n",
    "    meta_test_losses = []\n",
    "    model = learner.clone()\n",
    "    #tetask = test_tasks.sample()\n",
    "    evaluation_loss, evaluation_accuracy = inner_adapt_delpo(\n",
    "        tetask, reconst_loss, model, args.n_ways, args.k_shots, args.q_shots, args.inner_adapt_steps_test, args.device)\n",
    "    meta_test_losses.append([l.item() for l in evaluation_loss])\n",
    "    meta_test_acc.append(evaluation_accuracy.item())\n",
    "\n",
    "    # Logging\n",
    "    meta_test_losses = np.hstack(meta_test_losses)\n",
    "    meta_test_acc = np.array(meta_test_acc)\n",
    "    prof_test.log(row=[meta_test_acc.mean(), meta_test_losses[::5].mean(), meta_test_losses[1::5].mean(),\n",
    "                       meta_test_losses[2::5].mean(), meta_test_losses[3::5].mean(), meta_test_losses[4::5].mean(),\n",
    "                        meta_test_acc.std(), meta_test_losses[::5].std(), meta_test_losses[1::5].std(),\n",
    "                        meta_test_losses[2::5].std(), meta_test_losses[3::5].std(), meta_test_losses[4::5].std()])\n",
    "    # print('Meta Test Accuracy', np.array(meta_test_acc).mean(),\n",
    "    #       '+-', np.array(meta_test_acc).std())\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7e3af266dcb7df8b026f0780dbb396b062ee5ca2767a18f50e60e26ee6084121"
  },
  "kernelspec": {
   "display_name": "Python 3.8.11 64-bit ('torch': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
