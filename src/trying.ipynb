{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import learn2learn as l2l\n",
    "import pandas as pd\n",
    "\n",
    "from torch import nn, optim\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "from PIL.Image import LANCZOS\n",
    "from config import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('/home/anuj/Desktop/Work/TU_Delft/research/implement/learning_to_meta-learn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['/home/anuj/Desktop/Work/TU_Delft/research/implement/learning_to_meta-learn/src',\n",
       " '/home/anuj/.vscode/extensions/ms-toolsai.jupyter-2021.6.832593372/pythonFiles',\n",
       " '/home/anuj/.vscode/extensions/ms-toolsai.jupyter-2021.6.832593372/pythonFiles',\n",
       " '/home/anuj/.vscode/extensions/ms-toolsai.jupyter-2021.6.832593372/pythonFiles/lib/python',\n",
       " '/home/anuj/anaconda3/envs/torch/lib/python38.zip',\n",
       " '/home/anuj/anaconda3/envs/torch/lib/python3.8',\n",
       " '/home/anuj/anaconda3/envs/torch/lib/python3.8/lib-dynload',\n",
       " '',\n",
       " '/home/anuj/anaconda3/envs/torch/lib/python3.8/site-packages',\n",
       " '/home/anuj/anaconda3/envs/torch/lib/python3.8/site-packages/datasets-1.2.1-py3.8.egg',\n",
       " '/home/anuj/anaconda3/envs/torch/lib/python3.8/site-packages/locket-0.2.1-py3.8.egg',\n",
       " '/home/anuj/anaconda3/envs/torch/lib/python3.8/site-packages/IPython/extensions',\n",
       " '/home/anuj/.ipython',\n",
       " '/home/anuj/Desktop/Work/TU_Delft/research/implement/learning_to_meta-learn']"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "sys.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data.loaders import Omniglotmix, MiniImageNet\n",
    "from data.taskers import gen_tasks\n",
    "from src.zoo.archs import EncoderNN, MatchingNetwork\n",
    "from src.zoo.matching_nets_utils import setup, logits, accuracy, inner_adapt_matching\n",
    "#from src.utils import Profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/home/anuj/anaconda3/envs/torch/lib/python3.8/site-packages/torchvision/transforms/transforms.py:257: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "classes = list(range(1623))\n",
    "random.shuffle(classes)\n",
    "image_transforms = transforms.Compose([transforms.Resize(28, interpolation=LANCZOS),\n",
    "                                                    transforms.ToTensor(),\n",
    "                                                    lambda x: 1.0 - x,\n",
    "                                                ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tasks = gen_tasks('omniglot', '/home/anuj/Desktop/Work/TU_Delft/research/implement/omniglot', image_transforms=image_transforms, n_ways=5, k_shots=1, q_shots=15, classes=classes[:1100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([80, 1, 28, 28])"
      ]
     },
     "metadata": {},
     "execution_count": 61
    }
   ],
   "source": [
    "train_tasks.sample()[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "task = train_tasks.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tasks = gen_tasks(dataname='miniimagenet', root='../../mini_imagenet', mode='train', n_ways=5, k_shots=1, q_shots=15, image_transforms=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1, labels1 = train_tasks.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([80, 1, 28, 28])"
      ]
     },
     "metadata": {},
     "execution_count": 63
    }
   ],
   "source": [
    "data1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/home/anuj/anaconda3/envs/torch/lib/python3.8/site-packages/torchvision/transforms/transforms.py:257: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "train_tasks, valid_tasks, test_tasks, learner = setup(\n",
    "    'omniglot', '/home/anuj/Desktop/Work/TU_Delft/research/implement/omniglot', 5, 1, 15, 5, 1, 15, 1, 2, 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "ttask = train_tasks.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "ValueError",
     "evalue": "Expected input batch_size (5) to match target batch_size (75).",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-5fb9e3b7f250>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minner_adapt_matching\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mttask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNLLLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearner\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m15\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1e-8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'cpu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Desktop/Work/TU_Delft/research/implement/learning_to_meta-learn/src/zoo/matching_nets_utils.py\u001b[0m in \u001b[0;36minner_adapt_matching\u001b[0;34m(task, loss, learner, n_ways, k_shots, q_shots, EPSILON, device)\u001b[0m\n\u001b[1;32m    101\u001b[0m     \u001b[0mclipped_y_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclamp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEPSILON\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mEPSILON\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[0meval_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclipped_y_pred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mqueries_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m     \u001b[0meval_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclipped_y_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mqueries_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0meval_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_acc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/torch/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/torch/lib/python3.8/site-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    214\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnll_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/torch/lib/python3.8/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mnll_loss\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[1;32m   2382\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2383\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2384\u001b[0;31m         raise ValueError(\n\u001b[0m\u001b[1;32m   2385\u001b[0m             \u001b[0;34m\"Expected input batch_size ({}) to match target batch_size ({}).\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2386\u001b[0m         )\n",
      "\u001b[0;31mValueError\u001b[0m: Expected input batch_size (5) to match target batch_size (75)."
     ]
    }
   ],
   "source": [
    "loss, acc = inner_adapt_matching(ttask, nn.NLLLoss(), learner, 5, 1, 15, 1e-8, 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MatchingNetwork(1, (2,2), True,\n",
    "                 1, 64, 2, 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1, labels1 = train_tasks.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = learner.encoder(data1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([80, 64])"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([80])"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "labels1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cpu'\n",
    "n_ways = 5; k_shots = 1; q_shots = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, labels = a, labels1\n",
    "data, labels = data.to(device), labels.to(device)\n",
    "total = n_ways * (k_shots + q_shots)\n",
    "queries_index = np.zeros(total)\n",
    "\n",
    "# Extracting the evaluation datums from the entire task set, for the meta gradient calculation\n",
    "for offset in range(n_ways):\n",
    "    queries_index[np.random.choice(\n",
    "        k_shots+q_shots, q_shots, replace=False) + ((k_shots + q_shots)*offset)] = True\n",
    "support = data[np.where(queries_index == 0)]\n",
    "support_labels = labels[np.where(queries_index == 0)]\n",
    "queries = data[np.where(queries_index == 1)]\n",
    "queries_labels = labels[np.where(queries_index == 1)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([5, 64])"
      ]
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "source": [
    "support.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = BidrectionalLSTM(1600, 1).to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "support, _, _ = learner.support_encoder(support.unsqueeze(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "support = support.squeeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[ 9.0817e-01,  6.9583e-01,  4.3644e-01,  3.6229e-01,  1.0181e+00,\n",
       "          1.3247e+00,  5.1304e-01, -5.1149e-03,  2.3022e-01,  1.0113e+00,\n",
       "         -9.9955e-03,  4.4339e-01, -1.4620e-01, -5.7340e-02,  1.0940e+00,\n",
       "          3.6482e-01,  1.0741e+00,  3.9233e-01, -2.5983e-01,  1.6549e+00,\n",
       "          2.3516e-01,  3.3219e-01,  8.3859e-01,  4.9970e-01,  3.1441e-02,\n",
       "          5.1546e-01,  5.3329e-01,  2.3418e-01,  2.7515e-02, -3.5647e-01,\n",
       "          5.1191e-01,  5.1294e-01,  7.6902e-01,  2.9464e-01,  2.6142e-01,\n",
       "          7.3845e-02, -8.2232e-03,  1.1275e+00,  7.1412e-02,  3.6004e-01,\n",
       "          7.4423e-02, -9.9185e-02, -1.8506e-01, -3.0775e-01, -3.3919e-02,\n",
       "          5.6184e-01,  8.2037e-01,  8.8993e-02,  4.7030e-01,  8.6843e-01,\n",
       "          1.3225e-01,  1.6914e-01,  6.5826e-02,  7.4236e-01,  1.6496e-01,\n",
       "         -8.1656e-03, -1.5094e-01,  5.3654e-01,  5.1269e-01,  2.3687e-01,\n",
       "          2.0769e-01, -1.8344e-01,  9.6344e-01,  4.4144e-01],\n",
       "        [ 1.1875e+00,  3.7363e-01,  6.7797e-01,  1.3792e+00,  1.4536e+00,\n",
       "          1.2805e+00,  4.2097e-01, -1.6916e-01,  7.3282e-01,  1.0253e+00,\n",
       "          6.6296e-01,  5.9029e-01,  8.4682e-02,  3.6777e-01,  1.3626e+00,\n",
       "          5.0144e-01,  1.4525e+00,  1.7733e+00, -2.9233e-01,  1.3231e+00,\n",
       "          1.7594e-01,  2.8144e+00,  9.6268e-01,  1.9103e-01,  6.0892e-01,\n",
       "          2.2303e-01,  3.5662e-01,  8.0385e-01, -3.4561e-04, -5.5401e-01,\n",
       "          1.9637e-01,  7.2329e-01,  1.2657e-01,  4.8406e-02,  4.0934e-01,\n",
       "          2.3875e-01,  2.6272e-02,  8.4069e-01,  1.0963e-01,  7.7979e-01,\n",
       "          2.4160e-01, -1.0031e-01, -2.3882e-01, -2.7886e-01,  7.8849e-01,\n",
       "          3.0636e-02,  7.5985e-01,  4.7999e-01,  6.1982e-01,  9.2899e-01,\n",
       "         -5.7342e-02,  9.0209e-01,  6.3189e-01,  1.0725e+00,  5.7586e-01,\n",
       "          2.8223e-01, -3.0235e-01,  1.8323e+00,  2.6074e-01,  3.0396e-01,\n",
       "          2.9440e-01, -2.4878e-01,  8.0918e-01,  4.0535e-01],\n",
       "        [ 9.7920e-01,  4.4591e-01,  6.7634e-01,  5.5898e-01,  1.1055e+00,\n",
       "          1.0811e+00,  5.0679e-01, -1.4564e-01,  6.2884e-01,  5.3929e-01,\n",
       "          1.3112e+00,  3.4526e-01, -1.4830e-02, -3.0062e-02,  1.1960e+00,\n",
       "          6.2746e-01,  1.1576e+00,  2.5994e+00, -4.7006e-01,  9.1180e-01,\n",
       "          2.3341e-01,  1.7765e+00,  8.4676e-01,  3.9961e-02,  2.6150e-01,\n",
       "          1.7579e-01,  1.9287e-01,  7.3444e-01,  1.6050e-01, -5.1532e-01,\n",
       "          7.1565e-01,  1.2819e+00,  1.6641e+00,  1.2331e+00,  6.3548e-01,\n",
       "          4.1794e-01,  1.9273e-01,  4.9284e-01,  1.2720e-01,  4.3397e-01,\n",
       "         -3.5911e-02,  3.1104e-01, -1.0039e-01, -3.3334e-01,  8.0495e-01,\n",
       "          2.5647e-01,  8.6891e-01, -2.5037e-02,  8.8438e-01,  3.9577e-01,\n",
       "         -3.7589e-03,  1.2106e+00,  3.1452e-01,  1.0871e+00,  2.6589e-01,\n",
       "          8.2019e-01, -3.0981e-01,  1.2239e+00, -9.7875e-02,  2.3543e-01,\n",
       "          2.5031e-01, -2.2406e-01,  6.3857e-01,  3.7910e-01],\n",
       "        [ 1.4334e+00,  6.5093e-01,  7.5586e-01,  5.7288e-01,  1.0922e+00,\n",
       "          1.5264e+00,  2.8641e-01, -1.5942e-01,  3.4345e-01,  6.6237e-01,\n",
       "          7.1287e-01,  2.8831e-01,  4.7890e-01,  1.2994e+00,  3.1405e-01,\n",
       "          4.1978e-01,  1.1437e+00,  9.3802e-01, -5.4914e-01,  7.3113e-01,\n",
       "          4.1764e-01,  1.1822e+00,  6.3986e-01,  1.2295e-01,  5.6892e-01,\n",
       "          7.8313e-01,  8.1977e-01,  1.2961e+00,  1.6204e-01, -4.0435e-01,\n",
       "          1.0650e+00,  6.7005e-01,  2.0842e-01,  7.3042e-01,  6.3541e-01,\n",
       "          3.4969e-01,  5.2447e-01,  9.7429e-01,  1.6902e-01,  6.7601e-01,\n",
       "          4.2840e-01,  2.6597e-01, -1.5715e-01, -2.2772e-01,  6.4750e-01,\n",
       "          1.4203e-01,  1.9054e+00,  4.3651e-01,  6.4340e-01,  4.3200e-01,\n",
       "         -1.0722e-01,  3.9606e-01,  1.6166e-01,  9.5339e-01,  2.1707e-01,\n",
       "          2.0483e-01, -4.6802e-01,  2.6475e-01, -2.1150e-02,  2.2620e-01,\n",
       "          1.0828e-01, -1.6045e-01,  1.0207e-01,  3.7306e-01],\n",
       "        [ 1.3257e+00,  6.5146e-01,  1.0069e+00,  1.2555e+00,  6.8559e-01,\n",
       "          9.5186e-01,  3.8439e-01, -2.1303e-01,  1.5349e-01,  9.5827e-01,\n",
       "          3.4119e-01,  9.0325e-02,  2.3707e-01,  8.9763e-01,  7.4813e-01,\n",
       "          2.4812e-01,  1.3394e+00,  8.5607e-01, -3.3974e-01,  9.7751e-01,\n",
       "          5.1475e-01,  2.5759e+00,  7.6666e-01, -8.5737e-02, -5.9958e-02,\n",
       "          4.9792e-02,  7.8281e-01,  7.7598e-01,  1.3017e-01, -4.7027e-01,\n",
       "          8.9975e-01,  7.3420e-01,  1.0290e+00,  6.5589e-01,  4.9675e-01,\n",
       "          3.6608e-01,  2.2949e-01,  7.2563e-01,  1.6478e-01,  4.8784e-01,\n",
       "          9.7464e-02,  1.2936e-01, -9.2792e-02, -1.6658e-01,  1.1297e+00,\n",
       "          5.9728e-01,  1.4596e+00,  6.7426e-01,  4.9587e-01,  1.2606e+00,\n",
       "         -2.5176e-01,  2.4152e-01,  1.6230e-01,  6.9796e-01,  4.4409e-02,\n",
       "          1.1735e-01, -3.9519e-01,  8.3659e-01,  2.6572e-01,  1.7678e-01,\n",
       "          2.0955e-02, -1.1889e-01,  6.0600e-01,  5.6634e-01]],\n",
       "       grad_fn=<SqueezeBackward1>)"
      ]
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "source": [
    "support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'AttentionLSTM' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-892bda4fb38f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAttentionLSTM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1600\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cuda'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'AttentionLSTM' is not defined"
     ]
    }
   ],
   "source": [
    "f = AttentionLSTM(1600, 2).to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = learner.query_encoder(support, queries, 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([75, 5])"
      ]
     },
     "metadata": {},
     "execution_count": 19
    }
   ],
   "source": [
    "preds = logits(queries=queries, support=support, EPSILON=1e-8)\n",
    "preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([75, 5])"
      ]
     },
     "metadata": {},
     "execution_count": 20
    }
   ],
   "source": [
    "attention = (-preds).softmax(dim=1)\n",
    "attention.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([5, 5])"
      ]
     },
     "metadata": {},
     "execution_count": 21
    }
   ],
   "source": [
    "y_onehot = torch.zeros(n_ways * k_shots, n_ways).to(device)\n",
    "y_onehot.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = support_labels.unsqueeze(-1)\n",
    "y_onehot = y_onehot.scatter(1, y, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([0, 2, 3, 1, 4])"
      ]
     },
     "metadata": {},
     "execution_count": 23
    }
   ],
   "source": [
    "support_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0., 0., 0.],\n",
       "        [0., 0., 1., 0., 0.],\n",
       "        [0., 0., 0., 1., 0.],\n",
       "        [0., 1., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 1.]])"
      ]
     },
     "metadata": {},
     "execution_count": 24
    }
   ],
   "source": [
    "y_onehot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[0.2025, 0.1988, 0.1981, 0.2010, 0.1996],\n",
       "        [0.2021, 0.1966, 0.2055, 0.1929, 0.2029],\n",
       "        [0.2206, 0.1996, 0.1944, 0.1840, 0.2014],\n",
       "        [0.2084, 0.1958, 0.1949, 0.1989, 0.2019],\n",
       "        [0.2115, 0.1955, 0.1939, 0.1971, 0.2021],\n",
       "        [0.2082, 0.2008, 0.1920, 0.1999, 0.1992],\n",
       "        [0.2177, 0.1910, 0.1945, 0.1965, 0.2003],\n",
       "        [0.2045, 0.1969, 0.2023, 0.1913, 0.2049],\n",
       "        [0.2196, 0.1934, 0.1965, 0.1927, 0.1979],\n",
       "        [0.2119, 0.1984, 0.2015, 0.1920, 0.1963],\n",
       "        [0.2187, 0.1980, 0.1892, 0.1915, 0.2026],\n",
       "        [0.2277, 0.1968, 0.1943, 0.1838, 0.1974],\n",
       "        [0.2102, 0.1996, 0.1971, 0.1897, 0.2032],\n",
       "        [0.1963, 0.2083, 0.1848, 0.2067, 0.2038],\n",
       "        [0.2165, 0.2004, 0.1885, 0.1959, 0.1987],\n",
       "        [0.1936, 0.2017, 0.2045, 0.1946, 0.2056],\n",
       "        [0.2128, 0.2002, 0.1912, 0.1980, 0.1979],\n",
       "        [0.1985, 0.2005, 0.2020, 0.1999, 0.1992],\n",
       "        [0.1868, 0.2031, 0.2071, 0.1954, 0.2077],\n",
       "        [0.1890, 0.1921, 0.2199, 0.1937, 0.2054],\n",
       "        [0.1873, 0.1960, 0.2100, 0.2035, 0.2032],\n",
       "        [0.1952, 0.2110, 0.1989, 0.1926, 0.2023],\n",
       "        [0.1916, 0.2139, 0.1964, 0.1943, 0.2038],\n",
       "        [0.1847, 0.2021, 0.2139, 0.2025, 0.1969],\n",
       "        [0.1931, 0.2107, 0.1964, 0.1935, 0.2063],\n",
       "        [0.1967, 0.1969, 0.1966, 0.2050, 0.2048],\n",
       "        [0.1896, 0.1976, 0.2112, 0.1966, 0.2051],\n",
       "        [0.1963, 0.2069, 0.1925, 0.2010, 0.2033],\n",
       "        [0.1890, 0.2012, 0.2087, 0.2007, 0.2004],\n",
       "        [0.2056, 0.1976, 0.2009, 0.2062, 0.1898],\n",
       "        [0.1888, 0.1894, 0.2096, 0.2145, 0.1977],\n",
       "        [0.2012, 0.1948, 0.2006, 0.2076, 0.1958],\n",
       "        [0.2032, 0.2006, 0.1974, 0.1983, 0.2005],\n",
       "        [0.1965, 0.2110, 0.1914, 0.1973, 0.2038],\n",
       "        [0.1978, 0.1984, 0.2034, 0.1989, 0.2015],\n",
       "        [0.2047, 0.1990, 0.1918, 0.2046, 0.2000],\n",
       "        [0.2000, 0.2118, 0.1908, 0.1934, 0.2040],\n",
       "        [0.1952, 0.2047, 0.1968, 0.1962, 0.2071],\n",
       "        [0.1893, 0.2062, 0.2067, 0.1992, 0.1986],\n",
       "        [0.1902, 0.1942, 0.2032, 0.2107, 0.2016],\n",
       "        [0.2054, 0.2055, 0.1952, 0.1969, 0.1970],\n",
       "        [0.1906, 0.2028, 0.1960, 0.2022, 0.2085],\n",
       "        [0.2001, 0.1846, 0.2031, 0.2106, 0.2017],\n",
       "        [0.1990, 0.2066, 0.1964, 0.1991, 0.1990],\n",
       "        [0.1988, 0.2032, 0.1936, 0.2052, 0.1992],\n",
       "        [0.1896, 0.2127, 0.2002, 0.1945, 0.2030],\n",
       "        [0.1901, 0.2239, 0.1953, 0.1872, 0.2035],\n",
       "        [0.1899, 0.2274, 0.1908, 0.1845, 0.2073],\n",
       "        [0.2023, 0.2124, 0.1963, 0.1862, 0.2029],\n",
       "        [0.1963, 0.2247, 0.1895, 0.1858, 0.2037],\n",
       "        [0.1829, 0.2219, 0.1926, 0.1903, 0.2122],\n",
       "        [0.1858, 0.2120, 0.1979, 0.1989, 0.2054],\n",
       "        [0.1890, 0.2126, 0.1899, 0.2040, 0.2045],\n",
       "        [0.1835, 0.2165, 0.1902, 0.2019, 0.2080],\n",
       "        [0.1932, 0.2052, 0.1932, 0.2043, 0.2041],\n",
       "        [0.1873, 0.2159, 0.1897, 0.2030, 0.2041],\n",
       "        [0.1856, 0.2235, 0.1944, 0.1868, 0.2097],\n",
       "        [0.1883, 0.2205, 0.1937, 0.1900, 0.2075],\n",
       "        [0.1925, 0.2178, 0.1951, 0.1889, 0.2057],\n",
       "        [0.1917, 0.2168, 0.1953, 0.1879, 0.2082],\n",
       "        [0.2063, 0.2161, 0.1867, 0.1919, 0.1990],\n",
       "        [0.1946, 0.2095, 0.1955, 0.1954, 0.2050],\n",
       "        [0.1884, 0.2060, 0.2026, 0.1952, 0.2078],\n",
       "        [0.1928, 0.2114, 0.1944, 0.1930, 0.2084],\n",
       "        [0.2004, 0.2002, 0.1994, 0.1995, 0.2005],\n",
       "        [0.2121, 0.2005, 0.1949, 0.1863, 0.2063],\n",
       "        [0.2012, 0.2049, 0.1926, 0.1979, 0.2034],\n",
       "        [0.2069, 0.2040, 0.1936, 0.1984, 0.1970],\n",
       "        [0.1972, 0.2028, 0.2038, 0.1978, 0.1983],\n",
       "        [0.1938, 0.2058, 0.2028, 0.1906, 0.2070],\n",
       "        [0.1995, 0.2167, 0.1945, 0.1954, 0.1939],\n",
       "        [0.1897, 0.2107, 0.1972, 0.1961, 0.2063],\n",
       "        [0.2173, 0.2056, 0.1915, 0.1863, 0.1993],\n",
       "        [0.2016, 0.2107, 0.1976, 0.1860, 0.2042],\n",
       "        [0.1984, 0.2096, 0.1975, 0.1914, 0.2031]], grad_fn=<MmBackward>)"
      ]
     },
     "metadata": {},
     "execution_count": 25
    }
   ],
   "source": [
    "y_pred = torch.mm(attention, y_onehot.to(device))\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([75])"
      ]
     },
     "metadata": {},
     "execution_count": 26
    }
   ],
   "source": [
    "queries_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[0.2025, 0.1988, 0.1981, 0.2010, 0.1996],\n",
       "        [0.2021, 0.1966, 0.2055, 0.1929, 0.2029],\n",
       "        [0.2206, 0.1996, 0.1944, 0.1840, 0.2014],\n",
       "        [0.2084, 0.1958, 0.1949, 0.1989, 0.2019],\n",
       "        [0.2115, 0.1955, 0.1939, 0.1971, 0.2021],\n",
       "        [0.2082, 0.2008, 0.1920, 0.1999, 0.1992],\n",
       "        [0.2177, 0.1910, 0.1945, 0.1965, 0.2003],\n",
       "        [0.2045, 0.1969, 0.2023, 0.1913, 0.2049],\n",
       "        [0.2196, 0.1934, 0.1965, 0.1927, 0.1979],\n",
       "        [0.2119, 0.1984, 0.2015, 0.1920, 0.1963],\n",
       "        [0.2187, 0.1980, 0.1892, 0.1915, 0.2026],\n",
       "        [0.2277, 0.1968, 0.1943, 0.1838, 0.1974],\n",
       "        [0.2102, 0.1996, 0.1971, 0.1897, 0.2032],\n",
       "        [0.1963, 0.2083, 0.1848, 0.2067, 0.2038],\n",
       "        [0.2165, 0.2004, 0.1885, 0.1959, 0.1987],\n",
       "        [0.1936, 0.2017, 0.2045, 0.1946, 0.2056],\n",
       "        [0.2128, 0.2002, 0.1912, 0.1980, 0.1979],\n",
       "        [0.1985, 0.2005, 0.2020, 0.1999, 0.1992],\n",
       "        [0.1868, 0.2031, 0.2071, 0.1954, 0.2077],\n",
       "        [0.1890, 0.1921, 0.2199, 0.1937, 0.2054],\n",
       "        [0.1873, 0.1960, 0.2100, 0.2035, 0.2032],\n",
       "        [0.1952, 0.2110, 0.1989, 0.1926, 0.2023],\n",
       "        [0.1916, 0.2139, 0.1964, 0.1943, 0.2038],\n",
       "        [0.1847, 0.2021, 0.2139, 0.2025, 0.1969],\n",
       "        [0.1931, 0.2107, 0.1964, 0.1935, 0.2063],\n",
       "        [0.1967, 0.1969, 0.1966, 0.2050, 0.2048],\n",
       "        [0.1896, 0.1976, 0.2112, 0.1966, 0.2051],\n",
       "        [0.1963, 0.2069, 0.1925, 0.2010, 0.2033],\n",
       "        [0.1890, 0.2012, 0.2087, 0.2007, 0.2004],\n",
       "        [0.2056, 0.1976, 0.2009, 0.2062, 0.1898],\n",
       "        [0.1888, 0.1894, 0.2096, 0.2145, 0.1977],\n",
       "        [0.2012, 0.1948, 0.2006, 0.2076, 0.1958],\n",
       "        [0.2032, 0.2006, 0.1974, 0.1983, 0.2005],\n",
       "        [0.1965, 0.2110, 0.1914, 0.1973, 0.2038],\n",
       "        [0.1978, 0.1984, 0.2034, 0.1989, 0.2015],\n",
       "        [0.2047, 0.1990, 0.1918, 0.2046, 0.2000],\n",
       "        [0.2000, 0.2118, 0.1908, 0.1934, 0.2040],\n",
       "        [0.1952, 0.2047, 0.1968, 0.1962, 0.2071],\n",
       "        [0.1893, 0.2062, 0.2067, 0.1992, 0.1986],\n",
       "        [0.1902, 0.1942, 0.2032, 0.2107, 0.2016],\n",
       "        [0.2054, 0.2055, 0.1952, 0.1969, 0.1970],\n",
       "        [0.1906, 0.2028, 0.1960, 0.2022, 0.2085],\n",
       "        [0.2001, 0.1846, 0.2031, 0.2106, 0.2017],\n",
       "        [0.1990, 0.2066, 0.1964, 0.1991, 0.1990],\n",
       "        [0.1988, 0.2032, 0.1936, 0.2052, 0.1992],\n",
       "        [0.1896, 0.2127, 0.2002, 0.1945, 0.2030],\n",
       "        [0.1901, 0.2239, 0.1953, 0.1872, 0.2035],\n",
       "        [0.1899, 0.2274, 0.1908, 0.1845, 0.2073],\n",
       "        [0.2023, 0.2124, 0.1963, 0.1862, 0.2029],\n",
       "        [0.1963, 0.2247, 0.1895, 0.1858, 0.2037],\n",
       "        [0.1829, 0.2219, 0.1926, 0.1903, 0.2122],\n",
       "        [0.1858, 0.2120, 0.1979, 0.1989, 0.2054],\n",
       "        [0.1890, 0.2126, 0.1899, 0.2040, 0.2045],\n",
       "        [0.1835, 0.2165, 0.1902, 0.2019, 0.2080],\n",
       "        [0.1932, 0.2052, 0.1932, 0.2043, 0.2041],\n",
       "        [0.1873, 0.2159, 0.1897, 0.2030, 0.2041],\n",
       "        [0.1856, 0.2235, 0.1944, 0.1868, 0.2097],\n",
       "        [0.1883, 0.2205, 0.1937, 0.1900, 0.2075],\n",
       "        [0.1925, 0.2178, 0.1951, 0.1889, 0.2057],\n",
       "        [0.1917, 0.2168, 0.1953, 0.1879, 0.2082],\n",
       "        [0.2063, 0.2161, 0.1867, 0.1919, 0.1990],\n",
       "        [0.1946, 0.2095, 0.1955, 0.1954, 0.2050],\n",
       "        [0.1884, 0.2060, 0.2026, 0.1952, 0.2078],\n",
       "        [0.1928, 0.2114, 0.1944, 0.1930, 0.2084],\n",
       "        [0.2004, 0.2002, 0.1994, 0.1995, 0.2005],\n",
       "        [0.2121, 0.2005, 0.1949, 0.1863, 0.2063],\n",
       "        [0.2012, 0.2049, 0.1926, 0.1979, 0.2034],\n",
       "        [0.2069, 0.2040, 0.1936, 0.1984, 0.1970],\n",
       "        [0.1972, 0.2028, 0.2038, 0.1978, 0.1983],\n",
       "        [0.1938, 0.2058, 0.2028, 0.1906, 0.2070],\n",
       "        [0.1995, 0.2167, 0.1945, 0.1954, 0.1939],\n",
       "        [0.1897, 0.2107, 0.1972, 0.1961, 0.2063],\n",
       "        [0.2173, 0.2056, 0.1915, 0.1863, 0.1993],\n",
       "        [0.2016, 0.2107, 0.1976, 0.1860, 0.2042],\n",
       "        [0.1984, 0.2096, 0.1975, 0.1914, 0.2031]], grad_fn=<ClampBackward>)"
      ]
     },
     "metadata": {},
     "execution_count": 27
    }
   ],
   "source": [
    "clipped_y_pred = y_pred.clamp(1e-8, 1 - 1e-8)\n",
    "clipped_y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_loss = nn.NLLLoss()(clipped_y_pred.log(), queries_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor(1.5734, grad_fn=<NllLossBackward>)"
      ]
     },
     "metadata": {},
     "execution_count": 29
    }
   ],
   "source": [
    "eval_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_acc = accuracy(clipped_y_pred, queries_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Unsqueeze to force y to be of shape (K*n, 1) as this is needed for .scatter()\n",
    "\n",
    "\n",
    "# Calculated loss with negative log likelihood\n",
    "# Clip predictions for numerical stability\n",
    "eval_loss = loss(clipped_y_pred.log(), queries_labels)\n",
    "eval_acc = accuracy(clipped_y_pred, queries_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logits(support, queries, EPSILON):\n",
    "    # Module with cosine similarity\n",
    "\n",
    "    n_queries = queries.shape[0]\n",
    "    n_support = support.shape[0]\n",
    "\n",
    "    normalised_queries = queries / (queries.pow(2).sum(dim=1, keepdim=True).sqrt() + EPSILON)\n",
    "    normalised_support = support / (support.pow(2).sum(dim=1, keepdim=True).sqrt() + EPSILON)\n",
    "\n",
    "    expanded_x = normalised_queries.unsqueeze(1).expand(n_queries, n_support, -1)\n",
    "    expanded_y = normalised_support.unsqueeze(0).expand(n_queries, n_support, -1)\n",
    "\n",
    "    logits = (expanded_x * expanded_y).sum(dim=2)\n",
    "    return 1 - logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention = (-logits(support, queries, 0.00001)).softmax(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_onehot = torch.zeros(n_ways * k_shots, n_ways).to(device)\n",
    "\n",
    "# Unsqueeze to force y to be of shape (K*n, 1) as this\n",
    "# is needed for .scatter()\n",
    "y = support_labels.unsqueeze(-1)\n",
    "y_onehot = y_onehot.scatter(1, y, 1)\n",
    "\n",
    "y_pred = torch.mm(attention, y_onehot.cuda().double())\n",
    "\n",
    "# Calculated loss with negative log likelihood\n",
    "# Clip predictions for numerical stability\n",
    "clipped_y_pred = y_pred.clamp(0.0001, 1 - 0.0001)\n",
    "#eval_loss = loss(clipped_y_pred.log(), queries_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([0, 3, 0, 0, 3, 3, 1, 1, 1, 3, 0, 3, 1, 1, 2], device='cuda:0')"
      ]
     },
     "metadata": {},
     "execution_count": 56
    }
   ],
   "source": [
    "clipped_y_pred.argmax(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    # Optionally apply full context embeddings\n",
    "    if fce:\n",
    "        # LSTM requires input of shape (seq_len, batch, input_size). `support` is of\n",
    "        # shape (k_way * n_shot, embedding_dim) and we want the LSTM to treat the\n",
    "        # support set as a sequence so add a single dimension to transform support set\n",
    "        # to the shape (k_way * n_shot, 1, embedding_dim) and then remove the batch dimension\n",
    "        # afterwards\n",
    "\n",
    "        # Calculate the fully conditional embedding, g, for support set samples as described\n",
    "        # in appendix A.2 of the paper. g takes the form of a bidirectional LSTM with a\n",
    "        # skip connection from inputs to outputs\n",
    "        support, _, _ = model.g(support.unsqueeze(1))\n",
    "        support = support.squeeze(1)\n",
    "\n",
    "        # Calculate the fully conditional embedding, f, for the query set samples as described\n",
    "        # in appendix A.1 of the paper.\n",
    "        queries = model.f(support, queries)\n",
    "\n",
    "    # Efficiently calculate distance between all queries and all prototypes\n",
    "    # Output should have shape (q_queries * k_way, k_way) = (num_queries, k_way)\n",
    "    distances = pairwise_distances(queries, support, distance)\n",
    "\n",
    "    # Calculate \"attention\" as softmax over support-query distances\n",
    "    attention = (-distances).softmax(dim=1)\n",
    "\n",
    "    # Calculate predictions as in equation (1) from Matching Networks\n",
    "    # y_hat = \\sum_{i=1}^{k} a(x_hat, x_i) y_i\n",
    "    y_pred = matching_net_predictions(attention, n_shot, k_way, q_queries)\n",
    "\n",
    "    # Calculated loss with negative log likelihood\n",
    "    # Clip predictions for numerical stability\n",
    "    clipped_y_pred = y_pred.clamp(EPSILON, 1 - EPSILON)\n",
    "    loss = loss_fn(clipped_y_pred.log(), y)\n",
    "\n",
    "    if train:\n",
    "        # Backpropagate gradients\n",
    "        loss.backward()\n",
    "        # I found training to be quite unstable so I clip the norm\n",
    "        # of the gradient to be at most 1\n",
    "        clip_grad_norm_(model.parameters(), 1)\n",
    "        # Take gradient step\n",
    "        optimiser.step()\n",
    "\n",
    "    return loss, y_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logits(support, queries, n, k, q):\n",
    "    prototypes = support.view(n, k, -1).mean(dim=1)\n",
    "    a = queries.shape[0]\n",
    "    b = prototypes.shape[0]\n",
    "    logits = -((queries.unsqueeze(1).expand(a,b,-1) - prototypes.unsqueeze(0).expand(a,b,-1))**2).sum(dim=2)\n",
    "    return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "support.view()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([450, 30])"
      ]
     },
     "metadata": {},
     "execution_count": 30
    }
   ],
   "source": [
    "logits(support, queries, 30, 1, 15).shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([1, 1, 1, 1, 1, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 4, 4, 4, 4,\n",
       "        4], device='cuda:0')"
      ]
     },
     "metadata": {},
     "execution_count": 67
    }
   ],
   "source": [
    "queries_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[0.4918, 0.5899, 0.0555,  ..., 1.4189, 1.5469, 1.7144],\n",
       "        [0.1789, 0.5006, 0.4659,  ..., 1.0633, 1.3545, 0.5393],\n",
       "        [0.2796, 0.1395, 0.0412,  ..., 0.5320, 0.4916, 0.8295],\n",
       "        ...,\n",
       "        [0.0000, 0.0000, 0.0000,  ..., 0.7130, 0.5586, 0.8045],\n",
       "        [0.2357, 0.5359, 0.5710,  ..., 0.6700, 0.0000, 0.3146],\n",
       "        [0.0000, 0.0662, 0.5751,  ..., 0.6450, 0.8230, 0.8644]],\n",
       "       device='cuda:0', grad_fn=<IndexBackward>)"
      ]
     },
     "metadata": {},
     "execution_count": 25
    }
   ],
   "source": [
    "support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = support.view(n_ways, k_shots, -1).mean(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([5, 1600])"
      ]
     },
     "metadata": {},
     "execution_count": 45
    }
   ],
   "source": [
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = a.shape[0]\n",
    "m = b.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([100, 128])"
      ]
     },
     "metadata": {},
     "execution_count": 64
    }
   ],
   "source": [
    "input1 = torch.randn(100, 128)\n",
    "input1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input1 = torch.randn(100, 128)\n",
    "input2 = torch.randn(100, 128)\n",
    "cos = nn.CosineSimilarity(dim=1, eps=1e-6)\n",
    "output = cos(input1, input2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[[0.0000, 0.0000, 0.1363,  ..., 0.7791, 0.6836, 0.4011],\n",
       "         [0.6607, 0.6905, 0.4536,  ..., 1.0306, 1.2191, 1.4360],\n",
       "         [0.0000, 0.0103, 0.1365,  ..., 0.0908, 0.3466, 0.5294],\n",
       "         ...,\n",
       "         [0.6833, 0.2566, 0.2808,  ..., 0.9656, 0.0514, 0.3068],\n",
       "         [0.2939, 0.4591, 0.6109,  ..., 0.8635, 0.5289, 0.8151],\n",
       "         [0.4777, 0.2959, 0.3257,  ..., 0.6681, 0.5542, 0.3793]],\n",
       "\n",
       "        [[0.0000, 0.0000, 0.1363,  ..., 0.7791, 0.6836, 0.4011],\n",
       "         [0.6607, 0.6905, 0.4536,  ..., 1.0306, 1.2191, 1.4360],\n",
       "         [0.0000, 0.0103, 0.1365,  ..., 0.0908, 0.3466, 0.5294],\n",
       "         ...,\n",
       "         [0.6833, 0.2566, 0.2808,  ..., 0.9656, 0.0514, 0.3068],\n",
       "         [0.2939, 0.4591, 0.6109,  ..., 0.8635, 0.5289, 0.8151],\n",
       "         [0.4777, 0.2959, 0.3257,  ..., 0.6681, 0.5542, 0.3793]],\n",
       "\n",
       "        [[0.0000, 0.0000, 0.1363,  ..., 0.7791, 0.6836, 0.4011],\n",
       "         [0.6607, 0.6905, 0.4536,  ..., 1.0306, 1.2191, 1.4360],\n",
       "         [0.0000, 0.0103, 0.1365,  ..., 0.0908, 0.3466, 0.5294],\n",
       "         ...,\n",
       "         [0.6833, 0.2566, 0.2808,  ..., 0.9656, 0.0514, 0.3068],\n",
       "         [0.2939, 0.4591, 0.6109,  ..., 0.8635, 0.5289, 0.8151],\n",
       "         [0.4777, 0.2959, 0.3257,  ..., 0.6681, 0.5542, 0.3793]],\n",
       "\n",
       "        [[0.0000, 0.0000, 0.1363,  ..., 0.7791, 0.6836, 0.4011],\n",
       "         [0.6607, 0.6905, 0.4536,  ..., 1.0306, 1.2191, 1.4360],\n",
       "         [0.0000, 0.0103, 0.1365,  ..., 0.0908, 0.3466, 0.5294],\n",
       "         ...,\n",
       "         [0.6833, 0.2566, 0.2808,  ..., 0.9656, 0.0514, 0.3068],\n",
       "         [0.2939, 0.4591, 0.6109,  ..., 0.8635, 0.5289, 0.8151],\n",
       "         [0.4777, 0.2959, 0.3257,  ..., 0.6681, 0.5542, 0.3793]],\n",
       "\n",
       "        [[0.0000, 0.0000, 0.1363,  ..., 0.7791, 0.6836, 0.4011],\n",
       "         [0.6607, 0.6905, 0.4536,  ..., 1.0306, 1.2191, 1.4360],\n",
       "         [0.0000, 0.0103, 0.1365,  ..., 0.0908, 0.3466, 0.5294],\n",
       "         ...,\n",
       "         [0.6833, 0.2566, 0.2808,  ..., 0.9656, 0.0514, 0.3068],\n",
       "         [0.2939, 0.4591, 0.6109,  ..., 0.8635, 0.5289, 0.8151],\n",
       "         [0.4777, 0.2959, 0.3257,  ..., 0.6681, 0.5542, 0.3793]]],\n",
       "       device='cuda:0', grad_fn=<ExpandBackward>)"
      ]
     },
     "metadata": {},
     "execution_count": 60
    }
   ],
   "source": [
    "queries.unsqueeze(0).expand(n,m,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[[3.6123e-02, 1.1260e-01, 2.0007e-02,  ..., 7.4561e-02,\n",
       "          4.5304e-02, 2.9212e-01],\n",
       "         [2.2153e-01, 1.2597e-01, 3.0933e-02,  ..., 4.6551e-04,\n",
       "          1.0405e-01, 2.4451e-01],\n",
       "         [3.6123e-02, 1.0580e-01, 1.9951e-02,  ..., 9.2419e-01,\n",
       "          3.0241e-01, 1.6983e-01],\n",
       "         ...,\n",
       "         [2.4324e-01, 6.2327e-03, 9.3467e-06,  ..., 7.4968e-03,\n",
       "          7.1422e-01, 4.0293e-01],\n",
       "         [1.0791e-02, 1.5251e-02, 1.1098e-01,  ..., 3.5604e-02,\n",
       "          1.3514e-01, 1.5996e-02],\n",
       "         [8.2713e-02, 1.5748e-03, 2.3003e-03,  ..., 1.4747e-01,\n",
       "          1.1716e-01, 3.1607e-01]],\n",
       "\n",
       "        [[8.5715e-02, 9.8171e-02, 5.0794e-02,  ..., 1.8140e-04,\n",
       "          1.3637e-03, 2.7103e-02],\n",
       "         [1.3539e-01, 1.4225e-01, 8.4547e-03,  ..., 5.6651e-02,\n",
       "          2.4849e-01, 7.5748e-01],\n",
       "         [8.5715e-02, 9.1829e-02, 5.0705e-02,  ..., 4.9246e-01,\n",
       "          1.3987e-01, 1.3141e-03],\n",
       "         ...,\n",
       "         [1.5248e-01, 3.2159e-03, 6.5401e-03,  ..., 2.9931e-02,\n",
       "          4.4782e-01, 6.7036e-02],\n",
       "         [1.3686e-06, 2.1238e-02, 6.2101e-02,  ..., 5.0268e-03,\n",
       "          3.6745e-02, 6.2189e-02],\n",
       "         [3.4183e-02, 3.0437e-04, 1.2936e-03,  ..., 1.5481e-02,\n",
       "          2.7678e-02, 3.4726e-02]],\n",
       "\n",
       "        [[2.2964e-02, 2.4213e-01, 3.3238e-01,  ..., 3.0429e-02,\n",
       "          7.1779e-01, 3.9447e-01],\n",
       "         [2.5927e-01, 3.9368e-02, 6.7185e-02,  ..., 5.9359e-03,\n",
       "          9.7228e-02, 1.6557e-01],\n",
       "         [2.2964e-02, 2.3211e-01, 3.3215e-01,  ..., 7.4430e-01,\n",
       "          1.4026e+00, 2.4969e-01],\n",
       "         ...,\n",
       "         [2.8272e-01, 5.5437e-02, 1.8664e-01,  ..., 1.4487e-04,\n",
       "          2.1889e+00, 5.2179e-01],\n",
       "         [2.0279e-02, 1.0897e-03, 1.0393e-02,  ..., 8.1126e-03,\n",
       "          1.0040e+00, 4.5821e-02],\n",
       "         [1.0636e-01, 3.8490e-02, 1.4986e-01,  ..., 8.1449e-02,\n",
       "          9.5388e-01, 4.2222e-01]],\n",
       "\n",
       "        [[1.2849e-01, 8.3346e-02, 8.2018e-02,  ..., 1.2773e-02,\n",
       "          1.1995e-02, 7.0317e-02],\n",
       "         [9.1367e-02, 1.6143e-01, 9.5716e-04,  ..., 1.9172e-02,\n",
       "          1.8138e-01, 5.9258e-01],\n",
       "         [1.2849e-01, 7.7510e-02, 8.1904e-02,  ..., 6.4209e-01,\n",
       "          1.9944e-01, 1.8713e-02],\n",
       "         ...,\n",
       "         [1.0549e-01, 1.0293e-03, 2.0131e-02,  ..., 5.3956e-03,\n",
       "          5.5025e-01, 1.2921e-01],\n",
       "         [4.1621e-03, 2.9022e-02, 3.5415e-02,  ..., 8.2089e-04,\n",
       "          6.9846e-02, 2.2151e-02],\n",
       "         [1.4209e-02, 5.1561e-05, 9.4048e-03,  ..., 5.0164e-02,\n",
       "          5.7102e-02, 8.2307e-02]],\n",
       "\n",
       "        [[3.0861e-02, 3.8756e-02, 3.8711e-02,  ..., 3.6661e-03,\n",
       "          1.0678e-02, 1.0344e-01],\n",
       "         [2.3528e-01, 2.4365e-01, 1.4538e-02,  ..., 3.6456e-02,\n",
       "          1.8669e-01, 5.0885e-01],\n",
       "         [3.0861e-02, 3.4810e-02, 3.8633e-02,  ..., 5.6075e-01,\n",
       "          1.9396e-01, 3.7344e-02],\n",
       "         ...,\n",
       "         [2.5764e-01, 3.5700e-03, 2.7297e-03,  ..., 1.5857e-02,\n",
       "          5.4111e-01, 1.7298e-01],\n",
       "         [1.3988e-02, 6.8743e-02, 7.7187e-02,  ..., 5.6739e-04,\n",
       "          6.6615e-02, 8.5345e-03],\n",
       "         [9.1196e-02, 9.8034e-03, 5.3909e-05,  ..., 2.9413e-02,\n",
       "          5.4184e-02, 1.1788e-01]]], device='cuda:0', grad_fn=<PowBackward0>)"
      ]
     },
     "metadata": {},
     "execution_count": 61
    }
   ],
   "source": [
    "(a.unsqueeze(1).expand(n, m, -1) - queries.unsqueeze(0).expand(n,m,-1))**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "logits = -((a.unsqueeze(1).expand(n, m, -1) -\n",
    "            b.unsqueeze(0).expand(n, m, -1))**2).sum(dim=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/home/anuj/anaconda3/envs/torch/lib/python3.8/site-packages/torchvision/transforms/transforms.py:257: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "train_tasks, valid_tasks, test_tasks, learner = setup('omniglot', '../../omniglot', 5, 5, 5, False, 0.03, 'cuda')\n",
    "opt = optim.Adam(learner.parameters(), 0.01)\n",
    "loss = nn.CrossEntropyLoss(reduction='mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "ttask = train_tasks.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([50, 1, 28, 28])"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "ttask[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = learner.clone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, labels = ttask\n",
    "data, labels = data.to(device), labels.to(device)\n",
    "total = n_ways * (k_shots + q_shots)\n",
    "queries_index = np.zeros(total)\n",
    "\n",
    "# Extracting the evaluation datums from the entire task set, for the meta gradient calculation \n",
    "for offset in range(n_ways):\n",
    "    queries_index[np.random.choice(k_shots+q_shots, q_shots, replace=False) + ((k_shots + q_shots)*offset)] = True\n",
    "support = data[np.where(queries_index == 0)]\n",
    "support_labels = labels[np.where(queries_index == 0)]\n",
    "queries = data[np.where(queries_index == 1)]\n",
    "queries_labels = labels[np.where(queries_index == 1)]\n",
    "\n",
    "# Inner adapt step\n",
    "for _ in range(1):\n",
    "    adapt_loss = loss(learner(support), support_labels)\n",
    "    learner.adapt(adapt_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[-2.3058,  0.8712, -0.3009, -1.7466,  0.2174],\n",
       "        [-1.1646,  0.6933,  0.0663, -0.8611,  0.2450],\n",
       "        [-2.1604,  1.5395, -1.5346, -2.1959,  0.7367],\n",
       "        [-1.6999,  0.4818, -1.1877, -2.2231, -0.6955],\n",
       "        [-2.3995,  1.2572, -0.9198, -1.8828, -0.8643],\n",
       "        [-3.2638,  0.4875, -0.9614, -0.7871,  0.4653],\n",
       "        [-3.3043, -0.3927, -0.9587, -3.2453,  0.3632],\n",
       "        [-1.9540, -2.1205, -1.3125, -1.1369, -0.3256],\n",
       "        [-2.2095, -1.0025, -2.8084, -1.4914, -2.5339],\n",
       "        [-3.6524, -1.3129,  0.4602, -2.3179,  1.8694],\n",
       "        [-0.2180,  0.5564,  0.4151, -0.3363, -0.8863],\n",
       "        [ 3.1093, -2.3283, -1.8008,  0.9621, -2.6799],\n",
       "        [ 0.1015,  0.7979, -0.8207, -1.3333, -1.3204],\n",
       "        [ 0.5794, -1.4207, -0.9276,  0.3640, -1.8067],\n",
       "        [ 3.1537, -6.6684, -2.5053,  0.5397, -2.4689],\n",
       "        [ 1.1471, -0.7925, -0.6154, -1.3091, -1.4252],\n",
       "        [ 1.9084, -4.7103, -3.9228, -2.1826, -1.3713],\n",
       "        [ 2.2197, -0.5845, -0.1133,  0.5664, -1.8442],\n",
       "        [ 0.7250, -0.6186, -1.5460, -4.6297, -0.6061],\n",
       "        [-0.1871, -2.6361, -2.9961, -1.4738, -1.4592],\n",
       "        [ 0.1406, -0.9463,  1.4528, -0.4812, -1.1988],\n",
       "        [ 1.7844, -1.4845, -1.0468, -1.8976, -2.1174],\n",
       "        [-2.4476, -2.6299, -2.3048,  0.3068, -0.8256],\n",
       "        [-0.7030,  0.0141, -0.9806, -1.5074, -1.4905],\n",
       "        [-2.7172, -0.2624, -0.9600, -0.6464, -1.0488]], device='cuda:0',\n",
       "       grad_fn=<AddmmBackward>)"
      ]
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "source": [
    "preds = learner(queries)\n",
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_loss = loss(preds, queries_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([ True,  True,  True,  True,  True, False,  True,  True, False,  True,\n",
       "        False, False, False, False, False,  True,  True,  True,  True,  True,\n",
       "         True, False, False, False, False], device='cuda:0')"
      ]
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "source": [
    " preds.argmax(dim=1) == queries_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor(0.5600, device='cuda:0')"
      ]
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "source": [
    "predictions = preds.argmax(dim=1).view(queries_labels.shape)\n",
    "(predictions == queries_labels).sum().float() / queries_labels.size(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_loss, evaluation_accuracy = inner_adapt_maml(ttask, loss, model, 5,5,5, 1, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "RuntimeError",
     "evalue": "Trying to backward through the graph a second time, but the saved intermediate results have already been freed. Specify retain_graph=True when calling .backward() or autograd.grad() the first time.",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-373737206d5a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mevaluation_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/torch/lib/python3.8/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    243\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 245\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/torch/lib/python3.8/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    143\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m    146\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Trying to backward through the graph a second time, but the saved intermediate results have already been freed. Specify retain_graph=True when calling .backward() or autograd.grad() the first time."
     ]
    }
   ],
   "source": [
    "evaluation_loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "<ipython-input-27-8ed4d768114f>:2: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n  print(p.grad.data)\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'data'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-8ed4d768114f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'data'"
     ]
    }
   ],
   "source": [
    "for p in model.parameters():\n",
    "    print(p.grad.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.6800000071525574"
      ]
     },
     "metadata": {},
     "execution_count": 32
    }
   ],
   "source": [
    "evaluation_accuracy.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.816496580927726"
      ]
     },
     "metadata": {},
     "execution_count": 36
    }
   ],
   "source": [
    "np.arange(3).std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'anuj'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'../logs/{name}.csv'"
      ]
     },
     "metadata": {},
     "execution_count": 41
    }
   ],
   "source": [
    "path = '../logs/{name}.csv' \n",
    "path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "profiler = Profiler('ProNets_{}_{}-shot_{}-way_{}-queries'.format('omni', 5,5,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'omni'\n",
    "prof = Profiler('abc_{}'.format(name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "profiler.log([2,2,3,4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "prof = Profiler('MAML_omni')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "prof.log([1,2,3,4,5,6,7,8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(predictions, targets):\n",
    "    predictions = predictions.argmax(dim=1).view(targets.shape)\n",
    "    return (predictions == targets).sum().float() / targets.size(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_Dataset(seed, mach, nways, kshots, tasks, data_loc):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    device = torch.device('cuda') if mach == 'cuda' else torch.device('cpu')\n",
    "\n",
    "    data = l2l.vision.datasets.FullOmniglot(root=data_loc, transform=transforms.Compose([\n",
    "                                                transforms.Resize(28),\n",
    "                                                transforms.ToTensor()]))\n",
    "    data = l2l.data.MetaDataset(data)\n",
    "    transform = [\n",
    "    l2l.data.transforms.NWays(data, n=2*nways),\n",
    "    l2l.data.transforms.KShots(data, k=2*kshots),\n",
    "    l2l.data.transforms.LoadData(data),\n",
    "]\n",
    "    tasksets = l2l.data.TaskDataset(data, transform, num_tasks=tasks)\n",
    "    \n",
    "    return tasksets, data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "tasksets, data = set_Dataset(42, 'cuda', 5, 5, 20000, '../../omniglot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "32460"
      ]
     },
     "metadata": {},
     "execution_count": 157
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/home/anuj/anaconda3/envs/torch/lib/python3.8/site-packages/torchvision/transforms/transforms.py:257: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  warnings.warn(\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# Load train/validation/test tasksets using the benchmark interface (5-way, 5-shot)\n",
    "tasksets = l2l.vision.benchmarks.get_tasksets('omniglot',\n",
    "                                                train_ways=5,\n",
    "                                                train_samples=2*5,\n",
    "                                                test_ways=5,\n",
    "                                                test_samples=2*5,\n",
    "                                                num_tasks=20000,\n",
    "                                                root='../../omniglot',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "20000"
      ]
     },
     "metadata": {},
     "execution_count": 161
    }
   ],
   "source": [
    "len(tasksets.validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([50])"
      ]
     },
     "metadata": {},
     "execution_count": 46
    }
   ],
   "source": [
    "tasksets.train.sample()[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([50, 1, 28, 28])"
      ]
     },
     "metadata": {},
     "execution_count": 47
    }
   ],
   "source": [
    "tasksets.test.sample()[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model\n",
    "model = l2l.vision.models.OmniglotFC(28 ** 2, 5)\n",
    "model.to(device)\n",
    "maml = l2l.algorithms.MAML(model, lr=0.5, first_order=False) #wrapper on model for in-place weight updation (adaptation)\n",
    "opt = optim.Adam(maml.parameters(), 0.003) #meta-optimizer\n",
    "loss = nn.CrossEntropyLoss(reduction='mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "MAML(\n",
       "  (module): OmniglotFC(\n",
       "    (features): Sequential(\n",
       "      (0): Flatten()\n",
       "      (1): Sequential(\n",
       "        (0): LinearBlock(\n",
       "          (relu): ReLU()\n",
       "          (normalize): BatchNorm1d(256, eps=0.001, momentum=0.999, affine=True, track_running_stats=False)\n",
       "          (linear): Linear(in_features=784, out_features=256, bias=True)\n",
       "        )\n",
       "        (1): LinearBlock(\n",
       "          (relu): ReLU()\n",
       "          (normalize): BatchNorm1d(128, eps=0.001, momentum=0.999, affine=True, track_running_stats=False)\n",
       "          (linear): Linear(in_features=256, out_features=128, bias=True)\n",
       "        )\n",
       "        (2): LinearBlock(\n",
       "          (relu): ReLU()\n",
       "          (normalize): BatchNorm1d(64, eps=0.001, momentum=0.999, affine=True, track_running_stats=False)\n",
       "          (linear): Linear(in_features=128, out_features=64, bias=True)\n",
       "        )\n",
       "        (3): LinearBlock(\n",
       "          (relu): ReLU()\n",
       "          (normalize): BatchNorm1d(64, eps=0.001, momentum=0.999, affine=True, track_running_stats=False)\n",
       "          (linear): Linear(in_features=64, out_features=64, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (classifier): Linear(in_features=64, out_features=5, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "metadata": {},
     "execution_count": 49
    }
   ],
   "source": [
    "maml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def fast_adapt(batch, learner, loss, adaptation_steps, shots, ways, device):\n",
    "    data, labels = batch\n",
    "    data, labels = data.to(device), labels.to(device)\n",
    "\n",
    "    # Separate data into adaptation/evalutation sets\n",
    "    adaptation_indices = np.zeros(data.size(0), dtype=bool)\n",
    "    adaptation_indices[np.arange(shots*ways) * 2] = True # set even indices to true\n",
    "    evaluation_indices = torch.from_numpy(~adaptation_indices)\n",
    "    adaptation_indices = torch.from_numpy(adaptation_indices)\n",
    "    adaptation_data, adaptation_labels = data[adaptation_indices], labels[adaptation_indices]\n",
    "    evaluation_data, evaluation_labels = data[evaluation_indices], labels[evaluation_indices]\n",
    "\n",
    "    # Adapt the model\n",
    "    for step in range(adaptation_steps):\n",
    "        train_error = loss(learner(adaptation_data), adaptation_labels)\n",
    "        learner.adapt(train_error)\n",
    "\n",
    "    # Evaluate the adapted model\n",
    "    predictions = learner(evaluation_data)\n",
    "    valid_error = loss(predictions, evaluation_labels)\n",
    "    valid_accuracy = accuracy(predictions, evaluation_labels)\n",
    "    return valid_error, valid_accuracy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner = maml.clone()\n",
    "batch = tasksets.train.sample()\n",
    "evaluation_error, evaluation_accuracy = fast_adapt(batch,\n",
    "                                                    learner,\n",
    "                                                    loss,\n",
    "                                                    1,\n",
    "                                                    5,\n",
    "                                                    5,\n",
    "                                                    'cuda') # updates model inplace for inner update\n",
    "\n",
    "evaluation_error.backward() # gradients comp on eval set for meta-update "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([256])\ntorch.Size([256])\ntorch.Size([256, 784])\ntorch.Size([256])\ntorch.Size([128])\ntorch.Size([128])\ntorch.Size([128, 256])\ntorch.Size([128])\ntorch.Size([64])\ntorch.Size([64])\ntorch.Size([64, 128])\ntorch.Size([64])\ntorch.Size([64])\ntorch.Size([64])\ntorch.Size([64, 64])\ntorch.Size([64])\ntorch.Size([5, 64])\ntorch.Size([5])\n"
     ]
    }
   ],
   "source": [
    "a=[]\n",
    "for p in maml.parameters():\n",
    "    a.append(p.grad.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([ 0.0839, -0.0497, -0.0046, -0.0065, -0.0231], device='cuda:0')"
      ]
     },
     "metadata": {},
     "execution_count": 94
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for iteration in range(1):\n",
    "    opt.zero_grad()\n",
    "    meta_train_error = 0.0\n",
    "    meta_train_accuracy = 0.0\n",
    "    meta_valid_error = 0.0\n",
    "    meta_valid_accuracy = 0.0\n",
    "    for task in range(32):\n",
    "        # Compute meta-training loss\n",
    "        learner = maml.clone()\n",
    "        batch = tasksets.train.sample()\n",
    "        evaluation_error, evaluation_accuracy = fast_adapt(batch,\n",
    "                                                            learner,\n",
    "                                                            loss,\n",
    "                                                            adaptation_steps,\n",
    "                                                            shots,\n",
    "                                                            ways,\n",
    "                                                            device) # updates model inplace for inner update\n",
    "\n",
    "        evaluation_error.backward() # gradients comp on eval set for meta-update \n",
    "        meta_train_error += evaluation_error.item()\n",
    "        meta_train_accuracy += evaluation_accuracy.item()\n",
    "\n",
    "        # Compute meta-validation loss\n",
    "        learner = maml.clone()\n",
    "        batch = tasksets.validation.sample()\n",
    "        evaluation_error, evaluation_accuracy = fast_adapt(batch,\n",
    "                                                            learner,\n",
    "                                                            loss,\n",
    "                                                            adaptation_steps,\n",
    "                                                            shots,\n",
    "                                                            ways,\n",
    "                                                            device)\n",
    "        meta_valid_error += evaluation_error.item()\n",
    "        meta_valid_accuracy += evaluation_accuracy.item()\n",
    "\n",
    "    # Print some metrics\n",
    "    print('\\n')\n",
    "    print('Iteration', iteration)\n",
    "    print('Meta Train Error', meta_train_error / meta_batch_size)\n",
    "    print('Meta Train Accuracy', meta_train_accuracy / meta_batch_size)\n",
    "    print('Meta Valid Error', meta_valid_error / meta_batch_size)\n",
    "    print('Meta Valid Accuracy', meta_valid_accuracy / meta_batch_size)\n",
    "\n",
    "    # Average the accumulated gradients and optimize\n",
    "    for p in maml.parameters():\n",
    "        p.grad.data.mul_(1.0 / meta_batch_size)\n",
    "    opt.step()\n",
    "\n",
    "meta_test_error = 0.0\n",
    "meta_test_accuracy = 0.0\n",
    "for task in range(meta_batch_size):\n",
    "    # Compute meta-testing loss\n",
    "    learner = maml.clone()\n",
    "    batch = tasksets.test.sample()\n",
    "    evaluation_error, evaluation_accuracy = fast_adapt(batch,\n",
    "                                                        learner,\n",
    "                                                        loss,\n",
    "                                                        adaptation_steps,\n",
    "                                                        shots,\n",
    "                                                        ways,\n",
    "                                                        device)\n",
    "    meta_test_error += evaluation_error.item()\n",
    "    meta_test_accuracy += evaluation_accuracy.item()\n",
    "print('Meta Test Error', meta_test_error / meta_batch_size)\n",
    "print('Meta Test Accuracy', meta_test_accuracy / meta_batch_size)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def main(\n",
    "        ways=5,\n",
    "        shots=1,\n",
    "        meta_lr=0.003,\n",
    "        fast_lr=0.5,\n",
    "        meta_batch_size=32,\n",
    "        adaptation_steps=1,\n",
    "        num_iterations=60000,\n",
    "        cuda=True,\n",
    "        seed=42,\n",
    "):\n",
    "    \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python388jvsc74a57bd08d67afa83e86333a367939f878b63191e1d5d5d165e62fc4144602f7751c67e3",
   "display_name": "Python 3.8.8 64-bit ('torch': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}