{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "classifiers: nn.crossentropyloss = -log-likelihood --- use for logp(y) and -logq(y/x) for support <br>\n",
    "kl-div: <br>\n",
    "reconstr-loss: set reduction to none and then take mean of losses per image in the total batch. This gives reconstr-loss per image for further computation<br> "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "import sys\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import learn2learn as l2l\n",
    "import pandas as pd\n",
    "\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "from PIL.Image import LANCZOS\n",
    "from config import *\n",
    "\n",
    "import json\n",
    "import copy\n",
    "\n",
    "from torch.distributions.multivariate_normal import MultivariateNormal\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "torch.cuda.is_available()\n"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "sys.path.append('/home/anuj/Desktop/Work/TU_Delft/research/implement/learning_to_meta-learn')\n",
    "sys.path"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['/home/anuj/Desktop/Work/TU_Delft/research/implement/learning_to_meta-learn/src',\n",
       " '/home/anuj/.vscode/extensions/ms-toolsai.jupyter-2021.8.1236758218/pythonFiles',\n",
       " '/home/anuj/.vscode/extensions/ms-toolsai.jupyter-2021.8.1236758218/pythonFiles/lib/python',\n",
       " '/home/anuj/anaconda3/envs/torch/lib/python38.zip',\n",
       " '/home/anuj/anaconda3/envs/torch/lib/python3.8',\n",
       " '/home/anuj/anaconda3/envs/torch/lib/python3.8/lib-dynload',\n",
       " '',\n",
       " '/home/anuj/anaconda3/envs/torch/lib/python3.8/site-packages',\n",
       " '/home/anuj/anaconda3/envs/torch/lib/python3.8/site-packages/datasets-1.2.1-py3.8.egg',\n",
       " '/home/anuj/anaconda3/envs/torch/lib/python3.8/site-packages/locket-0.2.1-py3.8.egg',\n",
       " '/home/anuj/anaconda3/envs/torch/lib/python3.8/site-packages/IPython/extensions',\n",
       " '/home/anuj/.ipython',\n",
       " '/home/anuj/Desktop/Work/TU_Delft/research/implement/learning_to_meta-learn']"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "from data.loaders import Omniglotmix, MiniImageNet\n",
    "from data.taskers import gen_tasks\n",
    "from src.zoo.archs import CVAE\n",
    "from src.zoo.lpo_utils import proto_distr, classify, set_sets, accuracy, inner_adapt_lpo\n",
    "\n",
    "#from src.utils import Profiler"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "train_tasks = gen_tasks(dataname='miniimagenet', root='../../mini_imagenet', mode='train', n_ways=5, k_shots=3, q_shots=10, image_transforms=None)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "classes = list(range(1623))\n",
    "random.shuffle(classes)\n",
    "image_transforms = transforms.Compose([transforms.Resize(28, interpolation=LANCZOS),\n",
    "                                                    transforms.ToTensor(),\n",
    "                                                    lambda x: 1.0 - x,\n",
    "                                                ])\n",
    "train_tasks = gen_tasks('omniglot', '/home/anuj/Desktop/Work/TU_Delft/research/implement/omniglot', image_transforms=image_transforms, n_ways=5, k_shots=3, q_shots=10, classes=classes[:1100])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/home/anuj/anaconda3/envs/torch/lib/python3.8/site-packages/torchvision/transforms/transforms.py:257: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "ttask = train_tasks.sample()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "loss = nn.BCELoss(reduction='none')\n",
    "learner = CVAE(in_channels=1, y_shape=5, base_channels=32, latent_dim=64).to('cuda')\n",
    "support, y_support, queries, qs, y_queries, queries_labels = set_sets(ttask, 5, 3, 10, 'cuda')\n",
    "# Running inner adaptation loop\n",
    "J_alpha, query_preds = inner_adapt_lpo(support, y_support, qs, y_queries, learner, loss, 5,3,10)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "learner_temp = copy.deepcopy(learner.state_dict())"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "query_preds[::5,]"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[0.1999, 0.2000, 0.1999, 0.2002, 0.2000],\n",
       "        [0.1999, 0.2000, 0.1999, 0.2002, 0.2000],\n",
       "        [0.1999, 0.2000, 0.1999, 0.2002, 0.2000],\n",
       "        [0.1999, 0.2000, 0.1999, 0.2002, 0.2000],\n",
       "        [0.1999, 0.2000, 0.1999, 0.2002, 0.2000],\n",
       "        [0.1999, 0.2000, 0.1999, 0.2002, 0.2000],\n",
       "        [0.1999, 0.2000, 0.1999, 0.2002, 0.2000],\n",
       "        [0.1999, 0.2000, 0.1999, 0.2002, 0.2000],\n",
       "        [0.1999, 0.2000, 0.1999, 0.2002, 0.2000],\n",
       "        [0.1999, 0.2000, 0.1999, 0.2002, 0.2000],\n",
       "        [0.1999, 0.2000, 0.1999, 0.2002, 0.2000],\n",
       "        [0.1999, 0.2000, 0.1999, 0.2002, 0.2000],\n",
       "        [0.1999, 0.2000, 0.1999, 0.2002, 0.2000],\n",
       "        [0.1999, 0.2000, 0.1999, 0.2002, 0.2000],\n",
       "        [0.1999, 0.2000, 0.1999, 0.2002, 0.2000],\n",
       "        [0.1999, 0.2000, 0.1999, 0.2002, 0.2000],\n",
       "        [0.1999, 0.2000, 0.1999, 0.2002, 0.2000],\n",
       "        [0.1999, 0.2000, 0.1999, 0.2002, 0.2000],\n",
       "        [0.1999, 0.2000, 0.1999, 0.2002, 0.2000],\n",
       "        [0.1999, 0.2000, 0.1999, 0.2002, 0.2000],\n",
       "        [0.1999, 0.2000, 0.1999, 0.2002, 0.2000],\n",
       "        [0.1999, 0.2000, 0.1999, 0.2002, 0.2000],\n",
       "        [0.1999, 0.2000, 0.1999, 0.2002, 0.2000],\n",
       "        [0.1999, 0.2000, 0.1999, 0.2002, 0.2000],\n",
       "        [0.1999, 0.2000, 0.1999, 0.2002, 0.2000],\n",
       "        [0.1999, 0.2000, 0.1999, 0.2002, 0.2000],\n",
       "        [0.1999, 0.2000, 0.1999, 0.2002, 0.2000],\n",
       "        [0.1999, 0.2000, 0.1999, 0.2002, 0.2000],\n",
       "        [0.1999, 0.2000, 0.1999, 0.2002, 0.2000],\n",
       "        [0.1999, 0.2000, 0.1999, 0.2002, 0.2000],\n",
       "        [0.1999, 0.2000, 0.1999, 0.2002, 0.2000],\n",
       "        [0.1999, 0.2000, 0.1999, 0.2002, 0.2000],\n",
       "        [0.1999, 0.2000, 0.1999, 0.2002, 0.2000],\n",
       "        [0.1999, 0.2000, 0.1999, 0.2002, 0.2000],\n",
       "        [0.1999, 0.2000, 0.1999, 0.2002, 0.2000],\n",
       "        [0.1999, 0.2000, 0.1999, 0.2002, 0.2000],\n",
       "        [0.1999, 0.2000, 0.1999, 0.2002, 0.2000],\n",
       "        [0.1999, 0.2000, 0.1999, 0.2002, 0.2000],\n",
       "        [0.1999, 0.2000, 0.1999, 0.2002, 0.2000],\n",
       "        [0.1999, 0.2000, 0.1999, 0.2002, 0.2000],\n",
       "        [0.1999, 0.2000, 0.1999, 0.2002, 0.2000],\n",
       "        [0.1999, 0.2000, 0.1999, 0.2002, 0.2000],\n",
       "        [0.1999, 0.2000, 0.1999, 0.2002, 0.2000],\n",
       "        [0.1999, 0.2000, 0.1999, 0.2002, 0.2000],\n",
       "        [0.1999, 0.2000, 0.1999, 0.2002, 0.2000],\n",
       "        [0.1999, 0.2000, 0.1999, 0.2002, 0.2000],\n",
       "        [0.1999, 0.2000, 0.1999, 0.2002, 0.2000],\n",
       "        [0.1999, 0.2000, 0.1999, 0.2002, 0.2000],\n",
       "        [0.1999, 0.2000, 0.1999, 0.2002, 0.2000],\n",
       "        [0.1999, 0.2000, 0.1999, 0.2002, 0.2000]], device='cuda:0',\n",
       "       grad_fn=<SliceBackward>)"
      ]
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "accuracy(query_preds[::5,], queries_labels)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor(0.2000, device='cuda:0')"
      ]
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "class Encoder(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                 num_input_channels : int,\n",
    "                 base_channel_size : int,\n",
    "                 latent_dim : int,\n",
    "                 act_fn : object = nn.GELU):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "            - num_input_channels : Number of input channels of the image. For CIFAR, this parameter is 3\n",
    "            - base_channel_size : Number of channels we use in the first convolutional layers. Deeper layers might use a duplicate of it.\n",
    "            - latent_dim : Dimensionality of latent representation z\n",
    "            - act_fn : Activation function used throughout the encoder network\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        c_hid = base_channel_size\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(num_input_channels, c_hid, kernel_size=3, padding=1, stride=2), # 28x28 => 16x16\n",
    "            act_fn(),\n",
    "            nn.Conv2d(c_hid, c_hid, kernel_size=3, padding=1),\n",
    "            act_fn(),\n",
    "            nn.Conv2d(c_hid, 2*c_hid, kernel_size=3, padding=1, stride=2), # 16x16 => 8x8\n",
    "            act_fn(),\n",
    "            nn.Conv2d(2*c_hid, 2*c_hid, kernel_size=3, padding=1),\n",
    "            act_fn(),\n",
    "            nn.Conv2d(2*c_hid, 2*c_hid, kernel_size=3, padding=1, stride=2), # 8x8 => 4x4\n",
    "            act_fn(),\n",
    "            nn.Flatten(), # Image grid to single feature vector\n",
    "        )\n",
    "        self.h1 = nn.Linear(2*16*c_hid, latent_dim)\n",
    "        self.h2 = nn.Linear(2*16*c_hid, latent_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.net(x)\n",
    "        mu = self.h1(x)\n",
    "        log_var = self.h2(x)\n",
    "        return mu, log_var\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                 num_input_channels : int,\n",
    "                 base_channel_size : int,\n",
    "                 latent_dim : int,\n",
    "                 act_fn : object = nn.GELU):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "            - num_input_channels : Number of channels of the image to reconstruct. For CIFAR, this parameter is 3\n",
    "            - base_channel_size : Number of channels we use in the last convolutional layers. Early layers might use a duplicate of it.\n",
    "            - latent_dim : Dimensionality of latent representation z + Dimensionality of one-hot encoded label \n",
    "            - act_fn : Activation function used throughout the decoder network\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        c_hid = base_channel_size\n",
    "        self.linear = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 2*16*c_hid),\n",
    "            act_fn()\n",
    "        )\n",
    "        self.net = nn.Sequential(\n",
    "            nn.ConvTranspose2d(2*c_hid, 2*c_hid, kernel_size=3, padding=1, stride=2), # 4x4 => 8x8\n",
    "            act_fn(),\n",
    "            nn.Conv2d(2*c_hid, 2*c_hid, kernel_size=3, padding=1),\n",
    "            act_fn(),\n",
    "            nn.ConvTranspose2d(2*c_hid, c_hid, kernel_size=3, output_padding=1, padding=1, stride=2), # 8x8 => 16x16\n",
    "            act_fn(),\n",
    "            nn.Conv2d(c_hid, c_hid, kernel_size=3, padding=1),\n",
    "            act_fn(),\n",
    "            nn.ConvTranspose2d(c_hid, num_input_channels, kernel_size=3, output_padding=1, padding=1, stride=2), # 16x16 => 32x32\n",
    "            nn.Sigmoid() # The input images is scaled between -1 and 1, hence the output has to be bounded as well\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear(x)\n",
    "        x = x.reshape(x.shape[0], -1, 4, 4)\n",
    "        x = self.net(x)\n",
    "        return x\n",
    "\n",
    "class CVAE(nn.Module):\n",
    "    def __init__(self, in_channels, y_shape, base_channels=64, latent_dim=64):\n",
    "        super(CVAE, self).__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.base_channels = base_channels\n",
    "        self.latent_dim = latent_dim\n",
    "        self.dec_latent_dim = self.latent_dim + y_shape\n",
    "\n",
    "        self.encoder = Encoder(num_input_channels=self.in_channels, base_channel_size=self.base_channels, latent_dim=self.latent_dim)\n",
    "\n",
    "        self.decoder = Decoder(num_input_channels=self.in_channels, base_channel_size=self.base_channels, latent_dim=self.dec_latent_dim)\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        if self.training:\n",
    "            std = torch.exp(0.5 * logvar)\n",
    "            eps = torch.randn_like(std)\n",
    "            return eps.mul(std).add_(mu)\n",
    "        else:\n",
    "            return mu\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        mu, log_var = self.encoder(x)\n",
    "        z = self.reparameterize(mu, log_var)\n",
    "        x = self.decoder(torch.cat([z, y], dim=1))\n",
    "        return x, mu, log_var"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    "cvae = CVAE(1, 5, 16, 64)\n",
    "s_cap, mu_s, log_var_s = cvae(support, y_support)\n",
    "mu_s.shape"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([15, 64])"
      ]
     },
     "metadata": {},
     "execution_count": 20
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "support, y_support, queries, qs, y_queries =set_sets(ttask, 5, 3, 10, 'cpu')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "enc = Encoder(1, 16, 64)\n",
    "a = enc(support)\n",
    "a[0].shape"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([15, 64])"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "def reparameterize(mu, logvar):\n",
    "    std = torch.exp(0.5 * logvar)\n",
    "    eps = torch.randn_like(std)\n",
    "    return eps.mul(std).add_(mu)\n",
    "\n",
    "z = reparameterize(a[0], a[1])\n",
    "z.shape        "
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([15, 64])"
      ]
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "torch.cat([z, y_support], dim=1).shape"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([15, 69])"
      ]
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "model = VAE(3, 16, 3, 3)\n",
    "model.weight_init(mean=0, std=0.02)\n",
    "model.to('cpu')"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "VAE(\n",
       "  (conv1): Conv2d(3, 3, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "  (conv1_bn): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv2): Conv2d(3, 6, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "  (conv2_bn): BatchNorm2d(6, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv3): Conv2d(6, 12, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "  (conv3_bn): BatchNorm2d(12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (fc1): Linear(in_features=1452, out_features=16, bias=True)\n",
       "  (fc2): Linear(in_features=1452, out_features=16, bias=True)\n",
       "  (d1): Linear(in_features=16, out_features=1452, bias=True)\n",
       "  (deconv2): ConvTranspose2d(12, 6, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "  (deconv2_bn): BatchNorm2d(6, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (deconv3): ConvTranspose2d(6, 3, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "  (deconv3_bn): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (deconv4): ConvTranspose2d(3, 3, kernel_size=(3, 3), stride=(2, 2))\n",
       ")"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "def inner_adapt_lpo(task, n_ways, k_shots, q_shots, device):\n",
    "    data, labels = task\n",
    "    data, labels = data.to(device), labels.to(device)\n",
    "    #data, labels = data.squeeze(0), labels.squeeze(0)\n",
    "    sort = torch.sort(labels)\n",
    "    data = data.squeeze(0)[sort.indices].squeeze(0)\n",
    "    labels = labels.squeeze(0)[sort.indices].squeeze(0)\n",
    "    total = n_ways * (k_shots + q_shots)\n",
    "    queries_index = np.zeros(total)\n",
    "\n",
    "    # Extracting the query datums from the entire task set\n",
    "    for offset in range(n_ways):\n",
    "        queries_index[np.random.choice(\n",
    "            k_shots+q_shots, q_shots, replace=False) + ((k_shots + q_shots)*offset)] = True\n",
    "    support = data[np.where(queries_index == 0)]\n",
    "    support_labels = labels[np.where(queries_index == 0)]\n",
    "    queries = data[np.where(queries_index == 1)]\n",
    "    queries_labels = labels[np.where(queries_index == 1)]\n",
    "\n",
    "    # Forward pass on the Support datums\n",
    "    y_support = F.one_hot(support_labels, num_classes=n_ways)\n",
    "    #support_cap, support_mu, support_log_var = learner(support, y_support)\n",
    "    #proto_mu, proto_var = proto_distr(support_mu, support_log_var)\n",
    "\n",
    "    # Forward pass on the Query datums\n",
    "    y_queries = torch.tensor(range(n_ways))\n",
    "    y_queries = y_queries.repeat(n_ways*q_shots)\n",
    "    y_queries = F.one_hot(y_queries, num_classes=n_ways)\n",
    "    qs = queries.repeat_interleave(n_ways, dim=0)\n",
    "    #queries_cap, queries_mu, queries_log_var = learner(queries, y_queries)\n",
    "    return support, y_support, queries, qs, y_queries \n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "a,b,c,d,e = inner_adapt_lpo(train_tasks.sample(), 5, 3, 10, 'cpu')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "e.shape"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([250, 5])"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "r_s, mu_s, log_var_s = model(a)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "mu_p, var_p = proto_distr(mu_s, log_var_s, 5, 3, 'average')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "logits_s = classify(mu_p, var_p, mu_s)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "source": [
    "r_q, mu_q, log_var_q = model(d)\n",
    "logits_q = classify(mu_p, var_p, mu_q)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "source": [
    "logits_q"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[-5.9524, -5.4496, -4.9405, -8.4811, -5.8921],\n",
       "        [-5.9524, -5.4496, -4.9405, -8.4811, -5.8921],\n",
       "        [-5.9524, -5.4496, -4.9405, -8.4811, -5.8921],\n",
       "        ...,\n",
       "        [-4.5103, -2.3777, -3.4718, -5.9384, -4.3534],\n",
       "        [-4.5103, -2.3777, -3.4718, -5.9384, -4.3534],\n",
       "        [-4.5103, -2.3777, -3.4718, -5.9384, -4.3534]], grad_fn=<SubBackward0>)"
      ]
     },
     "metadata": {},
     "execution_count": 131
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "source": [
    "logits_q.shape"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([250, 5])"
      ]
     },
     "metadata": {},
     "execution_count": 132
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "source": [
    "classication_loss_q = torch.sum(e * torch.log(F.softmax(logits_q, dim=1)), dim=1).mean()\n",
    "classication_loss_q"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor(-3.6833, grad_fn=<MeanBackward0>)"
      ]
     },
     "metadata": {},
     "execution_count": 35
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "source": [
    "loss = nn.CrossEntropyLoss()\n",
    "classication_loss_q = loss(logits_q, torch.argmax(e, dim=1))\n",
    "classication_loss_q"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor(3.6833, grad_fn=<NllLossBackward>)"
      ]
     },
     "metadata": {},
     "execution_count": 34
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Keep cross-entropy loss from torch as is (just add alpha*celoss to other losses)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "source": [
    "prior = F.softmax(torch.ones_like(b).float(), dim=1)\n",
    "prior\n"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[0.2000, 0.2000, 0.2000, 0.2000, 0.2000],\n",
       "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000],\n",
       "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000],\n",
       "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000],\n",
       "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000],\n",
       "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000],\n",
       "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000],\n",
       "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000],\n",
       "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000],\n",
       "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000],\n",
       "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000],\n",
       "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000],\n",
       "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000],\n",
       "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000],\n",
       "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000]])"
      ]
     },
     "metadata": {},
     "execution_count": 37
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "source": [
    "cross_entropy = nn.CrossEntropyLoss(reduction='none')\n",
    "cross_entropy(prior, torch.argmax(b, dim=1))"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([1.6094, 1.6094, 1.6094, 1.6094, 1.6094, 1.6094, 1.6094, 1.6094, 1.6094,\n",
       "        1.6094, 1.6094, 1.6094, 1.6094, 1.6094, 1.6094])"
      ]
     },
     "metadata": {},
     "execution_count": 40
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "source": [
    "r_s = torch.randn(a.shape)\n",
    "m = nn.Sigmoid()\n",
    "r_s = m(r_s)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "source": [
    "r_s.shape"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([15, 3, 84, 84])"
      ]
     },
     "metadata": {},
     "execution_count": 67
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "source": [
    "BCE = -(a/256 * torch.log(r_s) + (1 - a/256) * torch.log(1 - r_s))\n",
    "BCE"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[[[0.8964, 1.3776, 1.3968,  ..., 0.5393, 0.5039, 0.8013],\n",
       "          [0.3896, 1.0126, 0.1623,  ..., 0.7525, 0.7026, 0.1423],\n",
       "          [0.2898, 2.6294, 1.6210,  ..., 1.0025, 0.8157, 0.4748],\n",
       "          ...,\n",
       "          [1.0197, 0.3582, 0.9278,  ..., 0.2167, 0.5153, 0.5956],\n",
       "          [1.3958, 0.4942, 0.3381,  ..., 0.5057, 0.6814, 1.8045],\n",
       "          [0.6241, 0.3327, 0.5814,  ..., 0.6714, 0.7805, 0.4100]],\n",
       "\n",
       "         [[0.5437, 0.7469, 1.6851,  ..., 0.7264, 0.6496, 0.2133],\n",
       "          [0.8684, 0.5148, 0.8361,  ..., 1.2772, 1.3730, 0.8977],\n",
       "          [0.5045, 0.5200, 0.9974,  ..., 1.1619, 0.5084, 0.2228],\n",
       "          ...,\n",
       "          [0.4560, 1.1664, 0.4945,  ..., 0.5591, 0.8939, 0.6995],\n",
       "          [1.7425, 0.7466, 0.5008,  ..., 0.6835, 0.4680, 1.3841],\n",
       "          [0.5263, 0.5986, 0.8558,  ..., 0.3519, 0.7603, 0.5786]],\n",
       "\n",
       "         [[1.2357, 0.7024, 1.1838,  ..., 1.1810, 0.3435, 0.8327],\n",
       "          [0.2351, 0.4920, 0.5594,  ..., 0.1887, 0.3621, 0.2419],\n",
       "          [0.6331, 0.6183, 1.1877,  ..., 0.5472, 0.5375, 0.5341],\n",
       "          ...,\n",
       "          [0.1194, 0.7156, 1.1914,  ..., 0.9977, 1.4743, 0.6648],\n",
       "          [0.4228, 1.3938, 1.6067,  ..., 1.3652, 0.2791, 0.8751],\n",
       "          [0.6691, 0.7505, 2.0672,  ..., 1.4707, 0.6478, 1.2417]]],\n",
       "\n",
       "\n",
       "        [[[0.9286, 0.7178, 1.1406,  ..., 0.5798, 1.6339, 0.9590],\n",
       "          [0.8528, 0.6668, 0.4982,  ..., 0.7671, 0.6060, 0.6266],\n",
       "          [0.9750, 0.6380, 0.8264,  ..., 1.0449, 0.5641, 1.4342],\n",
       "          ...,\n",
       "          [1.3867, 0.4314, 0.6039,  ..., 0.5860, 0.8617, 0.5359],\n",
       "          [0.3893, 0.5621, 0.6784,  ..., 0.2552, 0.7869, 0.5621],\n",
       "          [0.5409, 0.3514, 1.0565,  ..., 0.4530, 0.4987, 0.7738]],\n",
       "\n",
       "         [[0.7347, 0.5994, 0.6638,  ..., 0.6097, 0.4793, 1.4717],\n",
       "          [0.6393, 0.6633, 0.3612,  ..., 1.2257, 0.6225, 1.2411],\n",
       "          [0.6755, 0.7477, 0.3345,  ..., 0.6464, 0.8937, 0.9550],\n",
       "          ...,\n",
       "          [0.8933, 0.5136, 0.7983,  ..., 1.2632, 1.1536, 0.7802],\n",
       "          [0.5928, 1.1265, 1.8488,  ..., 1.0031, 0.5481, 0.6833],\n",
       "          [0.7176, 1.6142, 1.4431,  ..., 0.8388, 0.8505, 0.9927]],\n",
       "\n",
       "         [[0.6349, 1.3191, 0.1691,  ..., 0.4615, 0.4942, 0.5712],\n",
       "          [0.7134, 0.7228, 0.5102,  ..., 0.2530, 1.0010, 0.6824],\n",
       "          [0.6802, 0.6699, 0.5856,  ..., 0.3984, 1.3595, 0.7479],\n",
       "          ...,\n",
       "          [0.5809, 1.4738, 1.3616,  ..., 0.9266, 2.1102, 1.2335],\n",
       "          [1.2397, 0.7705, 0.9418,  ..., 1.6519, 0.4570, 0.9178],\n",
       "          [0.4311, 0.3050, 1.0575,  ..., 0.5833, 0.5453, 0.6311]]],\n",
       "\n",
       "\n",
       "        [[[0.7220, 0.6983, 1.2915,  ..., 0.8431, 1.1734, 0.8091],\n",
       "          [1.3500, 0.6804, 0.7579,  ..., 0.7171, 0.7420, 0.6950],\n",
       "          [0.6802, 0.6817, 0.6454,  ..., 1.0027, 0.6795, 0.7141],\n",
       "          ...,\n",
       "          [1.0907, 0.6768, 0.6065,  ..., 0.6907, 0.5389, 0.6399],\n",
       "          [0.6348, 0.5810, 0.4728,  ..., 0.5944, 0.7234, 1.3636],\n",
       "          [0.5491, 0.5335, 1.0360,  ..., 0.8905, 0.6104, 1.2475]],\n",
       "\n",
       "         [[0.7034, 0.9595, 0.7804,  ..., 0.4522, 0.5757, 0.4268],\n",
       "          [0.7010, 1.4552, 0.8384,  ..., 0.8852, 0.7574, 0.5898],\n",
       "          [0.6903, 1.0278, 0.7972,  ..., 1.2901, 0.8605, 1.0006],\n",
       "          ...,\n",
       "          [0.8011, 0.6988, 0.7963,  ..., 0.3282, 0.4674, 1.3289],\n",
       "          [0.7525, 0.7132, 0.9061,  ..., 0.5108, 0.6743, 0.8534],\n",
       "          [0.6822, 0.7540, 0.6550,  ..., 0.7518, 0.5923, 1.0982]],\n",
       "\n",
       "         [[0.6863, 0.7064, 0.9268,  ..., 0.7611, 1.0376, 0.5492],\n",
       "          [0.7101, 1.1476, 0.6783,  ..., 0.7796, 0.7183, 0.6674],\n",
       "          [0.6707, 0.6708, 0.6839,  ..., 0.5626, 0.6918, 1.1222],\n",
       "          ...,\n",
       "          [0.6886, 0.6724, 0.8049,  ..., 1.3719, 0.6124, 0.5716],\n",
       "          [0.6985, 0.8785, 2.0247,  ..., 0.6320, 0.7739, 0.6690],\n",
       "          [0.7660, 1.1231, 1.3290,  ..., 0.6057, 0.3461, 0.5299]]],\n",
       "\n",
       "\n",
       "        ...,\n",
       "\n",
       "\n",
       "        [[[0.6480, 1.1389, 0.7798,  ..., 0.7250, 0.7622, 0.7440],\n",
       "          [0.9873, 1.1776, 0.6732,  ..., 0.7735, 0.8102, 0.8000],\n",
       "          [0.6909, 0.6443, 0.6890,  ..., 0.8089, 0.7159, 0.6883],\n",
       "          ...,\n",
       "          [0.6693, 0.6348, 0.6398,  ..., 0.7690, 0.6634, 1.7826],\n",
       "          [0.6889, 0.7474, 0.6726,  ..., 0.6296, 1.4270, 1.2282],\n",
       "          [0.6643, 0.9042, 0.8050,  ..., 0.7461, 0.6789, 0.7891]],\n",
       "\n",
       "         [[0.5712, 0.6773, 1.1398,  ..., 0.8483, 0.9615, 0.7357],\n",
       "          [0.6960, 0.6452, 0.5655,  ..., 0.7711, 0.8463, 0.6145],\n",
       "          [0.7263, 0.5367, 0.5952,  ..., 0.6013, 1.5007, 0.7584],\n",
       "          ...,\n",
       "          [0.9778, 0.9606, 0.6878,  ..., 1.7714, 1.1649, 0.7321],\n",
       "          [0.7120, 0.6965, 0.8106,  ..., 0.6765, 1.0238, 0.9015],\n",
       "          [0.8930, 0.8131, 0.8822,  ..., 0.6822, 0.8161, 0.6865]],\n",
       "\n",
       "         [[0.9439, 0.6974, 0.3851,  ..., 0.8216, 0.4706, 0.8869],\n",
       "          [0.8045, 0.5820, 0.5686,  ..., 0.4501, 0.4305, 0.8328],\n",
       "          [0.6818, 0.3738, 0.7133,  ..., 0.9875, 1.2759, 0.8528],\n",
       "          ...,\n",
       "          [0.8194, 0.7090, 0.6923,  ..., 0.8324, 0.7332, 0.7266],\n",
       "          [0.7210, 0.9971, 0.8329,  ..., 0.6968, 1.0217, 1.0489],\n",
       "          [0.7602, 0.8347, 0.6404,  ..., 1.0093, 1.4709, 0.7052]]],\n",
       "\n",
       "\n",
       "        [[[0.6457, 0.6472, 0.6030,  ..., 1.1435, 0.7964, 1.4369],\n",
       "          [0.8179, 0.6767, 0.5930,  ..., 0.7087, 0.6589, 0.6727],\n",
       "          [0.7230, 0.6525, 1.5723,  ..., 0.7111, 0.6680, 0.6377],\n",
       "          ...,\n",
       "          [0.6207, 1.2531, 0.6566,  ..., 0.6873, 0.9046, 0.6468],\n",
       "          [0.8359, 0.6734, 0.7494,  ..., 0.7215, 0.7705, 0.8701],\n",
       "          [0.6929, 0.6875, 0.6927,  ..., 0.9289, 0.7104, 0.7033]],\n",
       "\n",
       "         [[0.7363, 0.7804, 0.6835,  ..., 1.4705, 1.6887, 0.5509],\n",
       "          [0.6703, 0.6386, 1.0668,  ..., 0.6515, 1.1042, 0.5134],\n",
       "          [0.6351, 0.7752, 0.7158,  ..., 0.8511, 0.7632, 0.6120],\n",
       "          ...,\n",
       "          [0.5876, 0.7154, 0.6133,  ..., 0.8136, 0.7030, 0.6297],\n",
       "          [0.6362, 0.6387, 0.6680,  ..., 0.7469, 0.7795, 0.7158],\n",
       "          [0.6958, 0.7605, 0.7221,  ..., 0.9308, 0.7031, 0.7168]],\n",
       "\n",
       "         [[0.4982, 0.7964, 0.5071,  ..., 1.2403, 1.4912, 0.9822],\n",
       "          [0.5976, 0.4666, 0.7428,  ..., 0.6386, 1.2525, 1.1410],\n",
       "          [0.4736, 0.4787, 0.9916,  ..., 0.6841, 0.6994, 0.6355],\n",
       "          ...,\n",
       "          [0.6802, 0.6180, 0.8756,  ..., 0.6473, 1.2700, 0.8428],\n",
       "          [0.6118, 1.0288, 0.5688,  ..., 0.6990, 0.7259, 0.7350],\n",
       "          [0.7914, 0.9310, 0.8787,  ..., 0.9068, 0.7525, 0.7856]]],\n",
       "\n",
       "\n",
       "        [[[0.9637, 1.1610, 0.4281,  ..., 0.8452, 0.6480, 1.6355],\n",
       "          [1.0262, 1.2693, 0.7076,  ..., 0.5300, 1.1064, 0.9469],\n",
       "          [0.6877, 0.5589, 0.6297,  ..., 0.9099, 1.2250, 1.1980],\n",
       "          ...,\n",
       "          [0.2453, 0.4859, 0.4623,  ..., 0.3983, 0.5696, 0.2924],\n",
       "          [2.5644, 0.4676, 0.2823,  ..., 0.6772, 0.5510, 0.7217],\n",
       "          [1.0225, 0.9081, 0.2935,  ..., 0.8436, 1.2528, 1.1746]],\n",
       "\n",
       "         [[0.7205, 0.8667, 1.3262,  ..., 2.0364, 0.7643, 0.3769],\n",
       "          [1.3092, 2.2209, 1.1267,  ..., 0.4079, 0.6817, 1.9924],\n",
       "          [0.9619, 1.5576, 0.7579,  ..., 0.4907, 0.9109, 1.1949],\n",
       "          ...,\n",
       "          [0.2637, 1.4711, 0.7426,  ..., 0.5335, 0.5242, 0.6014],\n",
       "          [0.8911, 0.6790, 0.4562,  ..., 0.4667, 0.5053, 0.6325],\n",
       "          [0.3215, 0.8114, 0.4912,  ..., 0.7560, 0.3104, 1.3732]],\n",
       "\n",
       "         [[1.4251, 0.5121, 0.7743,  ..., 0.7850, 0.6844, 1.8184],\n",
       "          [1.8884, 0.6190, 0.7251,  ..., 1.2849, 0.5514, 0.6533],\n",
       "          [1.1358, 1.3013, 0.5666,  ..., 0.8848, 1.7892, 0.3853],\n",
       "          ...,\n",
       "          [1.0701, 1.2141, 0.4099,  ..., 1.0074, 1.3022, 1.1235],\n",
       "          [0.3696, 1.0513, 0.6424,  ..., 0.3762, 0.4948, 1.3862],\n",
       "          [0.9075, 1.2429, 1.0141,  ..., 1.0171, 0.5496, 0.9625]]]])"
      ]
     },
     "metadata": {},
     "execution_count": 78
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "source": [
    "loss = nn.BCELoss(reduction='none')\n",
    "BCE = loss(r_s, (a/256))\n",
    "BCE.view(r_s.shape[0], -1).mean(dim=1).mean()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor(0.8075)"
      ]
     },
     "metadata": {},
     "execution_count": 80
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "source": [
    "loss = nn.BCELoss()\n",
    "BCE = loss(r_s, (a/256))\n",
    "BCE"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor(0.8075)"
      ]
     },
     "metadata": {},
     "execution_count": 58
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "source": [
    "def kl_div(mus, log_vars):\n",
    "    return - 0.5 * (1 + log_vars - mus**2 - torch.exp(log_vars)).sum(dim=1)\n",
    "\n",
    "kld = kl_div(mu_s, log_var_s)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "source": [
    "kld"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([3.6877, 1.8342, 3.3309, 1.1454, 1.5170, 1.0895, 1.6448, 0.8020, 3.0228,\n",
       "        1.8053, 4.7988, 4.8425, 1.9420, 1.6121, 1.4942],\n",
       "       grad_fn=<MulBackward0>)"
      ]
     },
     "metadata": {},
     "execution_count": 66
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "source": [
    "r_q = torch.randn(d.shape)\n",
    "m = nn.Sigmoid()\n",
    "r_q = m(r_q)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "source": [
    "r_q.shape"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([250, 3, 84, 84])"
      ]
     },
     "metadata": {},
     "execution_count": 85
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "source": [
    "reconstruction_loss = nn.BCELoss(reduction='none')\n",
    "ce_loss_q = torch.nn.CrossEntropyLoss(reduction='none')\n",
    "\n",
    "# BCE = loss(r_q, d/256)\n",
    "# BCE.view(r_q.shape[0], -1).mean(dim=-1).shape"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "source": [
    "l = -reconstruction_loss(r_q, d).view(d.shape[0], -1).mean(dim=1) - ce_loss_q(\n",
    "        F.softmax(torch.ones_like(e).float(), dim=1), torch.argmax(e, dim=1)) - kl_div(mu_q, log_var_q)\n",
    "l.shape"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([250])"
      ]
     },
     "metadata": {},
     "execution_count": 91
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "source": [
    "l"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([ -2.9386,  -4.2036,  -4.5291,  -3.3997,  -3.4342,  -4.0794,  -4.1871,\n",
       "         -4.4376,  -4.2197,  -4.0751,  -3.5086,  -3.0915,  -3.9436,  -4.0142,\n",
       "         -4.6801,  -4.5906,  -5.8111,  -5.2481,  -6.2033,  -5.0098,  -4.0412,\n",
       "         -3.5116,  -3.8603,  -3.3279,  -3.7606,  -3.8182,  -5.6941,  -3.4970,\n",
       "         -3.0530,  -4.1521,  -5.2496,  -4.9337,  -4.0076,  -3.9477,  -4.2349,\n",
       "         -4.6654,  -3.1890,  -5.1358,  -3.7509,  -4.4481,  -3.4389,  -4.2766,\n",
       "         -3.9453,  -3.8763,  -3.7239,  -3.0148,  -3.2896,  -3.0357,  -3.8589,\n",
       "         -3.7308,  -3.1677,  -3.4604,  -4.3223,  -4.1827,  -4.1109, -43.0757,\n",
       "        -40.6057, -42.4956, -41.6685, -41.9128,  -3.7205,  -4.9787,  -4.2097,\n",
       "         -4.6712,  -3.1462,  -2.3690,  -3.0369,  -3.9227,  -3.8489,  -3.8800,\n",
       "         -5.3113,  -2.9105,  -4.0527,  -5.0898,  -4.9180,  -2.1841,  -2.5850,\n",
       "         -2.9370,  -3.9071,  -2.1923,  -4.7662,  -3.1652,  -4.4739,  -4.6347,\n",
       "         -3.8818,  -2.7097,  -3.9221,  -3.2599,  -3.9838,  -4.5381,  -6.1871,\n",
       "         -4.6854,  -7.9825,  -6.2613,  -4.9013,  -4.9609,  -4.4720,  -3.2002,\n",
       "         -3.1195,  -6.7270,  -3.8848,  -3.5216,  -3.1735,  -3.2250,  -3.8798,\n",
       "         -2.8887,  -3.9328,  -3.3914,  -5.1277,  -4.4223,  -3.1603,  -4.2632,\n",
       "         -2.9617,  -3.7460,  -3.0706,  -3.4172,  -3.7633,  -4.3548,  -3.5051,\n",
       "         -3.8755,  -5.1876,  -1.8562,  -5.0686,  -3.0900,  -4.5749,  -4.4521,\n",
       "         -8.1246,  -3.7676,  -4.6973,  -3.1050,  -6.5372,  -4.8012,  -5.3411,\n",
       "         -4.5182,  -6.1508,  -5.5166,  -3.5642,  -4.0098,  -3.5624,  -4.7440,\n",
       "         -3.4624,  -3.4760,  -3.2980,  -4.0727,  -2.8733,  -1.3508,  -5.8798,\n",
       "         -3.0551,  -4.5635,  -4.2763,  -4.5025,  -2.7365,  -6.1714,  -6.1363,\n",
       "         -3.8691,  -4.9453,  -5.0478,  -4.2685,  -4.7991,  -4.6818,  -4.5777,\n",
       "         -3.1309,  -4.2145,  -4.1179,  -4.6537,  -4.0060,  -4.8165,  -2.8125,\n",
       "         -5.2928,  -3.9133, -10.2125, -11.0434, -10.3542, -11.0592,  -9.9941,\n",
       "         -5.8122,  -6.1446,  -6.4448,  -7.2205,  -6.0365,  -4.8607,  -4.4542,\n",
       "         -3.6046,  -5.8443,  -3.2142,  -4.6835,  -4.8040,  -3.3098,  -5.5344,\n",
       "         -3.5089,  -3.9362,  -3.7347,  -3.8354,  -4.0878,  -4.3383,  -4.1093,\n",
       "         -4.5265,  -3.9120,  -3.7781,  -3.7658,  -4.3327,  -3.0755,  -1.9273,\n",
       "         -3.6832,  -3.6240,  -3.0693,  -3.8903,  -2.3121,  -4.9386,  -3.8912,\n",
       "         -4.0118,  -2.4001,  -3.9093,  -5.8468,  -3.0483,  -1.8520,  -2.8366,\n",
       "         -3.9980,  -5.4153,  -1.0500,  -4.1979,  -6.9444,  -2.0440,  -3.3208,\n",
       "         -4.8544,  -2.7479,  -2.7642,  -4.3118,  -3.0642,  -3.0435,  -2.8985,\n",
       "         -2.1105,  -3.6690,  -3.4660,  -2.8073,  -4.8082,  -4.3665,  -5.5800,\n",
       "         -3.0617,  -5.3178,  -4.2475,  -2.3285,  -4.6373,  -3.0890,  -3.7416,\n",
       "         -5.0348,  -3.6410,  -4.6910,  -3.8043,  -3.7444],\n",
       "       grad_fn=<SubBackward0>)"
      ]
     },
     "metadata": {},
     "execution_count": 120
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "source": [
    "l.reshape([50, 5]).t().shape"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([5, 50])"
      ]
     },
     "metadata": {},
     "execution_count": 142
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "source": [
    "F.softmax(logits_q, dim=1)[::5,].shape"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([50, 5])"
      ]
     },
     "metadata": {},
     "execution_count": 141
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "source": [
    "torch.mm(F.softmax(logits_q, dim=1)[::5,], l.reshape([50, 5]).t()).diag().shape"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([50])"
      ]
     },
     "metadata": {},
     "execution_count": 150
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "source": [
    "lq = F.softmax(logits_q, dim=1)[::5,]\n",
    "h = -torch.sum(torch.mul(lq, torch.log(lq + 1e-8)), dim=1)\n",
    "h.shape"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([50])"
      ]
     },
     "metadata": {},
     "execution_count": 152
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "source": [
    "a = torch.Tensor([[1,2,3], [4,5,6]])\n",
    "b = torch.Tensor([[0,5,2], [8,9,7]])\n",
    "torch.mul(a,b)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[ 0., 10.,  6.],\n",
       "        [32., 45., 42.]])"
      ]
     },
     "metadata": {},
     "execution_count": 151
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/home/anuj/Desktop/Work/TU_Delft/research/implement/learning_to_meta-learn/src/zoo/lpo_utils.py:128: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  F.softmax(torch.ones_like(y_support).float()), torch.argmax(y_support, dim=1)) - kl_div(support_mu, support_log_var)  # = -L(x_s, y_s)\n",
      "/home/anuj/Desktop/Work/TU_Delft/research/implement/learning_to_meta-learn/src/zoo/lpo_utils.py:131: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  F.softmax(torch.ones_like(y_queries).float()), torch.argmax(y_queries, dim=1)) - kl_div(queries_mu, queries_log_var)  # = -L(x_q, y_q)\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "RuntimeError",
     "evalue": "multi-target not supported at /opt/conda/conda-bld/pytorch_1616554793803/work/aten/src/THCUNN/generic/ClassNLLCriterion.cu:15",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-39ecd86732fb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0msupport\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_support\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mqueries\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mqs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_queries\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset_sets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mttask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'cuda'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Running inner adaptation loop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mJ_alpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquery_preds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minner_adapt_lpo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msupport\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_support\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mqs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_queries\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCVAE\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_channels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbase_channels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlatent_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cuda'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Desktop/Work/TU_Delft/research/implement/learning_to_meta-learn/src/zoo/lpo_utils.py\u001b[0m in \u001b[0;36minner_adapt_lpo\u001b[0;34m(support, y_support, qs, y_queries, learner, reconstruction_loss, n_ways, k_shots, q_shots)\u001b[0m\n\u001b[1;32m    136\u001b[0m                                  ::5, ])), dim=1)\n\u001b[1;32m    137\u001b[0m     \u001b[0malpha\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.1\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq_shots\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mk_shots\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m     \u001b[0mJ_alpha\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mL_support\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mU_queries\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mce_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msupport_logits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_support\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    139\u001b[0m     \u001b[0mJ_alpha\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mJ_alpha\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/torch/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/torch/lib/python3.8/site-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m   1045\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1046\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1047\u001b[0;31m         return F.cross_entropy(input, target, weight=self.weight,\n\u001b[0m\u001b[1;32m   1048\u001b[0m                                ignore_index=self.ignore_index, reduction=self.reduction)\n\u001b[1;32m   1049\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/torch/lib/python3.8/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[1;32m   2691\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2692\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2693\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mnll_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2694\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2695\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/torch/lib/python3.8/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mnll_loss\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[1;32m   2386\u001b[0m         )\n\u001b[1;32m   2387\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2388\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnll_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2389\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2390\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnll_loss2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: multi-target not supported at /opt/conda/conda-bld/pytorch_1616554793803/work/aten/src/THCUNN/generic/ClassNLLCriterion.cu:15"
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "environment": {
   "name": "pytorch-gpu.1-8.m71",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-8:m71"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.8 64-bit ('torch': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "interpreter": {
   "hash": "8d67afa83e86333a367939f878b63191e1d5d5d165e62fc4144602f7751c67e3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}