{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "classifiers: nn.crossentropyloss = -log-likelihood --- use for logp(y) and -logq(y/x) for support <br>\n",
    "kl-div: <br>\n",
    "reconstr-loss: set reduction to none and then take mean of losses per image in the total batch. This gives reconstr-loss per image for further computation<br> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import learn2learn as l2l\n",
    "import pandas as pd\n",
    "import os\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from PIL.Image import LANCZOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/anuj/Desktop/Work/TU_Delft/research/implement/learning_to_meta-learn/src',\n",
       " '/home/anuj/.vscode/extensions/ms-toolsai.jupyter-2021.9.1101343141/pythonFiles',\n",
       " '/home/anuj/.vscode/extensions/ms-toolsai.jupyter-2021.9.1101343141/pythonFiles/lib/python',\n",
       " '/home/anuj/anaconda3/envs/torch/lib/python38.zip',\n",
       " '/home/anuj/anaconda3/envs/torch/lib/python3.8',\n",
       " '/home/anuj/anaconda3/envs/torch/lib/python3.8/lib-dynload',\n",
       " '',\n",
       " '/home/anuj/anaconda3/envs/torch/lib/python3.8/site-packages',\n",
       " '/home/anuj/anaconda3/envs/torch/lib/python3.8/site-packages/datasets-1.2.1-py3.8.egg',\n",
       " '/home/anuj/anaconda3/envs/torch/lib/python3.8/site-packages/locket-0.2.1-py3.8.egg',\n",
       " '/home/anuj/anaconda3/envs/torch/lib/python3.8/site-packages/IPython/extensions',\n",
       " '/home/anuj/.ipython',\n",
       " '/home/anuj/Desktop/Work/TU_Delft/research/implement/learning_to_meta-learn']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sys.path.append('/home/anuj/Desktop/Work/TU_Delft/research/implement/learning_to_meta-learn')\n",
    "sys.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from data.loaders import Omniglotmix, MiniImageNet\n",
    "from data.taskers import gen_tasks\n",
    "from src.zoo.archs import CCVAE, CEncoder, Classifier_VAE, ConvBase\n",
    "from src.zoo.delpo_utils import setup, inner_adapt_delpo, loss, accuracy\n",
    "from src.utils2 import Profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tasks = gen_tasks(dataname='miniimagenet', root='../../dataset/mini_imagenet', mode='train', n_ways=5, k_shots=5, q_shots=15, image_transforms=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/anuj/anaconda3/envs/torch/lib/python3.8/site-packages/torchvision/transforms/transforms.py:280: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "classes = list(range(1623))\n",
    "random.shuffle(classes)\n",
    "image_transforms = transforms.Compose([transforms.Resize(28, interpolation=LANCZOS),\n",
    "                                                    transforms.ToTensor(),\n",
    "                                                    lambda x: 1.0 - x,\n",
    "                                                ])\n",
    "train_tasks = gen_tasks('omniglot', '/home/anuj/Desktop/Work/TU_Delft/research/implement/dataset/omniglot', image_transforms=image_transforms, n_ways=5, k_shots=5, q_shots=15, classes=classes[:1100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda'\n",
    "n_ways = 5\n",
    "k_shots = 1\n",
    "q_shots = 15\n",
    "dataset = 'omniglot'\n",
    "root = '../../dataset/omniglot'\n",
    "order = False\n",
    "inner_lr = 0.01\n",
    "meta_lr = 0.001\n",
    "reconst_loss = nn.MSELoss(reduction='none')\n",
    "inner_adapt_steps_train = 5\n",
    "meta_batch_size = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/anuj/anaconda3/envs/torch/lib/python3.8/site-packages/torchvision/transforms/transforms.py:280: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "train_tasks, valid_tasks, test_tasks, learner = setup(\n",
    "    dataset, root, n_ways, k_shots, q_shots, order, inner_lr, device, download='False', task_adapt=False, task_adapt_fn='none', args=args)\n",
    "opt = optim.Adam(learner.parameters(), meta_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args:\n",
    "    def __init__(self, wt_ce, klwt, rec_wt, beta_l, beta_s):\n",
    "        #args.wt_ce, args.klwt, args.rec_wt, args.beta_l, args.beta_s\n",
    "        self.wt_ce = wt_ce\n",
    "        self.klwt = klwt\n",
    "        self.rec_wt = rec_wt\n",
    "        self.beta_l = beta_l\n",
    "        self.beta_s = beta_s\n",
    "        self.dataset = 'omniglot'\n",
    "        self.task_adapt = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = Args(10, False, 0.1, 1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MAML(\n",
       "  (module): CCVAE(\n",
       "    (encoder): CEncoder(\n",
       "      (net): Sequential(\n",
       "        (0): Conv2d(1, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU()\n",
       "        (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "        (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): ReLU()\n",
       "        (6): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "        (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (8): ReLU()\n",
       "        (9): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "        (10): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (11): ReLU()\n",
       "        (12): Flatten(start_dim=1, end_dim=-1)\n",
       "      )\n",
       "      (h1): Linear(in_features=256, out_features=64, bias=True)\n",
       "      (h2): Linear(in_features=256, out_features=64, bias=True)\n",
       "    )\n",
       "    (decoder): CDecoder(\n",
       "      (linear): Sequential(\n",
       "        (0): Linear(in_features=128, out_features=256, bias=True)\n",
       "        (1): ReLU()\n",
       "      )\n",
       "      (net): Sequential(\n",
       "        (0): UpsamplingNearest2d(size=(4, 4), mode=nearest)\n",
       "        (1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
       "        (2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (3): ReLU()\n",
       "        (4): UpsamplingNearest2d(size=(7, 7), mode=nearest)\n",
       "        (5): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
       "        (6): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (7): ReLU()\n",
       "        (8): UpsamplingNearest2d(size=(14, 14), mode=nearest)\n",
       "        (9): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
       "        (10): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (11): ReLU()\n",
       "        (12): UpsamplingNearest2d(size=(28, 28), mode=nearest)\n",
       "        (13): Conv2d(64, 1, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
       "        (14): BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (15): Sigmoid()\n",
       "      )\n",
       "    )\n",
       "    (classifier_vae): Classifier_VAE(\n",
       "      (encoder): CEncoder(\n",
       "        (net): Sequential(\n",
       "          (0): Conv2d(1, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU()\n",
       "          (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "          (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (5): ReLU()\n",
       "          (6): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "          (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (8): ReLU()\n",
       "          (9): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "          (10): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (11): ReLU()\n",
       "          (12): Flatten(start_dim=1, end_dim=-1)\n",
       "        )\n",
       "        (h1): Linear(in_features=256, out_features=64, bias=True)\n",
       "        (h2): Linear(in_features=256, out_features=64, bias=True)\n",
       "      )\n",
       "      (classifier): Sequential(\n",
       "        (0): Linear(in_features=64, out_features=32, bias=True)\n",
       "        (1): ReLU()\n",
       "        (2): Linear(in_features=32, out_features=5, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_path = '../logs/DELPO_omniglot_5-way_1-shot_15-queries/exp1/model_500.pt'\n",
    "model_name = '../logs/DELPO_omniglot_5-way_1-shot_15-queries/exp1/opt_500.pt'\n",
    "\n",
    "dict_s = torch.load(model_path).state_dict()\n",
    "learner.load_state_dict(dict_s)\n",
    "learner = learner.to('cuda')\n",
    "opt.load_state_dict(torch.load('./opt.pt'))\n",
    "learner.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('module.encoder.net.0.weight',\n",
       "              tensor([[[[-0.3042, -0.3240,  0.0983],\n",
       "                        [ 0.1889,  0.1973, -0.2662],\n",
       "                        [-0.2694, -0.1842, -0.1137]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.1531,  0.2689, -0.0521],\n",
       "                        [-0.1819,  0.0761, -0.0442],\n",
       "                        [ 0.0893,  0.3041, -0.2767]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.0046, -0.2908, -0.0801],\n",
       "                        [-0.2227, -0.1329,  0.1747],\n",
       "                        [-0.2689, -0.2431,  0.1932]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.3314, -0.0482,  0.2038],\n",
       "                        [ 0.2654,  0.0167, -0.1711],\n",
       "                        [ 0.2608,  0.0654, -0.2195]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.2262,  0.0568, -0.3043],\n",
       "                        [-0.1639, -0.0998,  0.0619],\n",
       "                        [-0.2274, -0.1743, -0.2025]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.2084, -0.0577, -0.0256],\n",
       "                        [-0.2084, -0.2400, -0.2591],\n",
       "                        [-0.0486, -0.1634, -0.2999]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.1800,  0.0343, -0.2651],\n",
       "                        [-0.3233, -0.2984, -0.1459],\n",
       "                        [-0.1730, -0.0635,  0.2135]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.0830,  0.0499, -0.3225],\n",
       "                        [ 0.3292,  0.3305,  0.2288],\n",
       "                        [-0.1260,  0.2929,  0.2055]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.2401, -0.2335,  0.1642],\n",
       "                        [ 0.1503, -0.0114,  0.0910],\n",
       "                        [ 0.3186, -0.3035,  0.2064]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.1411, -0.2601, -0.3090],\n",
       "                        [-0.1693, -0.0539,  0.0979],\n",
       "                        [ 0.0018, -0.0599,  0.1158]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.1981,  0.2660,  0.0511],\n",
       "                        [ 0.2585, -0.1521, -0.2067],\n",
       "                        [-0.2448,  0.0059,  0.2732]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.2812,  0.0140,  0.2497],\n",
       "                        [-0.2536,  0.1371, -0.2626],\n",
       "                        [-0.2134,  0.0763, -0.0289]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.1850, -0.2484,  0.0612],\n",
       "                        [-0.0749, -0.2507,  0.1842],\n",
       "                        [-0.1469,  0.0450, -0.2546]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.0822, -0.3279,  0.1708],\n",
       "                        [ 0.2481, -0.1305,  0.2766],\n",
       "                        [ 0.2957,  0.0679,  0.2385]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.2996, -0.1367,  0.0588],\n",
       "                        [-0.0074, -0.0821,  0.1787],\n",
       "                        [ 0.2464,  0.2513, -0.1783]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.0467, -0.0415,  0.0552],\n",
       "                        [ 0.2115,  0.3371, -0.0508],\n",
       "                        [ 0.0467,  0.2592, -0.0231]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.2926, -0.2932,  0.0276],\n",
       "                        [-0.0489, -0.1620, -0.2353],\n",
       "                        [ 0.1399,  0.0802, -0.2265]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.1296, -0.0445, -0.1037],\n",
       "                        [-0.3000,  0.0463,  0.3057],\n",
       "                        [-0.2083,  0.2345,  0.2882]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.2635, -0.0926, -0.0101],\n",
       "                        [-0.0461, -0.2151,  0.2205],\n",
       "                        [ 0.1035, -0.0760, -0.0775]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.1690,  0.0762,  0.0770],\n",
       "                        [ 0.3183, -0.2743, -0.1609],\n",
       "                        [-0.1453, -0.2862,  0.2908]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.1765,  0.2017, -0.2235],\n",
       "                        [ 0.1317,  0.0812, -0.1540],\n",
       "                        [-0.0469, -0.0128, -0.3126]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.0700,  0.2305, -0.2691],\n",
       "                        [-0.1384, -0.2773,  0.3299],\n",
       "                        [ 0.2351,  0.0498, -0.1708]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.3345,  0.1485, -0.3157],\n",
       "                        [ 0.3020, -0.2096,  0.2778],\n",
       "                        [-0.1521, -0.0992, -0.1240]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.1492, -0.1527,  0.1242],\n",
       "                        [ 0.1215,  0.2998,  0.2908],\n",
       "                        [-0.1592,  0.1929,  0.2621]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.0052,  0.0288,  0.2168],\n",
       "                        [ 0.1907,  0.2639,  0.0145],\n",
       "                        [-0.3182, -0.0798, -0.0224]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.0736, -0.1997,  0.1197],\n",
       "                        [-0.1993,  0.0080,  0.0761],\n",
       "                        [-0.2236, -0.3313,  0.1842]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.1707,  0.0769,  0.3003],\n",
       "                        [ 0.2791, -0.2112, -0.3320],\n",
       "                        [-0.1116, -0.0207,  0.1802]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.1574,  0.2774, -0.0374],\n",
       "                        [ 0.3240, -0.0900,  0.2569],\n",
       "                        [-0.2768,  0.2140, -0.0614]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.3336, -0.1491, -0.2263],\n",
       "                        [-0.1021,  0.0650, -0.1307],\n",
       "                        [-0.2028,  0.1548,  0.1829]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.1194, -0.1935,  0.0526],\n",
       "                        [ 0.2539, -0.0255, -0.2102],\n",
       "                        [ 0.2512,  0.2328,  0.3326]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.2154,  0.1650, -0.2675],\n",
       "                        [-0.2275,  0.0420, -0.3336],\n",
       "                        [-0.0497,  0.2117,  0.3049]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.0347, -0.0564,  0.0078],\n",
       "                        [-0.1142,  0.2047, -0.0676],\n",
       "                        [-0.2669,  0.3207,  0.2593]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.0474,  0.0784, -0.3229],\n",
       "                        [ 0.2935, -0.1378,  0.2789],\n",
       "                        [ 0.1423, -0.1820, -0.2901]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.2901, -0.0836,  0.2309],\n",
       "                        [-0.0546,  0.2204, -0.0105],\n",
       "                        [ 0.2634, -0.0669,  0.2987]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.1672,  0.3022, -0.1359],\n",
       "                        [ 0.3033, -0.2216,  0.1849],\n",
       "                        [ 0.0362,  0.1925, -0.0706]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.2640, -0.0778, -0.1891],\n",
       "                        [-0.0820, -0.1584, -0.0127],\n",
       "                        [-0.3138,  0.1557, -0.1322]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.2546, -0.0132,  0.3172],\n",
       "                        [ 0.2813,  0.1596, -0.0627],\n",
       "                        [ 0.0184,  0.0696, -0.2668]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.0427,  0.2544,  0.0599],\n",
       "                        [ 0.1233, -0.2283, -0.2728],\n",
       "                        [ 0.0205,  0.0149, -0.0696]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.0814, -0.1871,  0.0901],\n",
       "                        [ 0.0336, -0.3354,  0.2276],\n",
       "                        [ 0.3102, -0.1029, -0.1262]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.2011, -0.0497, -0.2863],\n",
       "                        [-0.3110,  0.0610, -0.0159],\n",
       "                        [-0.0997, -0.2285,  0.2074]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.1472,  0.0789, -0.0608],\n",
       "                        [-0.1459, -0.0658, -0.2852],\n",
       "                        [ 0.0371, -0.2265,  0.0418]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.2088, -0.0272,  0.3135],\n",
       "                        [ 0.3046,  0.2351, -0.1260],\n",
       "                        [ 0.3050, -0.0886, -0.0484]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.1693,  0.2540, -0.0238],\n",
       "                        [-0.3116,  0.2976,  0.2287],\n",
       "                        [-0.1194, -0.1151, -0.1382]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.3205, -0.2046,  0.2427],\n",
       "                        [ 0.2559, -0.0037, -0.2730],\n",
       "                        [-0.2076,  0.0353, -0.2335]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.1603,  0.1508,  0.0819],\n",
       "                        [-0.0270,  0.1154, -0.2650],\n",
       "                        [-0.0465, -0.3212,  0.1192]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.1476, -0.0592,  0.1376],\n",
       "                        [ 0.2991,  0.1914,  0.1168],\n",
       "                        [ 0.0990, -0.0548,  0.1296]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.1668,  0.2910,  0.2651],\n",
       "                        [ 0.2412, -0.0841, -0.0035],\n",
       "                        [-0.2898, -0.3139,  0.2873]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.1853,  0.2325,  0.0375],\n",
       "                        [-0.0638,  0.0287, -0.0790],\n",
       "                        [ 0.1764, -0.1890,  0.0143]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.1055, -0.2464, -0.0951],\n",
       "                        [-0.1974,  0.0245, -0.1894],\n",
       "                        [ 0.1927,  0.2105,  0.1251]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.1124, -0.1978, -0.2353],\n",
       "                        [ 0.3154, -0.0028,  0.0208],\n",
       "                        [ 0.2221, -0.0707,  0.0470]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.2493,  0.1494, -0.2490],\n",
       "                        [ 0.2312,  0.3006,  0.2747],\n",
       "                        [-0.2636, -0.2654, -0.2637]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.1477,  0.1938, -0.3372],\n",
       "                        [-0.0194, -0.2651,  0.3273],\n",
       "                        [-0.1474, -0.2210,  0.2939]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.1377, -0.0974, -0.0574],\n",
       "                        [ 0.0113,  0.2371,  0.2759],\n",
       "                        [ 0.1616,  0.0148, -0.2610]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.1268, -0.2695,  0.3090],\n",
       "                        [-0.3397,  0.2691, -0.2822],\n",
       "                        [ 0.0295, -0.0258,  0.0236]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.0655, -0.2860,  0.1033],\n",
       "                        [ 0.0051,  0.0062,  0.3177],\n",
       "                        [ 0.1896, -0.3026,  0.0137]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.3019,  0.2691, -0.1478],\n",
       "                        [-0.1433, -0.2771,  0.0098],\n",
       "                        [ 0.1527,  0.1918, -0.2454]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.2906, -0.1072,  0.1942],\n",
       "                        [ 0.1407,  0.1688,  0.1278],\n",
       "                        [-0.2025,  0.2003, -0.1434]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.2491,  0.0965, -0.0841],\n",
       "                        [-0.0554, -0.2754, -0.2346],\n",
       "                        [-0.2772,  0.1367, -0.1543]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.2544,  0.2717, -0.1511],\n",
       "                        [-0.3167,  0.2982, -0.3054],\n",
       "                        [ 0.2517,  0.1613,  0.0762]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.1795, -0.1230,  0.1422],\n",
       "                        [ 0.2271,  0.0080,  0.1955],\n",
       "                        [-0.1509, -0.2708,  0.2106]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.1187,  0.2304,  0.0474],\n",
       "                        [ 0.0942, -0.2733, -0.1604],\n",
       "                        [ 0.1713, -0.0103,  0.3086]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.1147, -0.2862, -0.2354],\n",
       "                        [ 0.0136, -0.2657, -0.2802],\n",
       "                        [ 0.0560,  0.0537, -0.1840]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.0874,  0.0845, -0.3230],\n",
       "                        [-0.1215, -0.0460,  0.1390],\n",
       "                        [ 0.1494,  0.2567, -0.0678]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.0967, -0.2025, -0.3254],\n",
       "                        [ 0.1306, -0.0354,  0.0469],\n",
       "                        [-0.1156,  0.2473,  0.2989]]]])),\n",
       "             ('module.encoder.net.0.bias',\n",
       "              tensor([-0.2076, -0.1491,  0.2511,  0.2898, -0.2622,  0.0258, -0.0989,  0.1470,\n",
       "                      -0.2095, -0.1323, -0.2582,  0.2880,  0.2915, -0.0799, -0.1268, -0.2792,\n",
       "                       0.2234, -0.1814,  0.1844, -0.2114,  0.2908, -0.1087, -0.0835,  0.2511,\n",
       "                      -0.3253,  0.2950, -0.1023, -0.1946, -0.1653, -0.1428,  0.0417, -0.1041,\n",
       "                       0.0317, -0.2834, -0.0286,  0.1900, -0.3118, -0.1666, -0.2815,  0.3212,\n",
       "                      -0.0789, -0.2948,  0.2386, -0.0582, -0.2373,  0.2038, -0.3166,  0.1054,\n",
       "                      -0.2136, -0.0769,  0.3355, -0.2875,  0.0278,  0.0456, -0.1571,  0.2859,\n",
       "                      -0.0190, -0.1680,  0.0646,  0.0935, -0.1734, -0.1597,  0.0616, -0.0292])),\n",
       "             ('module.encoder.net.1.weight',\n",
       "              tensor([1.0376, 0.9990, 0.9990, 0.9959, 1.0246, 1.0337, 1.0018, 1.0114, 1.0044,\n",
       "                      1.0080, 0.9905, 0.9957, 1.0126, 0.9913, 0.9960, 1.0006, 1.0471, 0.9861,\n",
       "                      0.9947, 0.9935, 0.9966, 0.9963, 1.0106, 1.0006, 1.0086, 1.0054, 0.9859,\n",
       "                      0.9998, 1.0089, 0.9983, 0.9975, 0.9929, 0.9964, 0.9893, 1.0151, 1.0662,\n",
       "                      0.9991, 0.9968, 0.9884, 1.0080, 1.0941, 1.0017, 1.0062, 0.9896, 0.9991,\n",
       "                      1.0003, 1.0039, 0.9955, 0.9928, 1.0063, 1.0005, 0.9893, 1.0063, 0.9894,\n",
       "                      0.9938, 1.0091, 0.9994, 0.9974, 1.0127, 0.9824, 1.0052, 1.0280, 0.9935,\n",
       "                      0.9646])),\n",
       "             ('module.encoder.net.1.bias',\n",
       "              tensor([ 0.0381, -0.0068,  0.0072,  0.0056,  0.0268,  0.0517,  0.0075,  0.0209,\n",
       "                       0.0112,  0.0339,  0.0168,  0.0028,  0.0204, -0.0058,  0.0075,  0.0082,\n",
       "                       0.0530,  0.0040,  0.0053,  0.0187, -0.0132, -0.0259,  0.0256,  0.0111,\n",
       "                       0.0126,  0.0137, -0.0049,  0.0147,  0.0123,  0.0057,  0.0221, -0.0011,\n",
       "                       0.0008,  0.0115,  0.0251,  0.0682,  0.0215, -0.0036, -0.0045,  0.0149,\n",
       "                       0.0810,  0.0175, -0.0024, -0.0159,  0.0060,  0.0139,  0.0140,  0.0079,\n",
       "                      -0.0016,  0.0048,  0.0055, -0.0061,  0.0186, -0.0036, -0.0026,  0.0242,\n",
       "                       0.0062,  0.0118,  0.0012,  0.0205,  0.0136,  0.0301, -0.0107, -0.0008])),\n",
       "             ('module.encoder.net.1.running_mean',\n",
       "              tensor([-2.9335e-01, -1.4737e-01,  1.7809e-01,  3.5087e-01, -3.7308e-01,\n",
       "                      -6.6517e-02, -1.6878e-01,  2.3574e-01, -1.5529e-01, -1.7437e-01,\n",
       "                      -2.5221e-01,  2.8825e-01,  2.1664e-01, -3.0074e-05, -7.1002e-02,\n",
       "                      -2.0926e-01,  1.3774e-01, -1.5094e-01,  1.9197e-01, -2.3345e-01,\n",
       "                       2.7708e-01, -1.1294e-01, -1.2938e-01,  3.4651e-01, -3.0226e-01,\n",
       "                       2.5180e-01, -9.9370e-02, -1.5804e-01, -2.2995e-01, -9.2346e-02,\n",
       "                       4.7721e-02, -7.5099e-02,  1.3948e-02, -2.4066e-01,  3.6220e-02,\n",
       "                       9.6941e-02, -2.9205e-01, -1.7179e-01, -2.9704e-01,  2.4205e-01,\n",
       "                      -1.4395e-01, -2.3981e-01,  2.3361e-01, -6.3105e-02, -2.6870e-01,\n",
       "                       2.6285e-01, -2.9555e-01,  1.3559e-01, -2.3613e-01, -8.4126e-02,\n",
       "                       3.0160e-01, -3.1427e-01,  6.4127e-02,  2.4232e-02, -1.4757e-01,\n",
       "                       2.4326e-01, -1.3252e-02, -2.1790e-01,  6.7846e-02,  1.3022e-01,\n",
       "                      -1.2464e-01, -2.6564e-01,  5.5760e-02, -1.9116e-02])),\n",
       "             ('module.encoder.net.1.running_var',\n",
       "              tensor([0.0302, 0.0081, 0.0364, 0.0341, 0.0476, 0.0503, 0.0321, 0.0499, 0.0183,\n",
       "                      0.0151, 0.0044, 0.0099, 0.0266, 0.0355, 0.0164, 0.0275, 0.0385, 0.0239,\n",
       "                      0.0048, 0.0087, 0.0193, 0.0032, 0.0105, 0.0482, 0.0131, 0.0199, 0.0044,\n",
       "                      0.0117, 0.0299, 0.0265, 0.0119, 0.0158, 0.0134, 0.0203, 0.0217, 0.0360,\n",
       "                      0.0106, 0.0087, 0.0102, 0.0321, 0.0196, 0.0217, 0.0164, 0.0165, 0.0077,\n",
       "                      0.0192, 0.0193, 0.0090, 0.0166, 0.0156, 0.0157, 0.0140, 0.0112, 0.0054,\n",
       "                      0.0075, 0.0123, 0.0101, 0.0216, 0.0153, 0.0134, 0.0122, 0.0595, 0.0081,\n",
       "                      0.0160])),\n",
       "             ('module.encoder.net.1.num_batches_tracked', tensor(0)),\n",
       "             ('module.encoder.net.3.weight',\n",
       "              tensor([[[[ 0.0171, -0.0184,  0.0343],\n",
       "                        [-0.0413, -0.0017, -0.0087],\n",
       "                        [ 0.0255,  0.0090, -0.0232]],\n",
       "              \n",
       "                       [[-0.0340,  0.0380,  0.0199],\n",
       "                        [-0.0359, -0.0120, -0.0218],\n",
       "                        [ 0.0254, -0.0253,  0.0209]],\n",
       "              \n",
       "                       [[-0.0208, -0.0153,  0.0015],\n",
       "                        [ 0.0155,  0.0102, -0.0101],\n",
       "                        [ 0.0042, -0.0065, -0.0334]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-0.0233,  0.0237,  0.0373],\n",
       "                        [ 0.0109, -0.0442,  0.0233],\n",
       "                        [ 0.0403, -0.0197, -0.0226]],\n",
       "              \n",
       "                       [[ 0.0417, -0.0023,  0.0004],\n",
       "                        [ 0.0220,  0.0259,  0.0075],\n",
       "                        [-0.0100,  0.0468,  0.0248]],\n",
       "              \n",
       "                       [[-0.0031, -0.0089, -0.0207],\n",
       "                        [-0.0152, -0.0243, -0.0234],\n",
       "                        [-0.0114, -0.0234,  0.0034]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.0305, -0.0153,  0.0277],\n",
       "                        [-0.0174, -0.0075, -0.0081],\n",
       "                        [-0.0224,  0.0192, -0.0046]],\n",
       "              \n",
       "                       [[ 0.0263, -0.0034,  0.0304],\n",
       "                        [ 0.0339,  0.0106,  0.0328],\n",
       "                        [ 0.0375, -0.0191,  0.0145]],\n",
       "              \n",
       "                       [[ 0.0183, -0.0404,  0.0189],\n",
       "                        [-0.0264,  0.0164,  0.0317],\n",
       "                        [ 0.0059,  0.0228,  0.0401]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 0.0080, -0.0352, -0.0007],\n",
       "                        [ 0.0219, -0.0236,  0.0345],\n",
       "                        [ 0.0221, -0.0446,  0.0036]],\n",
       "              \n",
       "                       [[ 0.0399, -0.0154,  0.0237],\n",
       "                        [ 0.0414,  0.0252,  0.0473],\n",
       "                        [ 0.0235,  0.0298,  0.0333]],\n",
       "              \n",
       "                       [[-0.0118, -0.0047,  0.0291],\n",
       "                        [ 0.0338,  0.0246,  0.0245],\n",
       "                        [-0.0123, -0.0003, -0.0097]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.0056,  0.0240,  0.0457],\n",
       "                        [ 0.0464, -0.0278,  0.0410],\n",
       "                        [-0.0157,  0.0435,  0.0376]],\n",
       "              \n",
       "                       [[ 0.0330, -0.0218,  0.0377],\n",
       "                        [-0.0345, -0.0368,  0.0256],\n",
       "                        [-0.0197, -0.0278,  0.0175]],\n",
       "              \n",
       "                       [[ 0.0404, -0.0153, -0.0012],\n",
       "                        [ 0.0173, -0.0094, -0.0336],\n",
       "                        [ 0.0360, -0.0264, -0.0141]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-0.0043,  0.0323,  0.0337],\n",
       "                        [ 0.0028,  0.0459,  0.0156],\n",
       "                        [ 0.0344,  0.0160,  0.0182]],\n",
       "              \n",
       "                       [[-0.0244, -0.0200, -0.0154],\n",
       "                        [ 0.0098,  0.0053, -0.0111],\n",
       "                        [ 0.0155,  0.0309,  0.0264]],\n",
       "              \n",
       "                       [[-0.0116,  0.0078, -0.0417],\n",
       "                        [ 0.0175,  0.0096, -0.0335],\n",
       "                        [ 0.0276,  0.0280, -0.0312]]],\n",
       "              \n",
       "              \n",
       "                      ...,\n",
       "              \n",
       "              \n",
       "                      [[[ 0.0263,  0.0037, -0.0406],\n",
       "                        [ 0.0067, -0.0044, -0.0352],\n",
       "                        [ 0.0196, -0.0122, -0.0034]],\n",
       "              \n",
       "                       [[ 0.0031,  0.0224, -0.0034],\n",
       "                        [-0.0387,  0.0048, -0.0143],\n",
       "                        [-0.0163,  0.0342,  0.0121]],\n",
       "              \n",
       "                       [[ 0.0175, -0.0216,  0.0208],\n",
       "                        [ 0.0241, -0.0230, -0.0134],\n",
       "                        [ 0.0233, -0.0169,  0.0177]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 0.0414, -0.0171,  0.0312],\n",
       "                        [ 0.0269,  0.0214, -0.0286],\n",
       "                        [ 0.0087, -0.0447, -0.0268]],\n",
       "              \n",
       "                       [[-0.0265,  0.0121, -0.0370],\n",
       "                        [-0.0182,  0.0284, -0.0043],\n",
       "                        [ 0.0030,  0.0411, -0.0362]],\n",
       "              \n",
       "                       [[ 0.0353,  0.0261, -0.0090],\n",
       "                        [-0.0224, -0.0282, -0.0408],\n",
       "                        [ 0.0200,  0.0263,  0.0329]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.0192,  0.0341,  0.0172],\n",
       "                        [ 0.0125,  0.0127, -0.0293],\n",
       "                        [ 0.0334,  0.0209, -0.0022]],\n",
       "              \n",
       "                       [[-0.0288,  0.0091, -0.0431],\n",
       "                        [-0.0294,  0.0067, -0.0267],\n",
       "                        [ 0.0102, -0.0231, -0.0102]],\n",
       "              \n",
       "                       [[ 0.0144, -0.0086, -0.0277],\n",
       "                        [-0.0410,  0.0437, -0.0104],\n",
       "                        [ 0.0365, -0.0246,  0.0188]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 0.0426, -0.0274, -0.0349],\n",
       "                        [-0.0090,  0.0207, -0.0061],\n",
       "                        [ 0.0004,  0.0347, -0.0029]],\n",
       "              \n",
       "                       [[-0.0328, -0.0369,  0.0225],\n",
       "                        [ 0.0068,  0.0241,  0.0381],\n",
       "                        [-0.0251, -0.0302, -0.0030]],\n",
       "              \n",
       "                       [[ 0.0271, -0.0386,  0.0237],\n",
       "                        [ 0.0122, -0.0180, -0.0352],\n",
       "                        [ 0.0268, -0.0054,  0.0248]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.0165,  0.0093,  0.0036],\n",
       "                        [-0.0172, -0.0026, -0.0009],\n",
       "                        [ 0.0094,  0.0107, -0.0469]],\n",
       "              \n",
       "                       [[ 0.0265, -0.0035,  0.0104],\n",
       "                        [ 0.0525, -0.0051, -0.0121],\n",
       "                        [-0.0216,  0.0416, -0.0006]],\n",
       "              \n",
       "                       [[ 0.0112,  0.0085, -0.0330],\n",
       "                        [-0.0150,  0.0039, -0.0384],\n",
       "                        [-0.0258, -0.0059,  0.0376]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-0.0190, -0.0242, -0.0431],\n",
       "                        [-0.0186, -0.0404,  0.0032],\n",
       "                        [ 0.0193, -0.0466,  0.0294]],\n",
       "              \n",
       "                       [[ 0.0095, -0.0395,  0.0287],\n",
       "                        [ 0.0092,  0.0455,  0.0097],\n",
       "                        [-0.0128, -0.0174,  0.0369]],\n",
       "              \n",
       "                       [[ 0.0451,  0.0054,  0.0226],\n",
       "                        [-0.0024, -0.0287,  0.0263],\n",
       "                        [-0.0153,  0.0027,  0.0506]]]])),\n",
       "             ('module.encoder.net.3.bias',\n",
       "              tensor([-3.6231e-02,  2.1929e-05,  3.0502e-02,  4.5237e-02, -3.0393e-02,\n",
       "                       4.0287e-03,  6.8068e-03,  3.0016e-02, -1.1197e-02, -3.7708e-02,\n",
       "                      -3.9872e-02, -1.9759e-02,  8.7803e-03, -3.2847e-02, -3.7157e-02,\n",
       "                       2.4796e-03,  1.1513e-02, -2.0793e-02,  3.7930e-02,  3.1503e-02,\n",
       "                      -7.5431e-03, -1.4833e-02, -1.4943e-03,  4.8916e-03, -2.5927e-02,\n",
       "                      -8.4778e-03,  2.9546e-02,  2.5422e-02,  1.6007e-02,  1.9493e-02,\n",
       "                       1.6603e-02,  3.2073e-02, -1.1522e-02,  2.5896e-02, -2.1811e-02,\n",
       "                      -1.3062e-02,  4.0379e-02,  3.2928e-02, -2.9993e-02, -3.6129e-02,\n",
       "                      -1.1400e-02, -3.4948e-02, -4.5066e-03, -3.5641e-02,  1.0117e-02,\n",
       "                       3.4265e-03, -3.4061e-02,  1.6865e-02,  1.9599e-02,  1.2220e-02,\n",
       "                       1.5695e-02, -4.6791e-03, -2.4592e-02, -3.0996e-03, -3.0928e-02,\n",
       "                       1.8827e-03, -2.7740e-03,  3.7435e-04, -9.1743e-03, -3.6483e-02,\n",
       "                       6.2939e-03,  2.1593e-02,  2.0345e-02,  3.2721e-02])),\n",
       "             ('module.encoder.net.4.weight',\n",
       "              tensor([0.9906, 0.9911, 1.0117, 1.0362, 1.0301, 1.0440, 0.9969, 0.9885, 0.9774,\n",
       "                      1.0246, 1.0245, 1.0518, 0.9818, 0.9944, 0.9840, 0.9954, 1.0008, 1.0273,\n",
       "                      1.0178, 0.9917, 1.0129, 1.0129, 1.0449, 1.0130, 0.9851, 1.1117, 0.9834,\n",
       "                      1.0546, 1.0038, 0.9883, 1.0108, 1.0705, 1.0017, 0.9975, 0.9911, 1.0295,\n",
       "                      0.9878, 0.9965, 0.9863, 0.9897, 0.9935, 1.0005, 0.9845, 0.9642, 1.0070,\n",
       "                      0.9648, 0.9963, 0.9681, 1.0141, 1.0197, 1.0035, 1.0073, 0.9791, 0.9966,\n",
       "                      1.0149, 0.9713, 1.0125, 1.0153, 0.9850, 0.9899, 1.0022, 0.9860, 1.0145,\n",
       "                      0.9852])),\n",
       "             ('module.encoder.net.4.bias',\n",
       "              tensor([ 0.0323,  0.0369,  0.0728,  0.0497,  0.0598,  0.0392,  0.0191,  0.0135,\n",
       "                       0.0435,  0.0504,  0.1456,  0.0756, -0.0016,  0.0136,  0.0321,  0.0148,\n",
       "                       0.0311,  0.0355,  0.0771, -0.0019,  0.0174,  0.0826,  0.0506,  0.0252,\n",
       "                       0.0275,  0.0843,  0.0082,  0.0539,  0.0309,  0.0096,  0.0579,  0.0401,\n",
       "                       0.0262,  0.0490,  0.0200,  0.0325,  0.0172,  0.0167,  0.0126,  0.0167,\n",
       "                       0.0199,  0.0165,  0.0083,  0.0184,  0.0583,  0.0067,  0.0230, -0.0098,\n",
       "                       0.0357,  0.0268,  0.0308,  0.0293,  0.0138,  0.0212,  0.0791,  0.0187,\n",
       "                       0.0339,  0.0319,  0.0052,  0.0231,  0.0269,  0.0030,  0.0624,  0.0034])),\n",
       "             ('module.encoder.net.4.running_mean',\n",
       "              tensor([ 0.8784,  0.5380, -0.2960,  0.4406, -0.6445, -0.4960,  0.1481,  0.4387,\n",
       "                       0.6118,  0.1557, -0.1110, -0.6371,  0.4884,  0.4008,  0.6516,  0.3697,\n",
       "                      -0.3566, -0.1576, -0.4015,  0.2838,  0.0830, -0.0932, -0.0566, -0.3257,\n",
       "                       0.7108,  0.5399,  0.2036,  0.4444,  0.2261,  0.4843,  0.2231, -0.3242,\n",
       "                       0.1677,  0.4038,  0.3366, -0.3989,  0.5883, -0.2672,  0.0955, -0.3302,\n",
       "                      -0.6210,  0.4751,  0.7041,  0.6357, -0.2078,  0.3112,  0.4489,  0.3848,\n",
       "                      -0.2750,  0.2844, -0.5328,  0.1336,  0.3068,  0.7818, -0.4618,  0.2891,\n",
       "                      -0.3708, -0.7600,  0.3152,  0.0907,  0.5046,  0.3649, -0.3308,  0.4188])),\n",
       "             ('module.encoder.net.4.running_var',\n",
       "              tensor([0.9744, 1.2081, 1.2042, 0.5737, 1.6740, 2.7132, 0.6070, 1.0089, 1.8012,\n",
       "                      0.4751, 0.7536, 0.8807, 0.5622, 0.5388, 2.0878, 0.8123, 0.8564, 0.7291,\n",
       "                      1.1113, 1.0687, 1.0345, 1.1641, 1.0615, 0.5630, 1.3712, 0.6609, 0.4274,\n",
       "                      0.5187, 0.5534, 0.7480, 0.9193, 0.5414, 0.3250, 0.4294, 1.1769, 1.0114,\n",
       "                      1.4416, 0.7260, 0.6141, 0.6023, 1.6708, 1.4788, 1.4545, 1.0311, 1.0887,\n",
       "                      0.6063, 1.4921, 0.7697, 0.8606, 0.4344, 0.9193, 0.4027, 0.9777, 1.1937,\n",
       "                      0.6624, 0.9114, 1.2744, 0.8628, 0.6208, 0.4342, 1.6102, 0.6396, 0.8572,\n",
       "                      1.1207])),\n",
       "             ('module.encoder.net.4.num_batches_tracked', tensor(0)),\n",
       "             ('module.encoder.net.6.weight',\n",
       "              tensor([[[[ 4.1922e-02,  1.6323e-02, -1.9784e-02],\n",
       "                        [ 2.4669e-02,  4.4957e-02, -1.7513e-02],\n",
       "                        [ 2.6848e-03, -1.7471e-02, -2.0042e-02]],\n",
       "              \n",
       "                       [[ 1.0387e-02,  2.0537e-02,  1.7225e-02],\n",
       "                        [-1.1026e-02,  2.8165e-02, -6.5323e-03],\n",
       "                        [-2.0206e-02,  2.9966e-02,  4.5357e-02]],\n",
       "              \n",
       "                       [[-1.4089e-02,  3.7378e-02, -6.8799e-03],\n",
       "                        [ 1.2731e-02,  2.4878e-03, -3.9412e-03],\n",
       "                        [-9.8800e-03,  9.1753e-03, -1.2540e-02]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 7.6749e-04,  4.9099e-02,  3.8223e-02],\n",
       "                        [ 2.7806e-02,  2.9270e-02, -2.0343e-02],\n",
       "                        [ 2.6846e-02,  2.8850e-03, -4.7232e-03]],\n",
       "              \n",
       "                       [[-1.5086e-02,  2.3815e-02,  2.3818e-02],\n",
       "                        [ 2.0750e-02,  2.1598e-02, -2.6761e-02],\n",
       "                        [-1.2152e-02, -2.9730e-02,  4.7715e-02]],\n",
       "              \n",
       "                       [[ 3.2357e-02,  9.1753e-03,  3.2958e-02],\n",
       "                        [-3.3519e-02,  1.6647e-02, -1.9357e-02],\n",
       "                        [ 3.6937e-02,  2.6661e-02, -2.5358e-03]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 7.7304e-03,  2.4860e-02, -2.2437e-02],\n",
       "                        [-3.1356e-03,  3.1769e-02, -1.4872e-02],\n",
       "                        [-1.5537e-02,  1.6582e-03,  1.4845e-02]],\n",
       "              \n",
       "                       [[ 1.3697e-02,  2.8542e-02, -3.8799e-02],\n",
       "                        [-2.7886e-03, -9.6990e-03, -1.2719e-02],\n",
       "                        [ 2.9039e-02,  1.7569e-02,  2.8035e-02]],\n",
       "              \n",
       "                       [[-4.6903e-02, -4.8484e-03,  2.2819e-02],\n",
       "                        [-1.5942e-02,  2.9450e-02,  1.3036e-02],\n",
       "                        [-1.0993e-02, -8.9105e-03, -1.7970e-03]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 2.2264e-02, -4.1080e-02, -4.3066e-03],\n",
       "                        [-1.8862e-02,  2.0081e-03, -1.4605e-02],\n",
       "                        [ 5.2789e-03,  1.5515e-02,  3.6543e-03]],\n",
       "              \n",
       "                       [[ 1.9026e-02,  4.7274e-03, -3.6383e-02],\n",
       "                        [ 2.5912e-02,  4.5742e-02,  1.5023e-02],\n",
       "                        [-3.3493e-02,  4.5099e-02,  3.2178e-02]],\n",
       "              \n",
       "                       [[-2.4114e-02, -2.7123e-02, -2.7134e-02],\n",
       "                        [ 1.3643e-02, -4.1954e-02,  7.7107e-03],\n",
       "                        [ 3.4223e-03,  2.8874e-02,  9.5260e-03]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 1.3594e-02, -2.2638e-02, -7.5532e-03],\n",
       "                        [ 4.0921e-02, -3.7419e-03,  1.2724e-02],\n",
       "                        [ 4.0279e-02,  3.9869e-02, -1.9028e-02]],\n",
       "              \n",
       "                       [[-2.3612e-02, -9.8447e-03,  8.5141e-03],\n",
       "                        [-1.5330e-02, -1.7701e-03,  2.1892e-02],\n",
       "                        [ 4.6548e-02, -1.5383e-02,  4.2285e-02]],\n",
       "              \n",
       "                       [[ 2.2851e-02,  9.7557e-03,  3.8486e-02],\n",
       "                        [-2.7977e-03,  1.2255e-02, -1.9654e-02],\n",
       "                        [-1.4565e-02, -2.3622e-02, -3.1786e-03]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 1.7836e-02,  2.3695e-02,  2.4574e-02],\n",
       "                        [ 4.6924e-02,  2.9670e-02,  2.3243e-03],\n",
       "                        [ 1.6773e-03, -1.8278e-02,  7.4426e-05]],\n",
       "              \n",
       "                       [[ 3.1658e-02,  2.5439e-02,  1.6289e-02],\n",
       "                        [ 9.5242e-03,  3.5913e-02, -2.0519e-02],\n",
       "                        [-2.4687e-03, -1.2524e-02,  2.7202e-02]],\n",
       "              \n",
       "                       [[-2.5506e-02,  7.1749e-03,  9.4420e-03],\n",
       "                        [ 3.6579e-02,  1.5932e-02, -1.3998e-02],\n",
       "                        [ 5.2365e-03,  2.6250e-02,  4.8854e-02]]],\n",
       "              \n",
       "              \n",
       "                      ...,\n",
       "              \n",
       "              \n",
       "                      [[[ 1.1051e-02,  4.7058e-03, -4.8670e-02],\n",
       "                        [-1.5625e-02,  1.5516e-02, -2.2578e-02],\n",
       "                        [ 2.2156e-02,  1.1190e-02, -1.3084e-02]],\n",
       "              \n",
       "                       [[-2.0987e-03,  2.8691e-02,  1.9821e-02],\n",
       "                        [ 2.8948e-02,  4.3419e-02,  3.0548e-02],\n",
       "                        [-2.9044e-02, -1.3023e-02, -4.0363e-03]],\n",
       "              \n",
       "                       [[ 8.6396e-03,  2.8342e-02, -5.2843e-03],\n",
       "                        [ 3.6071e-02, -9.3161e-03, -9.2641e-03],\n",
       "                        [ 4.7426e-02,  3.9241e-02, -2.8029e-02]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 2.7299e-02, -3.2907e-02,  1.0506e-02],\n",
       "                        [ 1.5431e-02,  1.9151e-02, -1.4097e-02],\n",
       "                        [ 3.8604e-03,  2.8938e-02, -3.7601e-02]],\n",
       "              \n",
       "                       [[ 9.2253e-03,  2.7534e-02, -2.5599e-02],\n",
       "                        [ 6.8406e-03, -2.0286e-02,  2.3023e-02],\n",
       "                        [-2.7511e-02,  2.2649e-03,  2.9643e-02]],\n",
       "              \n",
       "                       [[-1.0814e-02, -3.7385e-02, -1.0266e-02],\n",
       "                        [ 7.8515e-03,  2.8002e-02,  1.1865e-02],\n",
       "                        [ 2.5443e-02,  1.1245e-02,  4.0857e-02]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 1.9642e-02,  9.0468e-03,  5.3601e-03],\n",
       "                        [-5.4149e-04, -4.2590e-02, -2.1582e-03],\n",
       "                        [ 1.2621e-02,  2.5606e-02, -2.2637e-02]],\n",
       "              \n",
       "                       [[-1.3192e-02, -1.6463e-02,  2.7917e-02],\n",
       "                        [ 4.1216e-02,  2.3137e-02, -3.2747e-02],\n",
       "                        [-6.8765e-03,  2.1638e-02,  2.4772e-02]],\n",
       "              \n",
       "                       [[-2.7055e-02,  1.1027e-02,  2.6711e-02],\n",
       "                        [-1.7209e-04, -3.0953e-02, -4.0988e-03],\n",
       "                        [-2.7872e-02, -7.9796e-03,  1.3060e-02]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 1.0017e-02, -4.7599e-02, -4.9107e-02],\n",
       "                        [ 3.0522e-02, -2.8134e-02,  1.3826e-03],\n",
       "                        [-8.7048e-03, -1.6452e-02, -3.1574e-02]],\n",
       "              \n",
       "                       [[-2.7337e-02, -2.9906e-02, -1.4076e-02],\n",
       "                        [-1.7911e-02,  1.2129e-02,  3.5673e-02],\n",
       "                        [-1.1664e-02, -1.3092e-02,  5.5567e-03]],\n",
       "              \n",
       "                       [[-2.0537e-04,  3.1253e-02, -3.4138e-02],\n",
       "                        [ 3.1983e-02,  3.0546e-02, -3.2019e-02],\n",
       "                        [ 3.9913e-02, -5.8825e-04, -8.3009e-03]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 1.6265e-02,  1.4004e-02, -2.2242e-02],\n",
       "                        [-3.4667e-03, -9.6875e-03,  1.5853e-02],\n",
       "                        [ 1.0726e-02, -6.7780e-03, -2.5817e-02]],\n",
       "              \n",
       "                       [[ 1.4416e-03,  3.4526e-02,  1.9889e-02],\n",
       "                        [-1.4525e-02, -8.7446e-03, -3.9889e-03],\n",
       "                        [ 3.1751e-02, -1.9785e-02, -3.2774e-02]],\n",
       "              \n",
       "                       [[ 5.9594e-02,  1.7151e-02,  1.4143e-02],\n",
       "                        [-1.3080e-02, -3.4583e-02, -3.7027e-02],\n",
       "                        [ 4.4326e-02, -1.1449e-03,  6.7077e-03]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 9.7608e-03,  4.5658e-02, -3.9929e-03],\n",
       "                        [-1.2104e-02, -1.2944e-03, -2.7706e-02],\n",
       "                        [ 2.8293e-02,  2.7807e-04, -3.8850e-02]],\n",
       "              \n",
       "                       [[ 2.8840e-02, -4.9001e-03, -2.6724e-02],\n",
       "                        [ 1.7892e-02, -2.4608e-03, -1.5062e-02],\n",
       "                        [ 3.7145e-02, -7.3835e-03,  2.6075e-02]],\n",
       "              \n",
       "                       [[ 2.6327e-02,  1.9317e-02,  2.8963e-02],\n",
       "                        [-1.6021e-02,  3.3094e-03,  2.1440e-02],\n",
       "                        [-2.8574e-03, -1.0925e-02, -8.9811e-03]]]])),\n",
       "             ('module.encoder.net.6.bias',\n",
       "              tensor([-0.0172,  0.0070, -0.0406, -0.0124,  0.0208,  0.0226,  0.0166,  0.0235,\n",
       "                      -0.0217, -0.0349, -0.0178,  0.0201,  0.0218,  0.0339,  0.0371,  0.0123,\n",
       "                      -0.0406,  0.0279,  0.0333,  0.0117,  0.0097, -0.0378, -0.0060, -0.0159,\n",
       "                      -0.0046,  0.0208,  0.0253,  0.0365, -0.0259,  0.0034,  0.0363, -0.0076,\n",
       "                       0.0171, -0.0406, -0.0373,  0.0063, -0.0049, -0.0168, -0.0401, -0.0294,\n",
       "                      -0.0428, -0.0039, -0.0092, -0.0368, -0.0171, -0.0013,  0.0074, -0.0379,\n",
       "                       0.0164,  0.0124, -0.0244, -0.0015, -0.0337, -0.0314, -0.0246,  0.0230,\n",
       "                      -0.0172, -0.0356, -0.0326, -0.0250,  0.0372, -0.0050, -0.0416, -0.0113])),\n",
       "             ('module.encoder.net.7.weight',\n",
       "              tensor([0.9859, 1.0319, 1.0142, 1.0146, 0.9732, 1.0097, 0.9848, 0.9901, 1.0208,\n",
       "                      1.0198, 0.9672, 0.9991, 1.0134, 0.9927, 0.9916, 0.9845, 1.0083, 0.9764,\n",
       "                      0.9914, 1.0231, 0.9825, 1.0595, 0.9532, 0.9943, 0.9882, 1.0197, 0.9919,\n",
       "                      1.0057, 1.0158, 0.9900, 0.9960, 0.9995, 0.9979, 1.0286, 1.0021, 1.0389,\n",
       "                      1.0143, 1.0158, 0.9840, 0.9946, 1.0144, 1.0179, 1.0160, 0.9962, 1.0052,\n",
       "                      1.0006, 0.9994, 1.0010, 0.9922, 1.0162, 0.9698, 0.9995, 1.0205, 0.9938,\n",
       "                      1.0203, 0.9994, 1.0212, 0.9946, 1.0032, 0.9831, 0.9754, 1.0164, 0.9999,\n",
       "                      0.9914])),\n",
       "             ('module.encoder.net.7.bias',\n",
       "              tensor([ 0.0143,  0.0326,  0.0343,  0.0281,  0.0192,  0.0221,  0.0030, -0.0030,\n",
       "                       0.0265,  0.0464, -0.0290,  0.0077,  0.0364,  0.0171,  0.0021,  0.0062,\n",
       "                       0.0099, -0.0064,  0.0349,  0.0055,  0.0239,  0.0460,  0.0027,  0.0541,\n",
       "                       0.0324,  0.1062,  0.0145,  0.0402,  0.0384, -0.0014,  0.0309, -0.0086,\n",
       "                       0.0445,  0.0192,  0.0184,  0.0388,  0.0375,  0.0428, -0.0059,  0.0040,\n",
       "                       0.0471,  0.0473,  0.0436,  0.0235,  0.0193,  0.0046,  0.0670,  0.0310,\n",
       "                       0.0075,  0.0257,  0.0222,  0.0538,  0.0220,  0.0474,  0.0374,  0.0238,\n",
       "                       0.0334,  0.0360,  0.0329,  0.0028,  0.0148,  0.0204,  0.0112,  0.0060])),\n",
       "             ('module.encoder.net.7.running_mean',\n",
       "              tensor([ 0.7459, -1.1972,  1.3369, -0.3225,  0.4775,  1.0318, -0.3127, -0.8226,\n",
       "                      -0.8217,  1.5926,  0.4441, -0.4034, -0.3946,  1.4615,  0.4013, -0.3520,\n",
       "                      -0.3314,  0.6422,  1.3715, -0.7133,  1.2331, -0.5899, -0.0762,  1.4662,\n",
       "                       0.8583,  1.3976, -0.5421, -0.8452, -0.7166, -0.4669,  1.1833, -0.1940,\n",
       "                       1.3312, -1.0405, -0.7372, -0.8699, -0.6046,  1.4140, -0.4231, -0.7146,\n",
       "                       0.7308,  1.0151,  1.1476,  1.1590, -0.6335, -0.7864,  1.0342,  1.2619,\n",
       "                      -0.8007, -0.6624,  1.0250,  1.5729, -0.6361,  1.0556,  1.1676, -0.1034,\n",
       "                      -0.5618,  0.2264,  1.4008,  0.5954,  0.8340, -0.2751, -0.7414,  0.9932])),\n",
       "             ('module.encoder.net.7.running_var',\n",
       "              tensor([0.8287, 0.7975, 1.4420, 1.0261, 0.4087, 1.5064, 1.0219, 0.5499, 0.7876,\n",
       "                      1.5450, 0.8875, 0.8885, 1.2175, 1.3292, 0.6691, 0.5298, 1.0717, 1.3592,\n",
       "                      1.0658, 0.7837, 0.7549, 0.6256, 0.5760, 0.7756, 1.3434, 1.0705, 0.8067,\n",
       "                      0.7409, 0.7784, 0.6646, 1.2442, 0.5563, 1.1721, 0.6155, 0.6817, 0.4638,\n",
       "                      0.5206, 1.8840, 0.5326, 0.5826, 0.7818, 0.7414, 0.9830, 1.0531, 0.5199,\n",
       "                      0.8386, 0.9430, 1.1767, 0.8868, 1.8748, 0.6966, 1.0844, 0.6878, 1.2040,\n",
       "                      0.9915, 0.5341, 0.5355, 0.7908, 1.1352, 0.8845, 0.7531, 0.7024, 0.8306,\n",
       "                      0.7472])),\n",
       "             ('module.encoder.net.7.num_batches_tracked', tensor(0)),\n",
       "             ('module.encoder.net.9.weight',\n",
       "              tensor([[[[ 2.9440e-02,  3.2951e-02,  9.5708e-03],\n",
       "                        [ 3.3237e-02,  5.9818e-03, -2.9138e-02],\n",
       "                        [-1.0246e-02,  2.0571e-02, -2.4518e-02]],\n",
       "              \n",
       "                       [[-1.8294e-02, -4.3912e-03, -1.0683e-02],\n",
       "                        [-2.3081e-02, -4.7844e-02,  2.9651e-02],\n",
       "                        [-3.3252e-02,  6.7065e-03,  4.1311e-03]],\n",
       "              \n",
       "                       [[ 6.5229e-03,  3.8189e-02, -2.5371e-02],\n",
       "                        [ 5.5383e-03, -4.9879e-03, -4.2563e-02],\n",
       "                        [ 3.5208e-02, -7.3351e-03, -1.6050e-02]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 2.7421e-02,  2.8575e-02,  2.6872e-03],\n",
       "                        [ 2.3501e-02, -3.6710e-02, -4.0135e-05],\n",
       "                        [-2.5223e-02,  8.7016e-03,  2.5193e-02]],\n",
       "              \n",
       "                       [[-2.5707e-02,  5.3768e-03,  2.3532e-02],\n",
       "                        [ 2.3174e-02, -2.5764e-02, -2.9520e-03],\n",
       "                        [ 1.4607e-02,  5.4112e-03,  7.6781e-03]],\n",
       "              \n",
       "                       [[ 1.6834e-02,  1.0057e-02,  7.6930e-03],\n",
       "                        [ 4.5781e-02,  8.3023e-03, -3.0665e-02],\n",
       "                        [-2.5368e-02,  1.6963e-04,  2.0423e-02]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 1.0225e-02, -4.2113e-03,  3.0988e-02],\n",
       "                        [ 9.5045e-03,  9.9601e-03, -1.2827e-02],\n",
       "                        [ 3.1240e-03,  2.7305e-02, -3.5787e-02]],\n",
       "              \n",
       "                       [[-1.2884e-02,  4.2687e-02,  2.1936e-02],\n",
       "                        [ 1.9347e-02,  2.6355e-02, -2.4360e-02],\n",
       "                        [ 2.1191e-02, -3.6672e-02,  1.7397e-02]],\n",
       "              \n",
       "                       [[ 2.2711e-02, -6.2376e-03,  1.4588e-02],\n",
       "                        [ 3.9756e-02, -2.1556e-02, -3.6528e-02],\n",
       "                        [-1.6366e-02,  4.4785e-03, -3.6007e-02]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 7.8090e-03,  7.7978e-03,  1.7171e-02],\n",
       "                        [ 8.1308e-03, -2.2628e-02,  4.0726e-03],\n",
       "                        [-2.2423e-02, -5.3629e-03,  1.8870e-02]],\n",
       "              \n",
       "                       [[-7.5787e-03, -2.4190e-02, -2.6653e-02],\n",
       "                        [-2.2115e-02,  2.3242e-02,  3.9486e-02],\n",
       "                        [-9.2750e-03, -1.8722e-03,  4.6455e-02]],\n",
       "              \n",
       "                       [[-2.5638e-02,  3.1203e-02,  3.5174e-02],\n",
       "                        [ 4.4606e-02,  1.4135e-02, -7.0717e-03],\n",
       "                        [ 2.5252e-02,  2.0135e-02, -2.8448e-02]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 3.3365e-04,  3.9667e-02, -1.9254e-02],\n",
       "                        [-2.9138e-03,  5.0380e-02, -3.0633e-02],\n",
       "                        [ 4.0738e-02, -1.7933e-02, -2.3376e-02]],\n",
       "              \n",
       "                       [[ 1.3584e-02,  3.6033e-04, -4.6674e-03],\n",
       "                        [-3.8845e-02, -1.1314e-03, -9.4253e-03],\n",
       "                        [ 4.1920e-02, -4.0727e-02,  6.7944e-03]],\n",
       "              \n",
       "                       [[ 1.0995e-02, -2.1573e-03, -4.1034e-02],\n",
       "                        [ 3.6774e-02,  9.0355e-03, -3.7552e-04],\n",
       "                        [-3.4109e-02, -2.8473e-02,  1.1253e-02]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 1.3673e-02, -3.5371e-02,  3.0208e-02],\n",
       "                        [-3.9941e-02, -3.9026e-02,  1.7147e-02],\n",
       "                        [ 1.7926e-02, -4.2233e-03,  9.8002e-03]],\n",
       "              \n",
       "                       [[ 9.6720e-03,  3.0790e-02,  1.9979e-02],\n",
       "                        [-2.0641e-02,  1.3320e-02, -2.1017e-02],\n",
       "                        [-4.1688e-03,  3.4156e-02,  5.6553e-02]],\n",
       "              \n",
       "                       [[ 9.9145e-03,  7.1841e-03,  2.7199e-02],\n",
       "                        [ 2.8635e-02,  3.4890e-03, -4.1686e-02],\n",
       "                        [ 1.3015e-02, -5.3056e-02, -1.5641e-02]]],\n",
       "              \n",
       "              \n",
       "                      ...,\n",
       "              \n",
       "              \n",
       "                      [[[-2.3880e-03,  2.2950e-02, -1.2358e-03],\n",
       "                        [ 2.6262e-02,  1.8775e-02,  6.3716e-03],\n",
       "                        [-1.7239e-02,  2.5530e-02, -4.9667e-07]],\n",
       "              \n",
       "                       [[-1.3986e-02, -1.7156e-02,  1.1070e-02],\n",
       "                        [-3.3749e-02,  1.3150e-02,  1.1025e-02],\n",
       "                        [-3.5857e-02, -1.0317e-04, -1.9846e-02]],\n",
       "              \n",
       "                       [[-2.2696e-02, -5.2640e-02,  1.0520e-02],\n",
       "                        [-2.5238e-02,  2.0243e-02,  1.7420e-02],\n",
       "                        [-2.8204e-02, -8.1781e-03,  1.7938e-02]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-3.0034e-02, -1.9451e-02, -1.3484e-03],\n",
       "                        [ 3.6395e-02,  3.0684e-02, -2.5538e-02],\n",
       "                        [-3.9304e-02, -1.1962e-02, -2.5879e-02]],\n",
       "              \n",
       "                       [[-3.8325e-04,  5.1969e-03,  3.5162e-03],\n",
       "                        [ 3.1278e-02, -1.2408e-02, -2.1855e-02],\n",
       "                        [-2.0933e-03,  2.7806e-03, -5.0674e-02]],\n",
       "              \n",
       "                       [[ 1.8862e-02, -3.9456e-02, -1.9345e-02],\n",
       "                        [-2.2949e-02,  6.4214e-03,  1.3003e-02],\n",
       "                        [-2.6966e-02, -1.0374e-02,  1.6180e-02]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 6.7779e-04, -2.6573e-02,  1.1629e-02],\n",
       "                        [ 3.1402e-02,  1.4714e-02,  3.1123e-02],\n",
       "                        [-2.4181e-02,  2.7561e-02, -1.3669e-02]],\n",
       "              \n",
       "                       [[-3.4608e-02, -5.9351e-03,  2.5273e-02],\n",
       "                        [-1.6017e-02,  3.5520e-02,  3.0094e-02],\n",
       "                        [ 2.1594e-03,  2.1152e-02, -4.4292e-02]],\n",
       "              \n",
       "                       [[-3.6556e-02, -2.0280e-02,  1.1296e-02],\n",
       "                        [ 3.1418e-02,  2.7128e-02,  2.0314e-03],\n",
       "                        [ 1.6203e-02,  1.9324e-02,  3.7455e-02]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 2.4146e-02,  1.1383e-02, -1.6541e-02],\n",
       "                        [ 1.9896e-02, -1.2821e-02,  2.5029e-02],\n",
       "                        [ 2.1188e-02,  4.1570e-04,  7.3117e-03]],\n",
       "              \n",
       "                       [[ 5.0537e-02,  7.1376e-04, -3.3949e-02],\n",
       "                        [-1.2197e-02,  3.6226e-02, -1.2145e-02],\n",
       "                        [ 2.6069e-02, -2.2386e-02,  1.8296e-02]],\n",
       "              \n",
       "                       [[ 1.2536e-02,  2.7227e-02,  2.1419e-02],\n",
       "                        [ 3.2526e-03, -1.5262e-02,  3.6520e-03],\n",
       "                        [ 2.3222e-02,  2.3682e-02, -9.9028e-03]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 1.1761e-02,  6.7527e-03,  7.4284e-03],\n",
       "                        [ 1.7430e-03,  1.3593e-02,  3.7099e-02],\n",
       "                        [ 3.0871e-03, -4.7814e-03, -1.6743e-02]],\n",
       "              \n",
       "                       [[ 3.4774e-02,  1.1014e-02,  3.1397e-02],\n",
       "                        [ 1.7297e-02, -3.4285e-02, -4.4082e-02],\n",
       "                        [-2.7635e-02, -8.4930e-03,  2.7953e-02]],\n",
       "              \n",
       "                       [[ 2.4855e-02, -6.3372e-03, -4.6287e-02],\n",
       "                        [-2.7357e-02, -3.9719e-02,  2.6711e-02],\n",
       "                        [ 3.0376e-02,  1.9424e-02,  2.6200e-02]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 3.5017e-02,  2.5561e-02, -4.4686e-03],\n",
       "                        [ 2.2598e-02, -9.4132e-03, -3.6670e-02],\n",
       "                        [-3.0756e-02,  3.1764e-02,  1.8352e-02]],\n",
       "              \n",
       "                       [[ 2.5047e-02, -1.2116e-02, -6.3048e-03],\n",
       "                        [-3.0495e-02, -4.2019e-02,  1.5836e-02],\n",
       "                        [ 4.3042e-02, -6.6530e-03,  2.7823e-02]],\n",
       "              \n",
       "                       [[ 2.8891e-02,  1.8770e-02, -3.8539e-02],\n",
       "                        [ 2.1071e-02,  2.4649e-02,  1.8695e-02],\n",
       "                        [ 3.6223e-02,  4.6561e-02,  1.8756e-02]]]])),\n",
       "             ('module.encoder.net.9.bias',\n",
       "              tensor([-0.0090, -0.0389,  0.0065, -0.0276,  0.0190, -0.0450, -0.0050, -0.0017,\n",
       "                      -0.0256, -0.0176, -0.0046,  0.0187,  0.0153, -0.0396, -0.0122, -0.0375,\n",
       "                       0.0234, -0.0139, -0.0191,  0.0229, -0.0099, -0.0211,  0.0246,  0.0238,\n",
       "                       0.0359, -0.0099,  0.0115,  0.0350,  0.0282,  0.0345, -0.0451,  0.0425,\n",
       "                       0.0128,  0.0316,  0.0469,  0.0261, -0.0346, -0.0009,  0.0359,  0.0301,\n",
       "                       0.0440,  0.0374,  0.0331, -0.0264,  0.0369, -0.0072,  0.0166, -0.0220,\n",
       "                      -0.0381,  0.0340,  0.0107, -0.0301,  0.0122,  0.0104, -0.0296,  0.0424,\n",
       "                      -0.0441, -0.0120, -0.0315,  0.0276, -0.0375,  0.0425, -0.0307, -0.0251])),\n",
       "             ('module.encoder.net.10.weight',\n",
       "              tensor([0.9844, 0.9930, 0.9976, 0.9892, 1.0018, 1.0047, 0.9893, 0.9870, 0.9951,\n",
       "                      0.9981, 0.9844, 0.9859, 0.9745, 0.9772, 0.9979, 0.9994, 0.9828, 0.9940,\n",
       "                      0.9840, 0.9939, 0.9870, 0.9994, 0.9839, 0.9834, 0.9898, 0.9891, 0.9890,\n",
       "                      0.9805, 1.0038, 0.9953, 0.9909, 0.9847, 0.9872, 0.9856, 0.9900, 1.0093,\n",
       "                      0.9801, 0.9911, 0.9993, 0.9878, 1.0333, 1.0048, 0.9718, 0.9886, 0.9844,\n",
       "                      1.0066, 1.0088, 1.0152, 0.9807, 1.0030, 1.0066, 0.9925, 0.9907, 0.9966,\n",
       "                      0.9981, 0.9948, 1.0047, 0.9976, 0.9940, 0.9903, 1.0069, 0.9865, 0.9809,\n",
       "                      0.9941])),\n",
       "             ('module.encoder.net.10.bias',\n",
       "              tensor([ 0.0016,  0.0177,  0.0088, -0.0108, -0.0050,  0.0117, -0.0073, -0.0101,\n",
       "                       0.0173,  0.0051,  0.0040, -0.0076, -0.0121, -0.0049,  0.0281, -0.0022,\n",
       "                      -0.0162,  0.0222,  0.0032, -0.0071, -0.0275, -0.0053, -0.0241, -0.0047,\n",
       "                       0.0132, -0.0041,  0.0054, -0.0010,  0.0082, -0.0029,  0.0059, -0.0131,\n",
       "                      -0.0032, -0.0194, -0.0011, -0.0036, -0.0012,  0.0172, -0.0187, -0.0226,\n",
       "                       0.0104, -0.0132, -0.0085, -0.0014, -0.0164, -0.0030, -0.0086, -0.0090,\n",
       "                      -0.0163, -0.0143,  0.0277, -0.0002, -0.0157, -0.0022,  0.0053,  0.0074,\n",
       "                      -0.0014,  0.0381,  0.0141, -0.0026, -0.0018,  0.0068, -0.0070,  0.0122])),\n",
       "             ('module.encoder.net.10.running_mean',\n",
       "              tensor([ 0.6220,  0.6097,  0.5782, -0.5859, -1.0215, -0.4832, -0.7855, -0.8046,\n",
       "                      -0.7484,  0.3618,  0.5469, -0.6891, -0.3789,  0.5380,  0.4442, -0.7175,\n",
       "                      -0.7485,  0.8866,  0.3901, -0.5652, -0.5519, -0.4270, -0.6328, -0.2863,\n",
       "                      -0.3988, -0.1167, -0.3585, -0.5834, -0.5804, -0.4805,  0.0764, -0.7389,\n",
       "                      -0.3134, -0.4845, -0.6678, -0.1559, -0.5826, -0.8231, -0.5662, -0.9078,\n",
       "                      -0.7043, -0.8051, -0.7220, -0.6832, -0.8488, -0.3293, -0.4195, -0.4263,\n",
       "                      -0.4255, -0.3489,  0.6373, -0.6378, -0.7335, -0.8192, -0.9213, -0.5850,\n",
       "                      -0.0420,  0.7607,  0.3300, -0.7594, -1.1836, -0.7290, -0.5239,  0.6575])),\n",
       "             ('module.encoder.net.10.running_var',\n",
       "              tensor([ 3.9828,  4.6752,  5.1811,  3.2432,  3.6143,  4.6431,  3.6010,  4.1855,\n",
       "                       7.5864,  6.4107,  3.4305,  4.2701,  4.4443,  2.9074,  5.2817,  6.4618,\n",
       "                       3.9836,  4.1531,  4.7865,  5.5311,  3.2715,  4.7825,  6.1559,  4.3973,\n",
       "                       4.9431,  3.5869,  6.8600,  3.7589,  3.8456,  8.3875,  4.5089,  3.8242,\n",
       "                       3.2593,  3.8394,  5.7569,  6.5120,  3.2999,  6.5552,  5.1698,  5.2521,\n",
       "                       8.4428,  3.9928,  4.3784,  4.9246,  3.8842,  8.6247,  3.9996,  4.9044,\n",
       "                       2.9999,  5.7584,  6.4659,  5.0831,  3.5965,  5.1938,  5.7644, 12.0070,\n",
       "                       5.2187, 21.2001,  4.5689,  4.6420,  5.1347,  5.7821,  6.6756,  5.7497])),\n",
       "             ('module.encoder.net.10.num_batches_tracked', tensor(0)),\n",
       "             ('module.encoder.h1.weight',\n",
       "              tensor([[-0.0157, -0.0379,  0.0384,  ..., -0.0675, -0.0602,  0.0642],\n",
       "                      [-0.0434,  0.0240, -0.0061,  ..., -0.0074,  0.0573,  0.0278],\n",
       "                      [ 0.0023,  0.0221,  0.0726,  ..., -0.0455, -0.0611, -0.0414],\n",
       "                      ...,\n",
       "                      [ 0.0213,  0.0214,  0.0391,  ...,  0.0293,  0.0094,  0.0396],\n",
       "                      [-0.0505,  0.0283,  0.0655,  ..., -0.0220, -0.0630, -0.0345],\n",
       "                      [ 0.0327,  0.0289, -0.0171,  ...,  0.0208, -0.0492, -0.0172]])),\n",
       "             ('module.encoder.h1.bias',\n",
       "              tensor([-0.0108, -0.0092, -0.0611, -0.0075,  0.0122,  0.0391, -0.0001, -0.0058,\n",
       "                      -0.0277,  0.0136, -0.0282,  0.0195, -0.0236, -0.0428, -0.0071, -0.0315,\n",
       "                      -0.0189,  0.0325,  0.0057, -0.0316, -0.0209, -0.0030, -0.0239, -0.0309,\n",
       "                      -0.0496,  0.0121, -0.0209,  0.0081, -0.0178, -0.0280,  0.0187, -0.0168,\n",
       "                      -0.0188, -0.0021, -0.0031,  0.0189, -0.0175, -0.0353,  0.0355, -0.0464,\n",
       "                      -0.0284,  0.0006,  0.0150, -0.0047,  0.0235, -0.0440,  0.0259,  0.0390,\n",
       "                      -0.0441,  0.0459,  0.0334, -0.0450, -0.0228, -0.0527,  0.0328, -0.0199,\n",
       "                       0.0154, -0.0345,  0.0113, -0.0283, -0.0012,  0.0023,  0.0425,  0.0579])),\n",
       "             ('module.encoder.h2.weight',\n",
       "              tensor([[-0.0583, -0.0420,  0.0060,  ...,  0.0295,  0.0158, -0.0125],\n",
       "                      [ 0.0467,  0.0492, -0.0562,  ..., -0.0615,  0.0105,  0.0333],\n",
       "                      [ 0.0456, -0.0132,  0.0427,  ...,  0.0188, -0.0322,  0.0322],\n",
       "                      ...,\n",
       "                      [ 0.0114,  0.0259,  0.0225,  ...,  0.0224, -0.0271,  0.0150],\n",
       "                      [-0.0126, -0.0596, -0.0267,  ..., -0.0017, -0.0792,  0.0075],\n",
       "                      [-0.0417,  0.0472,  0.0066,  ..., -0.0283, -0.0088,  0.0067]])),\n",
       "             ('module.encoder.h2.bias',\n",
       "              tensor([-2.6462e-02, -1.8295e-02, -4.3301e-02,  6.6706e-03, -4.1553e-02,\n",
       "                       4.6090e-02,  5.2780e-02,  1.2413e-02,  2.8340e-02, -3.2237e-02,\n",
       "                      -4.7877e-02,  7.0453e-03, -1.9389e-02,  3.4964e-02,  3.5977e-02,\n",
       "                      -3.6291e-02, -3.3523e-02,  2.6893e-02, -1.6291e-02, -3.4937e-02,\n",
       "                      -2.5737e-02,  1.5583e-02,  2.0849e-02, -4.8387e-02,  2.0121e-02,\n",
       "                      -4.4504e-02, -3.7356e-02,  1.2048e-02, -4.0670e-02,  5.8316e-02,\n",
       "                      -2.4135e-02,  4.1008e-02,  1.6108e-02, -4.3319e-02, -2.5532e-02,\n",
       "                      -5.0486e-02,  4.2214e-02, -5.1921e-02,  4.2615e-02, -3.4405e-02,\n",
       "                       1.1555e-02,  3.0893e-02, -1.0457e-02,  3.2308e-02, -4.1987e-02,\n",
       "                       6.0254e-02,  2.1691e-02,  2.9829e-02,  4.3472e-02, -1.7790e-02,\n",
       "                      -3.1703e-02, -3.5070e-05, -1.2379e-02, -4.1670e-02,  3.5011e-02,\n",
       "                      -3.2202e-02,  2.7554e-02,  3.8311e-02, -2.1851e-02, -3.4262e-02,\n",
       "                      -1.9860e-02,  6.5393e-02,  1.3208e-02,  6.6374e-02])),\n",
       "             ('module.decoder.linear.0.weight',\n",
       "              tensor([[-0.0450,  0.0636, -0.0237,  ..., -0.0493, -0.1012,  0.0369],\n",
       "                      [ 0.0216,  0.0496,  0.0528,  ...,  0.0896,  0.0310,  0.0003],\n",
       "                      [ 0.0567, -0.0638,  0.0540,  ...,  0.0334, -0.0810,  0.0459],\n",
       "                      ...,\n",
       "                      [-0.0462,  0.0450, -0.0607,  ..., -0.0094,  0.0878, -0.0168],\n",
       "                      [ 0.0069, -0.0114,  0.0502,  ...,  0.0778,  0.0670,  0.0316],\n",
       "                      [-0.0919,  0.0541,  0.0548,  ...,  0.0389, -0.0859, -0.0836]])),\n",
       "             ('module.decoder.linear.0.bias',\n",
       "              tensor([ 0.1568,  0.1185,  0.0295,  0.0117,  0.0206, -0.0352,  0.0741,  0.1564,\n",
       "                      -0.0435, -0.0484, -0.1108,  0.1212,  0.0378, -0.0339,  0.0842,  0.0656,\n",
       "                       0.0340, -0.0942,  0.0795,  0.0099, -0.0620, -0.0265,  0.1671,  0.0781,\n",
       "                       0.0716,  0.0839,  0.0266,  0.1122,  0.0603, -0.1175, -0.0835,  0.0712,\n",
       "                       0.0181,  0.0972,  0.1031, -0.0886,  0.0326,  0.0065, -0.0047,  0.0436,\n",
       "                       0.0775,  0.0416, -0.0670, -0.0804,  0.0125, -0.0459,  0.0569,  0.0094,\n",
       "                      -0.0361,  0.0119,  0.0749, -0.1252, -0.0564,  0.0714,  0.0603,  0.1136,\n",
       "                      -0.0042,  0.0617,  0.1039,  0.0814, -0.0472,  0.1430,  0.1170, -0.0975,\n",
       "                       0.0596, -0.0186,  0.0041, -0.0214,  0.0746, -0.0263,  0.0132,  0.0848,\n",
       "                       0.1071, -0.0484,  0.1148,  0.0136,  0.0609,  0.0148,  0.0520,  0.0917,\n",
       "                      -0.0678,  0.0418,  0.1234, -0.0600,  0.0065,  0.0596, -0.0250, -0.0740,\n",
       "                       0.0948, -0.1227, -0.0428,  0.1034,  0.0837,  0.1040,  0.0521,  0.0097,\n",
       "                      -0.0107,  0.0509, -0.1017,  0.0606,  0.0663, -0.0288,  0.0990, -0.0505,\n",
       "                       0.0782,  0.0423,  0.0479,  0.0592,  0.0345, -0.0395,  0.1000,  0.0610,\n",
       "                       0.0581,  0.0893, -0.0728,  0.0601,  0.0636,  0.0601,  0.1226,  0.0336,\n",
       "                       0.0475,  0.1317,  0.0879, -0.0164,  0.0078,  0.0766,  0.0639, -0.0824,\n",
       "                       0.0962,  0.0497,  0.0235, -0.0917,  0.0469, -0.0070,  0.0756,  0.1053,\n",
       "                       0.0604, -0.0478,  0.0961,  0.0241, -0.0047, -0.0324,  0.1016,  0.0551,\n",
       "                       0.0147,  0.1323, -0.0002,  0.0833,  0.1146,  0.0056,  0.1042,  0.1079,\n",
       "                       0.1132, -0.0927, -0.0857, -0.0040, -0.0481,  0.0185,  0.0405, -0.1257,\n",
       "                       0.0242,  0.0610,  0.0587,  0.1003, -0.0024, -0.0735,  0.0849,  0.1218,\n",
       "                       0.0370, -0.0038,  0.0286,  0.0353, -0.0823,  0.0520, -0.0533,  0.0700,\n",
       "                       0.1381,  0.0137,  0.0559,  0.0403, -0.0263, -0.0258,  0.0739,  0.0452,\n",
       "                       0.0685,  0.0831, -0.0716,  0.0934,  0.1563,  0.1305, -0.0606, -0.0709,\n",
       "                      -0.0611,  0.0919,  0.0072, -0.0468, -0.0426,  0.1581,  0.0750,  0.0281,\n",
       "                       0.0559,  0.0582,  0.0803, -0.0072, -0.0335,  0.0028,  0.1154, -0.0185,\n",
       "                      -0.0043, -0.0518, -0.0305, -0.0531, -0.0767, -0.0033,  0.0123, -0.0097,\n",
       "                       0.0391, -0.0925,  0.1040, -0.1259, -0.0507, -0.0003, -0.0096,  0.0786,\n",
       "                      -0.0473, -0.1235,  0.0560,  0.1115, -0.0563,  0.0913,  0.0090,  0.0702,\n",
       "                      -0.0754, -0.1331,  0.0700,  0.0816, -0.0243, -0.0274, -0.0289,  0.0110,\n",
       "                       0.0333,  0.0135,  0.0212,  0.0547,  0.1078,  0.0646,  0.0554,  0.0398,\n",
       "                       0.0230,  0.1007,  0.0810,  0.0411,  0.1367, -0.0478, -0.0658,  0.0795])),\n",
       "             ('module.decoder.net.1.weight',\n",
       "              tensor([[[[ 0.0352, -0.0093,  0.0348],\n",
       "                        [-0.0662,  0.0307,  0.0057],\n",
       "                        [-0.0418, -0.0166,  0.0199]],\n",
       "              \n",
       "                       [[ 0.0131,  0.0480,  0.0460],\n",
       "                        [-0.0275,  0.0017, -0.0127],\n",
       "                        [-0.0109, -0.0021, -0.0374]],\n",
       "              \n",
       "                       [[ 0.0508, -0.0274,  0.0291],\n",
       "                        [ 0.0130, -0.0344, -0.0143],\n",
       "                        [-0.0436, -0.0064, -0.0301]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-0.0389,  0.0106, -0.0107],\n",
       "                        [ 0.0259, -0.0227, -0.0109],\n",
       "                        [ 0.0302, -0.0431, -0.0416]],\n",
       "              \n",
       "                       [[ 0.0075,  0.0086,  0.0406],\n",
       "                        [ 0.0362,  0.0426, -0.0207],\n",
       "                        [ 0.0271,  0.0280, -0.0036]],\n",
       "              \n",
       "                       [[ 0.0359, -0.0296, -0.0117],\n",
       "                        [-0.0324, -0.0061,  0.0307],\n",
       "                        [-0.0061, -0.0294, -0.0084]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.0333,  0.0365,  0.0085],\n",
       "                        [-0.0194, -0.0269, -0.0384],\n",
       "                        [-0.0240, -0.0354, -0.0041]],\n",
       "              \n",
       "                       [[-0.0234, -0.0294, -0.0307],\n",
       "                        [-0.0042, -0.0066, -0.0478],\n",
       "                        [-0.0219,  0.0125, -0.0180]],\n",
       "              \n",
       "                       [[-0.0183,  0.0058,  0.0077],\n",
       "                        [ 0.0230,  0.0380, -0.0175],\n",
       "                        [ 0.0202,  0.0178, -0.0127]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-0.0122, -0.0284, -0.0062],\n",
       "                        [ 0.0011,  0.0065,  0.0464],\n",
       "                        [ 0.0067, -0.0031, -0.0271]],\n",
       "              \n",
       "                       [[-0.0303,  0.0089,  0.0155],\n",
       "                        [ 0.0351, -0.0063,  0.0041],\n",
       "                        [ 0.0201, -0.0188, -0.0194]],\n",
       "              \n",
       "                       [[-0.0341,  0.0257, -0.0226],\n",
       "                        [-0.0298, -0.0075,  0.0126],\n",
       "                        [ 0.0143, -0.0266, -0.0353]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.0003,  0.0391, -0.0062],\n",
       "                        [-0.0451, -0.0118,  0.0379],\n",
       "                        [ 0.0063, -0.0321,  0.0096]],\n",
       "              \n",
       "                       [[ 0.0459, -0.0517,  0.0321],\n",
       "                        [ 0.0384, -0.0038,  0.0134],\n",
       "                        [ 0.0442,  0.0267,  0.0346]],\n",
       "              \n",
       "                       [[-0.0134, -0.0421,  0.0302],\n",
       "                        [-0.0214, -0.0459, -0.0111],\n",
       "                        [-0.0011,  0.0343,  0.0675]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-0.0063,  0.0259,  0.0435],\n",
       "                        [-0.0060,  0.0136, -0.0065],\n",
       "                        [ 0.0521, -0.0401,  0.0063]],\n",
       "              \n",
       "                       [[ 0.0311, -0.0294, -0.0132],\n",
       "                        [-0.0016, -0.0144,  0.0063],\n",
       "                        [ 0.0322,  0.0358, -0.0009]],\n",
       "              \n",
       "                       [[ 0.0551,  0.0063,  0.0376],\n",
       "                        [-0.0337, -0.0736,  0.0052],\n",
       "                        [ 0.0191, -0.0366,  0.0191]]],\n",
       "              \n",
       "              \n",
       "                      ...,\n",
       "              \n",
       "              \n",
       "                      [[[ 0.0019, -0.0141, -0.0053],\n",
       "                        [ 0.0145, -0.0411, -0.0318],\n",
       "                        [ 0.0250,  0.0351,  0.0077]],\n",
       "              \n",
       "                       [[-0.0066,  0.0296,  0.0294],\n",
       "                        [-0.0420,  0.0281,  0.0092],\n",
       "                        [ 0.0006,  0.0273,  0.0481]],\n",
       "              \n",
       "                       [[-0.0045,  0.0169,  0.0091],\n",
       "                        [-0.0478,  0.0076, -0.0320],\n",
       "                        [-0.0300,  0.0295, -0.0270]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-0.0076, -0.0318, -0.0015],\n",
       "                        [ 0.0107, -0.0301,  0.0063],\n",
       "                        [-0.0228, -0.0100, -0.0427]],\n",
       "              \n",
       "                       [[ 0.0165, -0.0091,  0.0259],\n",
       "                        [ 0.0287,  0.0087,  0.0373],\n",
       "                        [-0.0187,  0.0359,  0.0287]],\n",
       "              \n",
       "                       [[-0.0304,  0.0184,  0.0329],\n",
       "                        [ 0.0184, -0.0278,  0.0059],\n",
       "                        [-0.0042,  0.0048,  0.0425]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.0167,  0.0277,  0.0275],\n",
       "                        [-0.0273, -0.0234,  0.0279],\n",
       "                        [ 0.0027,  0.0601,  0.0289]],\n",
       "              \n",
       "                       [[-0.0381,  0.0498,  0.0022],\n",
       "                        [ 0.0249,  0.0108,  0.0340],\n",
       "                        [-0.0081,  0.0005, -0.0007]],\n",
       "              \n",
       "                       [[-0.0278, -0.0152, -0.0280],\n",
       "                        [-0.0309, -0.0112,  0.0094],\n",
       "                        [ 0.0263,  0.0091,  0.0504]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 0.0093, -0.0065, -0.0241],\n",
       "                        [ 0.0491, -0.0318,  0.0160],\n",
       "                        [-0.0112,  0.0100, -0.0259]],\n",
       "              \n",
       "                       [[ 0.0190,  0.0057, -0.0270],\n",
       "                        [ 0.0249,  0.0367, -0.0165],\n",
       "                        [ 0.0335, -0.0083,  0.0330]],\n",
       "              \n",
       "                       [[-0.0172,  0.0278, -0.0309],\n",
       "                        [-0.0071, -0.0139, -0.0442],\n",
       "                        [-0.0309,  0.0307,  0.0344]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.0040,  0.0335,  0.0278],\n",
       "                        [ 0.0025, -0.0370, -0.0374],\n",
       "                        [ 0.0331,  0.0382,  0.0348]],\n",
       "              \n",
       "                       [[-0.0136, -0.0276, -0.0260],\n",
       "                        [-0.0311, -0.0400, -0.0030],\n",
       "                        [ 0.0381, -0.0023,  0.0510]],\n",
       "              \n",
       "                       [[ 0.0387,  0.0199, -0.0341],\n",
       "                        [ 0.0249, -0.0256, -0.0159],\n",
       "                        [-0.0016, -0.0147, -0.0172]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 0.0515, -0.0099,  0.0248],\n",
       "                        [ 0.0283, -0.0168, -0.0091],\n",
       "                        [ 0.0459, -0.0161,  0.0044]],\n",
       "              \n",
       "                       [[ 0.0358, -0.0209, -0.0044],\n",
       "                        [ 0.0272,  0.0064, -0.0103],\n",
       "                        [ 0.0244,  0.0300, -0.0016]],\n",
       "              \n",
       "                       [[ 0.0254, -0.0094, -0.0309],\n",
       "                        [ 0.0117,  0.0030,  0.0114],\n",
       "                        [ 0.0179,  0.0161,  0.0178]]]])),\n",
       "             ('module.decoder.net.1.bias',\n",
       "              tensor([ 0.0029, -0.0215, -0.0247, -0.0305,  0.0318, -0.0183,  0.0479,  0.0233,\n",
       "                       0.0219,  0.0221, -0.0069, -0.0081, -0.0337,  0.0071,  0.0157,  0.0043,\n",
       "                       0.0218,  0.0098, -0.0410,  0.0388, -0.0403, -0.0291, -0.0146, -0.0328,\n",
       "                      -0.0364, -0.0356, -0.0198,  0.0037, -0.0189, -0.0439,  0.0478, -0.0054,\n",
       "                      -0.0311, -0.0063,  0.0344,  0.0108,  0.0305,  0.0100,  0.0121, -0.0164,\n",
       "                       0.0069,  0.0138,  0.0167,  0.0047,  0.0348,  0.0285, -0.0214,  0.0090,\n",
       "                       0.0272,  0.0279,  0.0115,  0.0121, -0.0343, -0.0369, -0.0073, -0.0313,\n",
       "                      -0.0300, -0.0046, -0.0071, -0.0274,  0.0011, -0.0241, -0.0073, -0.0341])),\n",
       "             ('module.decoder.net.2.weight',\n",
       "              tensor([0.9964, 1.0296, 1.0120, 0.9943, 1.0076, 0.9941, 1.0416, 0.9879, 0.9969,\n",
       "                      0.9935, 0.9635, 0.9716, 1.0526, 0.9760, 1.0705, 1.0142, 1.0281, 1.0078,\n",
       "                      0.9501, 1.0083, 0.9922, 0.9881, 0.9743, 0.9601, 1.0154, 0.9885, 1.0098,\n",
       "                      0.9717, 0.9842, 1.0954, 0.9629, 0.9759, 0.9768, 0.9634, 0.9701, 1.0145,\n",
       "                      1.0411, 1.0159, 0.9818, 1.0962, 0.9918, 1.0197, 1.0282, 1.0081, 0.9724,\n",
       "                      0.9784, 0.9893, 1.0174, 0.9698, 1.0317, 1.0150, 1.0444, 1.0088, 1.0196,\n",
       "                      0.9947, 0.9832, 1.0840, 0.9851, 0.9864, 1.0057, 0.9915, 0.9955, 0.9917,\n",
       "                      0.9858])),\n",
       "             ('module.decoder.net.2.bias',\n",
       "              tensor([ 0.0098, -0.0967,  0.0803,  0.0424,  0.0169,  0.0084, -0.1165,  0.0131,\n",
       "                       0.0522, -0.0141,  0.0453,  0.0137,  0.0748,  0.0400,  0.1030, -0.1833,\n",
       "                       0.0004,  0.0856, -0.0280,  0.0184,  0.0116, -0.1230,  0.0212,  0.0065,\n",
       "                      -0.0035,  0.0243,  0.0310, -0.0802, -0.0092, -0.1070, -0.0146,  0.0275,\n",
       "                       0.0499,  0.0374,  0.0178,  0.0660,  0.0532, -0.0016,  0.0596,  0.0756,\n",
       "                       0.0052,  0.0406, -0.0864, -0.0484, -0.0113, -0.0563,  0.0396,  0.0080,\n",
       "                       0.0238,  0.0206, -0.0511,  0.0527,  0.0078,  0.0848, -0.0515, -0.0163,\n",
       "                       0.1356,  0.0274, -0.0004,  0.0109, -0.0107,  0.0223,  0.0278,  0.0334])),\n",
       "             ('module.decoder.net.2.running_mean',\n",
       "              tensor([-0.6541, -0.0407,  0.2524, -0.5267, -0.3847,  0.1776,  0.0919,  0.0774,\n",
       "                       0.1254, -0.1852,  0.2530,  0.5234, -0.5049,  0.1934, -1.0353,  0.3579,\n",
       "                      -0.4854,  0.5748,  0.8987,  0.7997,  0.1650,  0.2335,  0.4971,  0.4763,\n",
       "                       0.5641, -0.5591, -0.9958,  0.3565, -0.0605,  0.0577,  0.9668, -0.2035,\n",
       "                       0.6008,  0.7943,  0.1398, -0.9467, -0.1481, -0.6988,  0.5108, -0.7394,\n",
       "                      -0.4527, -0.1442, -0.0720, -0.0390, -0.2774,  1.0028,  0.5945, -0.4666,\n",
       "                      -0.0998,  0.3324,  0.1890, -0.6941, -0.2627, -0.9685,  0.0690,  0.3810,\n",
       "                      -1.0652,  0.4350,  0.1439, -0.6649, -0.1996,  0.1398,  0.3185,  0.0185])),\n",
       "             ('module.decoder.net.2.running_var',\n",
       "              tensor([0.5979, 1.3267, 0.6805, 0.6304, 0.6204, 0.4523, 0.8696, 0.6714, 0.6597,\n",
       "                      0.7733, 0.9995, 0.6293, 0.8850, 0.4589, 1.2638, 1.3374, 0.9217, 1.0865,\n",
       "                      0.6748, 0.9297, 0.7209, 1.2123, 0.7314, 0.7885, 0.7410, 0.7979, 0.6089,\n",
       "                      0.6458, 0.4984, 0.8542, 0.8305, 0.7439, 0.9568, 0.6895, 0.4954, 0.9763,\n",
       "                      0.8848, 1.2495, 0.7764, 1.4704, 0.4469, 0.6097, 0.7759, 0.9812, 0.5510,\n",
       "                      0.9305, 0.6335, 1.2437, 0.3807, 0.9093, 0.9683, 0.7802, 0.6240, 0.7660,\n",
       "                      0.8288, 0.4419, 1.2750, 1.0190, 0.8159, 0.6335, 0.3095, 0.5949, 0.8170,\n",
       "                      0.6899])),\n",
       "             ('module.decoder.net.2.num_batches_tracked', tensor(0)),\n",
       "             ('module.decoder.net.5.weight',\n",
       "              tensor([[[[-4.4666e-02,  2.1231e-02, -3.9061e-02],\n",
       "                        [-8.2612e-03, -3.8279e-03, -1.0765e-02],\n",
       "                        [ 1.0995e-02, -3.5848e-02,  2.5824e-02]],\n",
       "              \n",
       "                       [[-5.7828e-03, -2.6363e-02,  1.8269e-02],\n",
       "                        [-3.4291e-02,  2.2985e-02,  2.3662e-02],\n",
       "                        [-4.2780e-03, -3.7889e-02,  4.2290e-02]],\n",
       "              \n",
       "                       [[ 2.7253e-02, -3.3512e-02,  1.1214e-02],\n",
       "                        [ 1.8411e-02, -3.6890e-03,  2.5737e-02],\n",
       "                        [-1.5046e-02, -3.4743e-02,  4.6833e-02]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 2.3125e-02,  3.4930e-02,  4.3599e-02],\n",
       "                        [-1.6950e-02,  3.6849e-02,  1.4114e-02],\n",
       "                        [ 2.4551e-03,  3.4413e-03, -1.5016e-02]],\n",
       "              \n",
       "                       [[ 4.3786e-02,  2.0778e-02,  1.9251e-02],\n",
       "                        [ 3.1898e-02,  1.2630e-02, -7.7404e-03],\n",
       "                        [ 5.2175e-02, -1.9121e-02, -1.5171e-02]],\n",
       "              \n",
       "                       [[-1.9467e-02,  5.2703e-04,  2.4360e-02],\n",
       "                        [ 2.4634e-02, -1.4908e-02,  2.4724e-03],\n",
       "                        [ 1.5220e-02,  3.4593e-02,  5.8194e-03]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 2.4046e-03, -8.9726e-04, -1.4135e-02],\n",
       "                        [-7.6119e-03,  3.5189e-02, -4.6772e-02],\n",
       "                        [-4.1316e-02, -3.3863e-02,  3.6069e-02]],\n",
       "              \n",
       "                       [[-1.2391e-02,  2.5201e-02,  4.4817e-02],\n",
       "                        [-2.2226e-02,  4.7860e-02,  2.2361e-02],\n",
       "                        [ 1.0308e-02, -1.3669e-02, -3.2421e-02]],\n",
       "              \n",
       "                       [[-5.2869e-03,  8.1818e-03,  1.9722e-02],\n",
       "                        [-1.5819e-02,  2.2421e-02,  3.3984e-03],\n",
       "                        [-1.1506e-02, -2.1921e-03, -2.4996e-02]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-2.9767e-02, -2.3558e-02,  2.3718e-02],\n",
       "                        [-3.6240e-03,  3.9553e-02, -1.0014e-02],\n",
       "                        [-2.0930e-03, -4.9498e-02, -2.8157e-02]],\n",
       "              \n",
       "                       [[ 1.3717e-02, -6.0359e-02, -2.9160e-02],\n",
       "                        [-1.7192e-02, -5.5164e-02, -1.7413e-03],\n",
       "                        [ 7.9333e-03, -2.9516e-02, -1.1807e-02]],\n",
       "              \n",
       "                       [[ 2.7777e-02, -4.8643e-04, -1.7770e-02],\n",
       "                        [ 1.0068e-02, -1.0402e-03,  3.3189e-02],\n",
       "                        [-2.9386e-02, -1.7303e-02,  3.2797e-03]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 3.0877e-02, -9.9061e-04,  6.6029e-03],\n",
       "                        [-1.2432e-02, -5.2386e-02,  1.9854e-02],\n",
       "                        [ 2.3671e-02, -4.3379e-02,  1.7559e-02]],\n",
       "              \n",
       "                       [[-3.4207e-02, -2.7252e-02,  2.8333e-03],\n",
       "                        [-3.5497e-02, -2.7928e-02,  3.6734e-02],\n",
       "                        [-1.8185e-02,  8.6854e-03,  2.2241e-02]],\n",
       "              \n",
       "                       [[-6.0723e-02,  2.2672e-02, -2.4575e-02],\n",
       "                        [ 1.4406e-02, -8.4735e-03, -2.1566e-02],\n",
       "                        [-2.4692e-03, -1.7180e-02, -2.6475e-03]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-5.7984e-02, -6.2179e-02, -2.3298e-02],\n",
       "                        [-5.1770e-02, -3.6653e-02,  2.8087e-02],\n",
       "                        [ 1.6647e-02, -3.4677e-02, -1.8029e-02]],\n",
       "              \n",
       "                       [[ 3.6731e-02, -2.2192e-02,  1.5378e-02],\n",
       "                        [ 3.4947e-03, -4.7636e-02,  1.5352e-02],\n",
       "                        [ 4.7787e-03, -7.5833e-03,  3.3133e-02]],\n",
       "              \n",
       "                       [[ 1.5290e-02,  3.4719e-02,  3.5807e-02],\n",
       "                        [-9.2917e-03, -1.3614e-02,  1.9503e-02],\n",
       "                        [-4.4816e-03, -1.2404e-02,  2.8622e-02]]],\n",
       "              \n",
       "              \n",
       "                      ...,\n",
       "              \n",
       "              \n",
       "                      [[[-1.8254e-02, -7.1438e-03, -1.3102e-02],\n",
       "                        [-3.1245e-02, -3.8472e-02, -3.8368e-02],\n",
       "                        [-1.5816e-02,  3.5217e-02,  7.1938e-03]],\n",
       "              \n",
       "                       [[ 4.9688e-02,  5.1030e-02, -5.3585e-03],\n",
       "                        [ 2.2480e-02,  1.7957e-02, -2.8330e-02],\n",
       "                        [ 6.7626e-02,  4.0251e-02,  3.1763e-02]],\n",
       "              \n",
       "                       [[-4.5753e-02, -8.4011e-04, -1.3455e-02],\n",
       "                        [ 7.5904e-03, -1.7326e-02,  3.4553e-02],\n",
       "                        [-6.6804e-03,  2.3350e-03,  2.7643e-02]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-1.5756e-02, -8.0798e-03, -3.0808e-02],\n",
       "                        [-4.6721e-03, -6.9924e-03,  1.8791e-03],\n",
       "                        [-9.3636e-03, -2.6543e-02, -9.9091e-03]],\n",
       "              \n",
       "                       [[-5.6244e-03,  6.0830e-05, -2.1296e-02],\n",
       "                        [-2.8193e-02,  5.2916e-03,  1.0483e-02],\n",
       "                        [-4.6972e-02,  2.7325e-03, -3.0403e-03]],\n",
       "              \n",
       "                       [[-3.2991e-02, -2.8507e-02,  3.3788e-02],\n",
       "                        [ 2.7809e-02, -2.8284e-02,  1.8944e-02],\n",
       "                        [ 2.7829e-02, -1.9171e-02, -1.6896e-02]]],\n",
       "              \n",
       "              \n",
       "                      [[[-1.4572e-02, -5.9936e-02,  1.7445e-02],\n",
       "                        [ 2.0214e-02, -3.2514e-02, -3.3755e-02],\n",
       "                        [ 8.0773e-03, -3.1324e-03,  3.0823e-02]],\n",
       "              \n",
       "                       [[ 7.4948e-02,  4.1195e-03,  3.5999e-02],\n",
       "                        [-3.3481e-03, -2.1896e-02,  1.0617e-02],\n",
       "                        [ 3.7393e-02,  5.6666e-02,  4.0745e-02]],\n",
       "              \n",
       "                       [[-9.1160e-03, -3.6874e-03,  1.9229e-02],\n",
       "                        [ 3.5503e-03, -4.9844e-02, -2.0421e-03],\n",
       "                        [-2.2313e-02, -3.9678e-02, -1.9668e-02]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-2.3802e-02,  8.2083e-06, -2.9441e-02],\n",
       "                        [-3.9688e-02,  2.6609e-02, -5.6712e-02],\n",
       "                        [ 4.4278e-02,  2.9166e-03, -1.7151e-02]],\n",
       "              \n",
       "                       [[ 1.8846e-02, -2.5532e-02, -2.8948e-02],\n",
       "                        [-4.5683e-02, -4.6453e-02, -1.4986e-03],\n",
       "                        [ 1.3677e-02, -2.5904e-02, -2.5363e-03]],\n",
       "              \n",
       "                       [[-3.2816e-02,  1.0869e-02, -1.2745e-02],\n",
       "                        [ 3.5449e-02, -2.6974e-02,  1.1315e-02],\n",
       "                        [ 4.2018e-02, -9.0131e-03, -3.7910e-02]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 3.1757e-02,  4.4644e-03, -4.1347e-02],\n",
       "                        [-3.8276e-02, -4.2309e-02, -8.0232e-03],\n",
       "                        [-3.2558e-02,  7.7784e-03, -6.8774e-03]],\n",
       "              \n",
       "                       [[-5.4496e-02, -8.5899e-04, -1.8077e-02],\n",
       "                        [-8.3938e-04,  4.6253e-02,  6.3345e-03],\n",
       "                        [-4.6540e-02,  4.4050e-02, -3.5058e-02]],\n",
       "              \n",
       "                       [[-1.7893e-02,  8.0749e-02,  5.2085e-02],\n",
       "                        [ 3.0166e-02, -2.5831e-02,  5.3046e-02],\n",
       "                        [ 2.5967e-02,  1.2841e-02,  4.2742e-02]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-3.0761e-02,  4.0704e-03, -7.7810e-03],\n",
       "                        [-2.6644e-02,  2.4524e-02, -1.1593e-02],\n",
       "                        [-4.8495e-02,  1.6615e-02, -2.8146e-03]],\n",
       "              \n",
       "                       [[-2.6180e-02, -3.4278e-02,  1.6883e-02],\n",
       "                        [-8.3414e-03, -2.7598e-03, -8.4430e-03],\n",
       "                        [ 2.7240e-02,  1.6199e-02, -3.3813e-02]],\n",
       "              \n",
       "                       [[ 1.9565e-02,  6.7018e-02,  4.9044e-02],\n",
       "                        [-8.4429e-03, -1.6074e-02, -2.3614e-02],\n",
       "                        [ 2.9445e-03,  4.2843e-02,  2.6543e-02]]]])),\n",
       "             ('module.decoder.net.5.bias',\n",
       "              tensor([ 0.0023,  0.0207, -0.0389, -0.0152,  0.0123,  0.0122, -0.0160, -0.0258,\n",
       "                      -0.0215, -0.0386,  0.0107,  0.0262,  0.0488,  0.0268, -0.0308,  0.0210,\n",
       "                      -0.0248, -0.0221,  0.0290, -0.0227,  0.0033, -0.0313,  0.0258, -0.0255,\n",
       "                      -0.0212,  0.0102,  0.0408,  0.0143, -0.0164,  0.0157, -0.0250, -0.0324,\n",
       "                       0.0130, -0.0261,  0.0144,  0.0176, -0.0075,  0.0358,  0.0176, -0.0393,\n",
       "                      -0.0038, -0.0317, -0.0020,  0.0298, -0.0237, -0.0187,  0.0077, -0.0216,\n",
       "                       0.0073, -0.0007, -0.0098,  0.0378,  0.0435,  0.0335, -0.0105,  0.0459,\n",
       "                      -0.0120,  0.0078,  0.0351, -0.0371,  0.0238,  0.0158, -0.0114,  0.0382])),\n",
       "             ('module.decoder.net.6.weight',\n",
       "              tensor([0.9814, 1.0039, 0.9750, 0.9789, 1.0300, 1.0814, 1.0008, 1.0030, 0.9415,\n",
       "                      0.9523, 1.0149, 1.0478, 1.0607, 1.0433, 1.0057, 1.0095, 0.9889, 0.9866,\n",
       "                      0.9441, 1.0179, 1.0384, 0.9896, 0.9950, 1.0079, 1.0182, 0.9781, 0.9847,\n",
       "                      1.0037, 1.0112, 0.9258, 1.0019, 1.0550, 1.0036, 0.9763, 1.0067, 0.9801,\n",
       "                      0.9587, 0.9503, 0.9924, 0.9924, 0.9837, 0.9972, 1.0056, 0.9624, 0.9917,\n",
       "                      1.0162, 0.9582, 0.9431, 1.0254, 0.9981, 0.9659, 0.9753, 0.9986, 0.9567,\n",
       "                      1.0636, 1.0586, 1.0271, 1.0631, 1.0043, 0.9893, 1.0085, 1.0208, 1.0014,\n",
       "                      0.9928])),\n",
       "             ('module.decoder.net.6.bias',\n",
       "              tensor([ 0.0053, -0.0174, -0.0142,  0.0065,  0.0149,  0.0498, -0.0308, -0.0193,\n",
       "                      -0.0267, -0.0134, -0.0256,  0.0855,  0.0101, -0.1970, -0.0045,  0.0086,\n",
       "                      -0.1697,  0.0055, -0.0227,  0.0334, -0.1808, -0.0079, -0.0569,  0.0456,\n",
       "                      -0.0208, -0.0225, -0.0167,  0.0606, -0.0299, -0.0375, -0.0039, -0.1904,\n",
       "                      -0.0258,  0.0647, -0.1359,  0.0260,  0.0026, -0.0382, -0.0384, -0.0049,\n",
       "                      -0.0098, -0.0415, -0.1631,  0.0026, -0.0289,  0.0168, -0.0058, -0.0041,\n",
       "                      -0.1858, -0.0250,  0.0012, -0.0736, -0.0012, -0.0149,  0.0067, -0.0076,\n",
       "                       0.0179,  0.0549, -0.0392, -0.0040,  0.0246, -0.1395, -0.1377, -0.0192])),\n",
       "             ('module.decoder.net.6.running_mean',\n",
       "              tensor([ 0.3639, -0.4122, -0.6451, -0.6319, -0.0740,  0.9224, -0.4678, -0.6120,\n",
       "                       0.0258,  0.5216, -0.0808, -0.4954, -0.3885,  0.1361, -0.3461, -0.4157,\n",
       "                      -0.0581,  0.2111,  0.3425, -0.7930,  0.1816,  0.2323,  0.4855, -0.3351,\n",
       "                      -0.3942, -1.0320, -0.5212, -0.4072,  0.2251,  0.2357, -0.3352, -0.0863,\n",
       "                      -0.7528, -1.1099, -0.8988,  0.0191, -0.1288, -0.2115, -0.3549, -0.4198,\n",
       "                      -1.0872,  0.5884, -0.2957, -0.5774, -1.4092, -0.4661, -0.4331, -0.5131,\n",
       "                       0.3770, -1.1931, -0.6367,  0.4230, -0.6348,  0.1579, -0.3245, -0.3227,\n",
       "                      -0.8622, -0.2627, -0.6295, -0.3499, -0.5400, -0.1946, -0.3309,  0.4491])),\n",
       "             ('module.decoder.net.6.running_var',\n",
       "              tensor([1.4739, 0.5857, 0.3495, 0.4178, 0.6182, 3.1771, 0.9123, 1.0255, 0.5889,\n",
       "                      1.4006, 1.8343, 1.1243, 1.1660, 0.9980, 0.6362, 1.0076, 0.5867, 1.3797,\n",
       "                      0.9599, 1.2041, 0.9305, 0.6995, 0.8144, 1.0440, 1.2875, 0.6333, 0.6591,\n",
       "                      1.1268, 0.5584, 0.5505, 2.0369, 0.7992, 0.6932, 1.0934, 0.8547, 1.3438,\n",
       "                      0.8863, 0.8458, 0.5177, 0.6407, 0.9208, 1.0757, 0.8261, 0.9036, 1.2028,\n",
       "                      1.0039, 0.5949, 0.8224, 0.5809, 1.0100, 0.6736, 0.5735, 0.5442, 0.4996,\n",
       "                      0.6055, 0.7775, 1.2380, 0.7035, 0.9739, 0.9713, 0.4396, 0.7196, 0.8538,\n",
       "                      0.6610])),\n",
       "             ('module.decoder.net.6.num_batches_tracked', tensor(0)),\n",
       "             ('module.decoder.net.9.weight',\n",
       "              tensor([[[[ 1.4832e-02, -2.2842e-02,  2.0670e-02],\n",
       "                        [ 4.6337e-03, -2.0678e-02,  2.9922e-03],\n",
       "                        [-5.2565e-02,  8.1664e-03,  9.8261e-03]],\n",
       "              \n",
       "                       [[ 5.2398e-03,  5.3137e-03,  2.5730e-02],\n",
       "                        [-8.6184e-03, -2.0149e-02, -6.8285e-03],\n",
       "                        [-3.4353e-03, -1.5524e-02, -3.8062e-02]],\n",
       "              \n",
       "                       [[-1.6183e-02, -1.2420e-02,  1.4720e-02],\n",
       "                        [-2.1385e-02, -4.3337e-03,  3.1046e-02],\n",
       "                        [ 8.3468e-04,  3.0876e-02,  1.3474e-02]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-7.7421e-03, -1.9233e-02,  2.5358e-02],\n",
       "                        [-2.4188e-03,  2.9438e-02, -2.0839e-02],\n",
       "                        [ 2.2067e-03, -1.3337e-02,  3.0465e-03]],\n",
       "              \n",
       "                       [[ 2.8411e-02,  6.6579e-02, -3.7907e-03],\n",
       "                        [ 2.7577e-02,  4.8124e-02,  4.3261e-02],\n",
       "                        [ 2.1502e-02, -2.2175e-02,  2.1186e-02]],\n",
       "              \n",
       "                       [[-4.6162e-03, -2.6167e-02, -1.5099e-02],\n",
       "                        [ 4.1290e-02,  9.6316e-03,  2.9543e-02],\n",
       "                        [-5.7876e-03,  3.8629e-04,  5.1518e-03]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 2.4353e-02, -3.2707e-02, -4.5588e-02],\n",
       "                        [-1.1558e-03, -5.5505e-02,  2.2604e-02],\n",
       "                        [-1.9420e-02,  7.7393e-03, -2.9973e-02]],\n",
       "              \n",
       "                       [[-2.5816e-02,  1.9275e-02, -8.4505e-03],\n",
       "                        [-4.2941e-02, -4.1288e-02, -3.5785e-02],\n",
       "                        [ 1.3826e-04,  1.5822e-02, -5.5838e-02]],\n",
       "              \n",
       "                       [[-2.7842e-03,  2.4094e-02,  6.0582e-03],\n",
       "                        [-1.4395e-02, -1.0046e-02, -4.8095e-03],\n",
       "                        [ 3.0908e-02, -4.5377e-02,  2.7770e-02]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-4.8088e-02, -6.2869e-02, -3.2472e-02],\n",
       "                        [ 8.3946e-03, -1.3735e-02, -4.6843e-03],\n",
       "                        [ 2.2948e-02,  4.8700e-02,  4.2024e-02]],\n",
       "              \n",
       "                       [[-6.3692e-03, -5.6218e-03, -1.4822e-02],\n",
       "                        [-2.7681e-03, -1.3297e-02, -1.8063e-02],\n",
       "                        [-6.8435e-03,  3.1436e-02, -1.3891e-04]],\n",
       "              \n",
       "                       [[ 2.3522e-02,  2.8027e-02, -2.6214e-02],\n",
       "                        [ 2.3080e-02, -8.7814e-03, -4.6104e-02],\n",
       "                        [ 3.0884e-02, -1.9359e-03, -3.1695e-02]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 3.3846e-02, -1.1196e-02,  3.5954e-02],\n",
       "                        [ 3.8647e-03,  4.6323e-02,  5.7287e-02],\n",
       "                        [ 3.0316e-02, -2.2417e-02,  5.2550e-02]],\n",
       "              \n",
       "                       [[-3.1151e-02, -1.5093e-02,  9.0427e-03],\n",
       "                        [-6.0654e-03, -3.6534e-02, -5.0896e-02],\n",
       "                        [ 1.4993e-03, -2.1160e-02, -6.5796e-03]],\n",
       "              \n",
       "                       [[ 1.0019e-02, -6.1218e-03, -2.7759e-02],\n",
       "                        [ 3.1138e-02,  3.2256e-03, -2.3988e-02],\n",
       "                        [ 5.5298e-02,  1.7096e-02,  2.3524e-02]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-4.4279e-02, -3.9350e-02,  4.4293e-02],\n",
       "                        [-2.4263e-02, -4.4582e-02,  1.5868e-02],\n",
       "                        [-1.7141e-02,  2.4779e-02, -3.4755e-02]],\n",
       "              \n",
       "                       [[ 3.6598e-03,  3.4609e-02,  3.5382e-02],\n",
       "                        [-4.3784e-02, -1.2043e-03,  6.6929e-03],\n",
       "                        [ 1.2912e-02, -6.9257e-03,  2.9313e-02]],\n",
       "              \n",
       "                       [[-4.6430e-02, -1.0305e-02, -4.4180e-03],\n",
       "                        [-4.0226e-02, -3.5298e-02,  3.1019e-02],\n",
       "                        [-2.6268e-02,  9.8735e-03, -1.3932e-02]]],\n",
       "              \n",
       "              \n",
       "                      ...,\n",
       "              \n",
       "              \n",
       "                      [[[-7.5312e-03,  1.7777e-02, -2.4650e-02],\n",
       "                        [-1.9653e-02,  5.0097e-02,  4.1339e-02],\n",
       "                        [-3.4568e-03,  4.0268e-02, -2.3635e-02]],\n",
       "              \n",
       "                       [[-1.3264e-02,  9.5604e-03,  2.9152e-02],\n",
       "                        [-4.8186e-02, -6.3873e-03,  8.1678e-03],\n",
       "                        [-1.8558e-02, -2.8349e-02, -4.1475e-02]],\n",
       "              \n",
       "                       [[ 3.2057e-02,  2.7495e-02,  3.5577e-02],\n",
       "                        [-1.6831e-02,  1.8242e-02,  8.7315e-03],\n",
       "                        [ 3.7248e-02, -4.3961e-03, -3.4689e-02]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-2.4200e-02, -2.3704e-02, -4.4389e-05],\n",
       "                        [ 1.3069e-02,  1.2457e-02,  2.9222e-02],\n",
       "                        [-2.7209e-02,  1.8669e-02, -7.4192e-03]],\n",
       "              \n",
       "                       [[-2.5644e-02,  9.0848e-03,  5.0345e-02],\n",
       "                        [ 1.9784e-02,  6.6598e-03,  9.0474e-03],\n",
       "                        [-5.4224e-03,  1.2946e-02,  4.1164e-02]],\n",
       "              \n",
       "                       [[-1.1942e-03,  1.3998e-02,  3.0734e-02],\n",
       "                        [ 3.4851e-02,  2.4650e-02,  2.1934e-02],\n",
       "                        [-4.2248e-02,  6.8486e-03, -2.4653e-02]]],\n",
       "              \n",
       "              \n",
       "                      [[[-2.4381e-02, -7.8940e-03,  2.6693e-02],\n",
       "                        [ 2.6045e-02,  1.3325e-02, -2.8323e-02],\n",
       "                        [-7.2153e-03,  2.2680e-03,  1.7097e-02]],\n",
       "              \n",
       "                       [[-2.8275e-02, -4.9965e-03, -1.1801e-02],\n",
       "                        [ 1.2612e-02, -2.8636e-02, -3.0702e-02],\n",
       "                        [ 5.9864e-04, -2.9123e-02, -6.5871e-02]],\n",
       "              \n",
       "                       [[ 3.7677e-02, -2.3530e-02, -3.3771e-04],\n",
       "                        [ 1.4232e-02,  1.6856e-02,  3.9913e-03],\n",
       "                        [-1.9965e-02, -2.8321e-02, -1.4122e-02]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-4.4638e-02,  2.3273e-02, -3.1368e-02],\n",
       "                        [-4.5833e-02, -5.1115e-03,  1.7248e-02],\n",
       "                        [ 2.3055e-02, -2.4363e-02, -8.8707e-03]],\n",
       "              \n",
       "                       [[-2.8866e-02,  1.6965e-02, -1.5759e-02],\n",
       "                        [ 3.2381e-02,  1.0222e-02,  5.9872e-02],\n",
       "                        [-3.2331e-02,  5.5793e-02, -9.6861e-03]],\n",
       "              \n",
       "                       [[-1.9731e-02, -6.6928e-04,  3.6008e-02],\n",
       "                        [ 1.8864e-02, -2.8762e-02, -1.7051e-02],\n",
       "                        [-3.4973e-02, -1.0365e-02, -1.1639e-03]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 1.8542e-02, -2.3088e-02,  1.5857e-02],\n",
       "                        [-3.8407e-02, -3.2629e-02, -3.0930e-02],\n",
       "                        [-4.8843e-02,  1.8065e-03, -1.0042e-02]],\n",
       "              \n",
       "                       [[-5.4520e-03,  6.5934e-03, -6.0470e-03],\n",
       "                        [-1.0173e-02,  1.7176e-02,  4.1810e-04],\n",
       "                        [ 4.4571e-02,  3.2610e-02,  1.8906e-02]],\n",
       "              \n",
       "                       [[-4.7146e-02, -2.3480e-02,  2.2951e-02],\n",
       "                        [-9.2822e-03,  1.5473e-02, -1.3384e-02],\n",
       "                        [-8.1222e-03, -2.4355e-02, -4.7760e-02]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 1.5674e-02, -2.1423e-02, -2.6956e-02],\n",
       "                        [-2.5941e-02, -4.4649e-02,  1.2552e-02],\n",
       "                        [-4.4854e-02, -4.6966e-02, -2.6388e-02]],\n",
       "              \n",
       "                       [[ 3.1081e-02,  1.2784e-02,  2.0320e-02],\n",
       "                        [ 1.8182e-02,  1.9909e-02,  7.8186e-03],\n",
       "                        [ 1.9265e-02, -1.9657e-02, -1.5851e-02]],\n",
       "              \n",
       "                       [[-2.9672e-02, -5.7446e-02, -5.2944e-02],\n",
       "                        [-5.2958e-02, -1.9681e-02,  2.3733e-02],\n",
       "                        [-4.4552e-02, -1.8982e-02, -1.2571e-02]]]])),\n",
       "             ('module.decoder.net.9.bias',\n",
       "              tensor([ 3.2257e-02, -3.6025e-02,  1.7843e-02, -9.4365e-03,  1.9432e-02,\n",
       "                       3.4797e-02,  2.4108e-02, -2.8618e-02,  2.5076e-02,  2.9218e-02,\n",
       "                      -4.1863e-02,  2.2649e-02,  2.2408e-02,  2.8246e-02,  7.8870e-03,\n",
       "                       1.9746e-02, -3.1861e-02,  2.7076e-02, -2.2462e-02, -3.4282e-02,\n",
       "                      -1.1150e-03,  3.4913e-02,  7.2656e-03,  5.8897e-03,  1.4966e-02,\n",
       "                       3.6906e-02,  7.0810e-03, -9.9856e-03, -4.4772e-03, -2.8432e-02,\n",
       "                       1.0837e-02, -2.6706e-02, -1.4396e-02, -1.8138e-02,  2.5438e-02,\n",
       "                      -3.8562e-03,  3.7676e-04,  1.6452e-02,  3.6232e-02, -1.7972e-02,\n",
       "                      -1.8679e-02,  2.2688e-02, -2.9627e-02, -3.9498e-02,  5.2869e-03,\n",
       "                      -3.5372e-02, -3.1619e-02,  4.0234e-02,  4.7558e-02, -1.0827e-02,\n",
       "                       2.1327e-03, -5.7433e-03,  2.2687e-02, -7.4019e-04,  5.4749e-04,\n",
       "                       3.5292e-05, -3.4603e-03, -1.4056e-02,  2.6944e-02, -2.5082e-03,\n",
       "                      -3.3548e-02,  3.0964e-02, -3.0950e-02, -3.3472e-02])),\n",
       "             ('module.decoder.net.10.weight',\n",
       "              tensor([0.9837, 0.9704, 1.0026, 1.0429, 0.9888, 0.9751, 0.9954, 1.0260, 1.0094,\n",
       "                      1.0107, 1.0239, 0.9954, 1.0124, 1.0074, 1.0074, 0.9854, 1.0237, 1.0060,\n",
       "                      0.9463, 0.9901, 0.9966, 1.0113, 0.9905, 1.0660, 0.9952, 0.9700, 0.9884,\n",
       "                      1.0094, 0.9821, 1.0216, 0.9917, 0.9872, 0.9762, 0.9838, 0.9946, 0.9944,\n",
       "                      0.9844, 0.9798, 0.9668, 0.9824, 0.9919, 0.9978, 0.9760, 1.0549, 1.0006,\n",
       "                      0.9941, 0.9920, 1.0027, 0.9890, 1.0051, 1.0244, 0.9164, 0.9940, 1.0066,\n",
       "                      0.9994, 0.9978, 0.9853, 0.9742, 1.0272, 1.0172, 1.0228, 1.0154, 1.0078,\n",
       "                      0.9807])),\n",
       "             ('module.decoder.net.10.bias',\n",
       "              tensor([-0.0242, -0.0327, -0.0095, -0.1444, -0.0101, -0.0653,  0.0103,  0.0239,\n",
       "                      -0.0142,  0.0155, -0.0638, -0.0199,  0.0226, -0.1862, -0.0974,  0.0051,\n",
       "                      -0.0510,  0.0259, -0.0297,  0.0098, -0.0002,  0.0386, -0.0278, -0.1694,\n",
       "                      -0.0107, -0.0216, -0.1044, -0.1219,  0.0217, -0.0424,  0.0086, -0.0159,\n",
       "                      -0.0111, -0.0657,  0.0231,  0.0286,  0.0214, -0.0183, -0.0406, -0.0109,\n",
       "                      -0.0015, -0.0860, -0.0114, -0.1798, -0.0045, -0.0922, -0.1455, -0.0106,\n",
       "                       0.0054,  0.0096, -0.1385, -0.0837, -0.0065, -0.0855, -0.0042, -0.0080,\n",
       "                      -0.0029, -0.0313, -0.1297, -0.0853,  0.0312,  0.0248,  0.0011,  0.0150])),\n",
       "             ('module.decoder.net.10.running_mean',\n",
       "              tensor([-0.2936, -0.6547, -0.1552,  0.4329, -1.0062, -0.0376, -0.7719, -1.0518,\n",
       "                      -0.0472, -0.9958, -0.6950, -0.2218, -0.9596,  0.1484, -0.3403, -0.6825,\n",
       "                      -0.3590,  0.1173,  0.3982, -0.4503, -0.6684, -0.8134, -0.5967,  0.4547,\n",
       "                      -0.0993, -0.0069,  0.0920, -0.3834, -1.0125, -0.0372, -0.6001, -0.4650,\n",
       "                       0.2933, -0.3452, -0.8234, -0.0150, -0.3252, -0.6733, -0.8685, -0.0602,\n",
       "                       0.5356, -0.1612, -0.7469,  0.2310,  0.3954, -0.2210,  0.2912,  0.5988,\n",
       "                      -0.3847, -0.3113,  0.0921,  0.1145, -0.9080,  0.5548, -0.7494, -0.5454,\n",
       "                      -1.1038, -0.8644, -0.2665, -0.4994, -0.1933, -0.3147, -0.9126, -1.3351])),\n",
       "             ('module.decoder.net.10.running_var',\n",
       "              tensor([1.0950, 0.6607, 1.8759, 0.3637, 0.6438, 0.5832, 0.7937, 1.0231, 0.5599,\n",
       "                      1.1601, 0.5099, 1.0611, 1.0669, 0.4904, 0.6954, 1.0098, 0.6586, 2.2583,\n",
       "                      1.6914, 2.3246, 1.0779, 1.0992, 1.4687, 0.3945, 1.9446, 0.8265, 0.7169,\n",
       "                      0.5891, 1.1582, 0.7489, 0.3580, 0.9484, 1.9858, 0.3301, 0.8606, 1.3956,\n",
       "                      0.6024, 0.5219, 2.5487, 0.6199, 2.5569, 0.5275, 1.0437, 0.6233, 2.9374,\n",
       "                      0.5444, 1.4086, 2.0471, 1.2877, 1.1929, 0.5097, 0.9784, 0.8435, 0.6182,\n",
       "                      1.0687, 1.5489, 1.5449, 0.7390, 0.7229, 0.6670, 1.0215, 1.1312, 0.9165,\n",
       "                      1.3388])),\n",
       "             ('module.decoder.net.10.num_batches_tracked', tensor(0)),\n",
       "             ('module.decoder.net.13.weight',\n",
       "              tensor([[[[-1.7357e-02, -1.1333e-03, -3.1786e-02],\n",
       "                        [-1.4149e-02,  1.1520e-02,  1.4453e-02],\n",
       "                        [ 3.1262e-02, -9.5272e-03,  1.4333e-02]],\n",
       "              \n",
       "                       [[ 3.8400e-02,  4.0562e-02,  2.0451e-03],\n",
       "                        [-1.5145e-02,  1.9831e-03, -4.9333e-03],\n",
       "                        [-2.4389e-02, -2.9632e-02,  2.2519e-02]],\n",
       "              \n",
       "                       [[-3.7179e-02, -2.3495e-02, -8.6114e-03],\n",
       "                        [-5.2899e-02,  1.8750e-02, -4.2142e-03],\n",
       "                        [-5.4670e-02, -4.7502e-02,  1.6975e-02]],\n",
       "              \n",
       "                       [[-5.1061e-02, -1.1220e-02,  3.2404e-02],\n",
       "                        [-5.7323e-02,  3.3654e-02,  4.0942e-02],\n",
       "                        [-3.6340e-02, -2.2595e-03,  3.1877e-02]],\n",
       "              \n",
       "                       [[ 3.4380e-02, -3.8585e-02,  3.8324e-02],\n",
       "                        [ 1.6667e-02,  3.6300e-02,  3.5729e-02],\n",
       "                        [-2.5926e-02, -3.8132e-02, -3.8894e-02]],\n",
       "              \n",
       "                       [[-5.9425e-03, -1.1403e-02,  6.4247e-05],\n",
       "                        [ 1.3943e-02,  2.4408e-02,  3.9325e-02],\n",
       "                        [-3.0822e-02, -3.0047e-02,  7.7995e-03]],\n",
       "              \n",
       "                       [[-1.6413e-02,  3.4812e-02,  2.4610e-02],\n",
       "                        [ 6.3261e-03,  2.3862e-02, -1.3611e-02],\n",
       "                        [ 1.3192e-02,  4.3348e-02,  8.8683e-03]],\n",
       "              \n",
       "                       [[ 3.3991e-02, -8.4524e-03,  8.6471e-03],\n",
       "                        [-2.9690e-02, -7.7016e-03,  3.2162e-02],\n",
       "                        [ 2.1274e-02, -1.1334e-03, -1.8880e-02]],\n",
       "              \n",
       "                       [[ 8.6979e-03,  1.5562e-02, -8.1590e-03],\n",
       "                        [ 1.0040e-04, -2.3199e-02, -2.4255e-03],\n",
       "                        [ 2.9186e-02,  3.8975e-02,  1.2992e-02]],\n",
       "              \n",
       "                       [[-4.7037e-02,  2.6720e-02, -2.0676e-02],\n",
       "                        [-1.2623e-02,  1.1590e-02, -4.8452e-03],\n",
       "                        [-5.1327e-02, -2.6391e-02,  2.4948e-02]],\n",
       "              \n",
       "                       [[ 3.3895e-03, -4.5888e-02, -3.6604e-02],\n",
       "                        [ 1.2425e-02,  2.5441e-02,  1.6980e-02],\n",
       "                        [-3.6592e-02,  1.9115e-02,  1.1082e-02]],\n",
       "              \n",
       "                       [[ 5.8385e-03, -3.3845e-02, -8.7400e-04],\n",
       "                        [-6.2394e-03, -3.9291e-02,  3.5949e-02],\n",
       "                        [-9.0005e-03,  8.9487e-03, -1.4928e-02]],\n",
       "              \n",
       "                       [[ 4.4152e-02, -1.5874e-02,  1.6820e-02],\n",
       "                        [ 2.6608e-04,  3.6212e-02,  3.2342e-02],\n",
       "                        [ 1.0731e-02,  5.5735e-02, -1.6578e-02]],\n",
       "              \n",
       "                       [[-1.2261e-02, -2.4326e-02, -1.1854e-02],\n",
       "                        [-1.2140e-02,  3.8476e-02,  3.1819e-02],\n",
       "                        [ 2.4572e-02,  1.5639e-02,  1.7000e-02]],\n",
       "              \n",
       "                       [[-2.2316e-02, -4.1679e-02,  3.4389e-02],\n",
       "                        [-3.9989e-02, -7.6417e-03,  1.6491e-02],\n",
       "                        [ 5.9174e-03,  3.6491e-02,  2.1180e-03]],\n",
       "              \n",
       "                       [[-2.2199e-02,  3.4184e-02, -2.9566e-02],\n",
       "                        [ 7.9542e-03,  6.7238e-03, -1.7521e-02],\n",
       "                        [ 2.5556e-02,  2.3671e-02,  3.9584e-02]],\n",
       "              \n",
       "                       [[-6.4203e-03, -3.7531e-02,  4.3992e-02],\n",
       "                        [-1.4186e-02,  9.9089e-03, -2.6291e-02],\n",
       "                        [-2.3425e-02, -4.6276e-03,  4.5649e-02]],\n",
       "              \n",
       "                       [[-3.2176e-02, -3.9750e-02, -2.0488e-02],\n",
       "                        [ 1.3833e-02, -1.2348e-02,  9.6542e-03],\n",
       "                        [-5.1677e-02, -3.7801e-02, -1.1435e-02]],\n",
       "              \n",
       "                       [[-5.4024e-02, -3.0917e-02, -3.8053e-02],\n",
       "                        [-2.1896e-02,  2.9554e-02,  5.7971e-03],\n",
       "                        [-1.9803e-02, -2.1081e-02,  1.3528e-02]],\n",
       "              \n",
       "                       [[-4.2790e-02,  2.3730e-02, -2.5381e-02],\n",
       "                        [-2.6416e-02, -2.1776e-02,  9.1662e-03],\n",
       "                        [-1.4734e-02, -9.4973e-03, -3.8793e-02]],\n",
       "              \n",
       "                       [[ 8.1732e-03, -4.5521e-02, -5.6396e-02],\n",
       "                        [-6.1161e-03,  2.3185e-02, -3.4207e-02],\n",
       "                        [-1.7335e-02, -7.8180e-03, -2.2243e-02]],\n",
       "              \n",
       "                       [[-1.4717e-02, -1.1387e-02, -3.4748e-02],\n",
       "                        [ 2.5999e-02, -5.6323e-03, -2.1243e-02],\n",
       "                        [-2.3810e-02, -4.8499e-02,  1.8130e-02]],\n",
       "              \n",
       "                       [[-3.0949e-02,  3.6549e-02,  3.8780e-02],\n",
       "                        [-6.8815e-03,  3.3334e-02, -4.4879e-02],\n",
       "                        [ 3.6947e-02, -3.9280e-02, -1.3549e-02]],\n",
       "              \n",
       "                       [[ 3.2822e-02,  2.2189e-02,  5.1911e-04],\n",
       "                        [ 1.0880e-02,  2.8960e-02, -5.0893e-02],\n",
       "                        [ 5.4478e-02,  4.8640e-03, -8.4250e-02]],\n",
       "              \n",
       "                       [[-2.9630e-02,  5.9245e-03, -4.0135e-02],\n",
       "                        [ 8.6161e-03, -3.6669e-02, -3.6339e-02],\n",
       "                        [-4.2391e-02, -3.7327e-02,  5.4629e-03]],\n",
       "              \n",
       "                       [[-4.2554e-02, -2.7660e-02, -2.8694e-02],\n",
       "                        [-2.6867e-02, -3.5357e-02,  2.7995e-02],\n",
       "                        [-1.6762e-02,  2.2154e-03,  2.2530e-02]],\n",
       "              \n",
       "                       [[-1.1266e-02, -3.0388e-02,  2.6391e-02],\n",
       "                        [-3.4009e-02,  3.9098e-02,  2.0380e-02],\n",
       "                        [ 1.7112e-02,  2.7117e-02, -3.0555e-02]],\n",
       "              \n",
       "                       [[ 4.7433e-02, -2.2469e-02, -1.5985e-02],\n",
       "                        [-1.0181e-02,  3.7378e-02,  4.4380e-02],\n",
       "                        [-3.2229e-02,  3.6144e-02, -1.8463e-02]],\n",
       "              \n",
       "                       [[-2.3655e-02,  2.7051e-02,  2.2141e-02],\n",
       "                        [ 2.2954e-02, -3.7539e-02,  8.7571e-03],\n",
       "                        [ 1.6986e-02, -1.6827e-03,  2.9803e-03]],\n",
       "              \n",
       "                       [[ 1.1969e-02,  1.3760e-02, -3.5987e-02],\n",
       "                        [-1.6330e-02,  1.4444e-02,  1.2973e-02],\n",
       "                        [ 2.5609e-02, -3.6943e-04,  1.1554e-02]],\n",
       "              \n",
       "                       [[-3.2838e-02,  1.2501e-02, -9.0685e-03],\n",
       "                        [-1.6103e-02,  1.3962e-02,  2.9244e-02],\n",
       "                        [-3.9747e-02,  2.0362e-02,  4.2750e-02]],\n",
       "              \n",
       "                       [[-1.5050e-02,  1.3196e-02,  1.0359e-02],\n",
       "                        [-3.5827e-02, -2.8844e-02,  2.8464e-02],\n",
       "                        [ 3.1546e-03, -3.6723e-02,  3.0935e-02]],\n",
       "              \n",
       "                       [[ 2.6140e-02, -3.9190e-02, -2.0950e-02],\n",
       "                        [-4.0866e-02, -4.5257e-02,  2.3748e-02],\n",
       "                        [-2.5085e-03, -3.5559e-02, -2.7511e-02]],\n",
       "              \n",
       "                       [[ 9.5568e-03,  1.3967e-02, -2.5731e-02],\n",
       "                        [-8.7342e-03,  2.9059e-02, -3.0116e-02],\n",
       "                        [ 2.4309e-02, -3.0227e-02,  1.1447e-02]],\n",
       "              \n",
       "                       [[ 9.0933e-03,  7.2425e-03,  5.3777e-03],\n",
       "                        [-3.1536e-02, -7.5789e-03, -1.4243e-02],\n",
       "                        [ 2.8936e-02, -3.8706e-02, -4.9707e-02]],\n",
       "              \n",
       "                       [[ 9.1481e-03, -2.3582e-03, -9.2890e-03],\n",
       "                        [ 1.0835e-02, -1.4783e-02, -4.6661e-02],\n",
       "                        [-3.0805e-02, -2.1978e-02, -3.8723e-02]],\n",
       "              \n",
       "                       [[ 2.8028e-02,  3.9164e-02, -3.8168e-02],\n",
       "                        [-3.5382e-02, -1.0255e-02, -3.6188e-02],\n",
       "                        [ 1.1341e-02, -3.7000e-02,  2.4044e-02]],\n",
       "              \n",
       "                       [[-1.7066e-02, -1.3779e-02, -1.0863e-02],\n",
       "                        [-8.6836e-04,  2.1893e-02,  2.3659e-02],\n",
       "                        [-5.3407e-02,  1.8499e-02,  2.7211e-02]],\n",
       "              \n",
       "                       [[ 2.2317e-02,  3.2695e-02, -1.0804e-02],\n",
       "                        [ 1.9964e-02,  7.7237e-03, -2.1851e-02],\n",
       "                        [ 3.9366e-02, -2.6307e-02,  2.0419e-04]],\n",
       "              \n",
       "                       [[ 1.1023e-02, -1.7385e-02,  3.6723e-03],\n",
       "                        [ 2.2868e-02,  1.8355e-02, -3.9450e-02],\n",
       "                        [-3.3053e-03,  8.5069e-03,  2.4286e-02]],\n",
       "              \n",
       "                       [[ 2.5193e-02, -4.5983e-02, -3.4187e-02],\n",
       "                        [-5.4915e-02, -1.6594e-02, -5.5872e-02],\n",
       "                        [-7.1965e-03, -4.7173e-02,  9.0815e-03]],\n",
       "              \n",
       "                       [[-2.4755e-02,  3.3078e-02,  2.7804e-02],\n",
       "                        [ 2.8716e-04, -9.0149e-03,  3.3774e-02],\n",
       "                        [-4.0794e-02,  3.1337e-02,  3.0258e-02]],\n",
       "              \n",
       "                       [[-8.6721e-03,  2.7629e-02, -2.6745e-02],\n",
       "                        [ 2.4472e-02,  1.7260e-02, -8.6918e-03],\n",
       "                        [ 1.2679e-02,  1.4895e-02, -2.9262e-02]],\n",
       "              \n",
       "                       [[ 4.1854e-03, -3.4177e-02,  2.0163e-02],\n",
       "                        [ 1.3654e-02, -2.6851e-02,  9.1615e-03],\n",
       "                        [-2.2959e-02,  4.8321e-02,  1.9987e-02]],\n",
       "              \n",
       "                       [[-2.7094e-02,  6.4108e-03,  9.5176e-03],\n",
       "                        [-5.3458e-02, -5.1937e-02, -4.9228e-02],\n",
       "                        [-4.8264e-02,  1.1070e-02, -3.9805e-02]],\n",
       "              \n",
       "                       [[-2.2601e-02,  3.3870e-02, -2.0180e-02],\n",
       "                        [-1.7595e-02,  3.8121e-02,  4.1339e-02],\n",
       "                        [ 9.9567e-03, -2.3455e-04, -3.1853e-02]],\n",
       "              \n",
       "                       [[ 2.3306e-02,  3.1327e-02,  3.1345e-03],\n",
       "                        [-1.8892e-02,  2.5497e-02,  1.0544e-02],\n",
       "                        [ 1.7918e-02,  2.0140e-04,  3.2506e-02]],\n",
       "              \n",
       "                       [[-4.2532e-02, -5.4988e-02, -4.4743e-02],\n",
       "                        [-4.3632e-02, -4.9153e-02, -3.0284e-02],\n",
       "                        [ 1.4961e-02, -5.5261e-02, -4.1166e-02]],\n",
       "              \n",
       "                       [[-3.0489e-02, -4.0100e-02, -5.2761e-02],\n",
       "                        [-3.3290e-02, -6.6249e-03,  1.5035e-02],\n",
       "                        [ 1.7155e-02, -1.2571e-02, -2.1268e-02]],\n",
       "              \n",
       "                       [[-2.9026e-02, -2.1442e-02, -1.5036e-02],\n",
       "                        [-1.5048e-02, -1.4386e-02,  4.6853e-02],\n",
       "                        [ 5.8783e-02,  5.1897e-02, -1.0020e-02]],\n",
       "              \n",
       "                       [[ 1.6461e-02,  2.6155e-02,  2.2703e-02],\n",
       "                        [-3.9174e-03, -1.1755e-02,  3.8065e-02],\n",
       "                        [ 5.1855e-03,  2.0727e-02,  4.2477e-02]],\n",
       "              \n",
       "                       [[-2.6612e-03,  3.8799e-02, -1.8210e-02],\n",
       "                        [-2.8439e-02,  1.0678e-02,  2.3842e-02],\n",
       "                        [-1.8099e-02, -4.2308e-03, -9.9566e-03]],\n",
       "              \n",
       "                       [[ 2.1752e-02, -2.3703e-02, -1.3215e-02],\n",
       "                        [ 1.1966e-02, -4.2310e-03, -1.4545e-02],\n",
       "                        [ 1.1683e-03,  1.5596e-02, -6.6282e-04]],\n",
       "              \n",
       "                       [[-3.6005e-02,  3.7958e-02,  8.7489e-03],\n",
       "                        [ 2.0206e-02, -2.1820e-02, -9.8614e-03],\n",
       "                        [ 3.2928e-02,  4.2488e-02,  1.3851e-02]],\n",
       "              \n",
       "                       [[ 2.9603e-02, -2.7094e-02, -3.7101e-02],\n",
       "                        [-3.7127e-02, -2.3441e-02, -3.0507e-02],\n",
       "                        [ 3.1063e-02, -2.1038e-02, -1.7052e-02]],\n",
       "              \n",
       "                       [[-3.1960e-02, -1.2170e-02,  6.9552e-03],\n",
       "                        [-1.8973e-02, -1.4161e-02, -7.4833e-03],\n",
       "                        [-2.1555e-02, -3.4915e-02, -3.1680e-02]],\n",
       "              \n",
       "                       [[ 2.8455e-02, -1.0606e-02, -1.4047e-02],\n",
       "                        [ 3.5111e-02,  1.2565e-02, -8.2748e-03],\n",
       "                        [-2.4322e-02, -6.2712e-03,  3.6095e-02]],\n",
       "              \n",
       "                       [[ 1.9689e-02,  1.3436e-02, -2.3246e-02],\n",
       "                        [ 2.6729e-02, -2.7764e-02, -2.3776e-02],\n",
       "                        [ 1.8812e-02, -7.6045e-03,  2.9281e-02]],\n",
       "              \n",
       "                       [[ 1.8226e-02, -1.9144e-02, -3.7025e-02],\n",
       "                        [ 3.5450e-02, -1.3215e-02, -2.9041e-02],\n",
       "                        [ 3.2344e-02,  2.2588e-02, -1.8434e-02]],\n",
       "              \n",
       "                       [[-2.0114e-02,  1.8983e-03, -1.3852e-02],\n",
       "                        [-4.3964e-02, -2.3030e-02,  3.1741e-02],\n",
       "                        [ 2.7711e-02, -4.6159e-03,  4.6377e-02]],\n",
       "              \n",
       "                       [[-5.0072e-03, -4.4552e-02, -4.8214e-02],\n",
       "                        [ 1.9109e-02,  4.4022e-03,  2.4434e-03],\n",
       "                        [ 5.4416e-03, -3.1368e-02, -2.7619e-02]],\n",
       "              \n",
       "                       [[-2.1889e-02,  4.1701e-04, -4.5122e-02],\n",
       "                        [-1.2481e-02,  1.2243e-02, -4.0301e-02],\n",
       "                        [-2.0882e-02,  3.2548e-02, -1.3993e-03]],\n",
       "              \n",
       "                       [[ 1.5402e-02, -1.8236e-02, -3.8977e-02],\n",
       "                        [-2.1957e-02,  1.0027e-02,  7.0522e-03],\n",
       "                        [ 2.9413e-02, -4.3002e-02, -3.1973e-02]],\n",
       "              \n",
       "                       [[ 1.3532e-02, -7.1191e-03,  5.9014e-03],\n",
       "                        [ 2.6816e-02,  7.5056e-03, -2.6586e-02],\n",
       "                        [ 2.2488e-02,  2.4194e-02,  3.6047e-02]]]])),\n",
       "             ('module.decoder.net.13.bias', tensor([0.2102])),\n",
       "             ('module.decoder.net.14.weight', tensor([1.1511])),\n",
       "             ('module.decoder.net.14.bias', tensor([-0.4406])),\n",
       "             ('module.decoder.net.14.running_mean', tensor([0.0464])),\n",
       "             ('module.decoder.net.14.running_var', tensor([3.8893])),\n",
       "             ('module.decoder.net.14.num_batches_tracked', tensor(0)),\n",
       "             ('module.classifier_vae.encoder.net.0.weight',\n",
       "              tensor([[[[ 1.4440e-01,  9.8252e-02, -2.4057e-01],\n",
       "                        [-1.8567e-01, -1.0225e-01, -1.4140e-01],\n",
       "                        [ 1.2879e-01, -2.6759e-01,  2.4943e-01]]],\n",
       "              \n",
       "              \n",
       "                      [[[-2.0420e-01,  3.5261e-01,  2.9945e-01],\n",
       "                        [ 2.5720e-01, -1.0059e-01,  1.3406e-01],\n",
       "                        [ 1.5064e-01,  2.0771e-01, -2.0646e-01]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 6.7563e-02,  1.8503e-01,  2.0294e-01],\n",
       "                        [ 1.3162e-02, -7.4823e-02,  7.0902e-02],\n",
       "                        [-4.0840e-02,  3.5041e-01,  2.5611e-01]]],\n",
       "              \n",
       "              \n",
       "                      [[[-3.9582e-02,  2.5191e-01, -2.2719e-01],\n",
       "                        [-2.1623e-01,  2.2924e-02, -2.8187e-01],\n",
       "                        [ 1.4102e-01, -5.5157e-02,  2.1213e-01]]],\n",
       "              \n",
       "              \n",
       "                      [[[-1.2675e-01, -9.5493e-02,  2.4762e-01],\n",
       "                        [ 2.9220e-01,  2.5999e-01, -5.9564e-02],\n",
       "                        [ 2.0041e-02,  1.3406e-01, -1.0737e-01]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 1.5774e-02, -3.2178e-01, -3.1879e-02],\n",
       "                        [-1.6245e-01,  7.4549e-02,  1.6293e-01],\n",
       "                        [-1.0380e-02, -1.3999e-01, -3.6981e-03]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 1.3193e-01, -1.9277e-01,  2.6895e-01],\n",
       "                        [ 1.9566e-01, -3.7550e-02, -1.2181e-01],\n",
       "                        [-1.2836e-01, -1.5128e-01,  5.9149e-02]]],\n",
       "              \n",
       "              \n",
       "                      [[[-8.5115e-02,  3.3039e-01, -3.2153e-01],\n",
       "                        [ 1.9859e-01,  2.4100e-01,  9.7664e-02],\n",
       "                        [ 1.4665e-01, -2.7060e-01, -1.7684e-01]]],\n",
       "              \n",
       "              \n",
       "                      [[[-2.9812e-01, -2.2809e-01, -6.8039e-02],\n",
       "                        [ 2.9600e-01, -4.9546e-02, -3.3224e-01],\n",
       "                        [ 1.8697e-01,  2.2355e-01, -2.9525e-01]]],\n",
       "              \n",
       "              \n",
       "                      [[[-5.1758e-02,  2.1881e-01,  2.3480e-01],\n",
       "                        [-1.6983e-01,  8.8905e-02,  5.6792e-03],\n",
       "                        [-1.6093e-01,  2.9908e-01, -9.6718e-02]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 3.0913e-01,  1.4517e-01, -9.9772e-02],\n",
       "                        [ 1.4072e-01,  2.0894e-01,  2.9602e-02],\n",
       "                        [-2.8343e-01,  3.2256e-01,  1.2008e-01]]],\n",
       "              \n",
       "              \n",
       "                      [[[-3.5005e-01,  3.5347e-01, -7.7499e-02],\n",
       "                        [ 1.7249e-01,  1.3529e-01, -3.0643e-01],\n",
       "                        [-1.8604e-01,  3.1959e-01, -2.3623e-01]]],\n",
       "              \n",
       "              \n",
       "                      [[[-6.6493e-02,  2.7897e-01, -1.0096e-01],\n",
       "                        [-8.6033e-02,  6.6035e-02, -5.6127e-02],\n",
       "                        [ 2.9404e-02, -2.5229e-01,  1.7870e-01]]],\n",
       "              \n",
       "              \n",
       "                      [[[-6.2018e-02, -3.4222e-02,  3.0184e-01],\n",
       "                        [-3.3568e-01,  8.0097e-02, -1.8187e-01],\n",
       "                        [-2.5875e-01, -3.1970e-01,  2.2277e-01]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 1.7909e-02,  3.6697e-01,  2.8905e-02],\n",
       "                        [ 2.2903e-01, -1.9181e-01,  1.3058e-01],\n",
       "                        [ 2.1568e-01,  3.1891e-01,  2.6200e-02]]],\n",
       "              \n",
       "              \n",
       "                      [[[-2.4862e-01, -1.0274e-01,  8.9806e-02],\n",
       "                        [ 3.2148e-01,  6.8766e-03,  1.7089e-01],\n",
       "                        [-1.2950e-01, -6.7403e-03, -1.9281e-01]]],\n",
       "              \n",
       "              \n",
       "                      [[[-6.2794e-02,  3.6094e-01, -2.4777e-01],\n",
       "                        [ 5.8269e-02, -2.0383e-01,  5.6466e-02],\n",
       "                        [ 9.0549e-03, -1.9439e-01,  2.3694e-01]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 3.1555e-02, -9.9211e-02, -3.3719e-01],\n",
       "                        [-1.3772e-01, -1.2858e-01,  9.0863e-02],\n",
       "                        [ 1.8156e-01,  6.9035e-02,  2.6072e-01]]],\n",
       "              \n",
       "              \n",
       "                      [[[-2.7827e-02, -1.2570e-01,  4.7579e-02],\n",
       "                        [-3.6836e-01, -1.6445e-01,  8.6251e-03],\n",
       "                        [ 1.5230e-01, -2.4140e-02,  2.8650e-01]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 5.5374e-02, -3.3993e-01, -1.3253e-01],\n",
       "                        [-1.6553e-01,  4.4352e-02, -3.0398e-01],\n",
       "                        [ 2.2311e-01,  2.5545e-01,  1.7411e-01]]],\n",
       "              \n",
       "              \n",
       "                      [[[-3.5166e-01,  6.7499e-02,  2.9849e-01],\n",
       "                        [-2.6124e-01, -1.6934e-01, -1.1528e-01],\n",
       "                        [ 1.0207e-02, -2.3935e-01,  2.3521e-01]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 7.1127e-02, -7.0043e-02,  3.8254e-02],\n",
       "                        [ 2.3816e-03,  5.3564e-02,  2.0193e-01],\n",
       "                        [ 2.2476e-01,  1.5483e-01,  6.5290e-02]]],\n",
       "              \n",
       "              \n",
       "                      [[[-1.8354e-02,  2.2049e-01, -3.5563e-01],\n",
       "                        [ 4.2968e-02, -1.1006e-01,  3.3100e-03],\n",
       "                        [ 3.7402e-02, -2.1918e-01, -7.1332e-02]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 2.8179e-01, -1.6619e-01, -2.3893e-01],\n",
       "                        [-2.0521e-01,  5.2052e-02,  1.1249e-01],\n",
       "                        [-1.9071e-01, -1.0219e-01, -1.6269e-01]]],\n",
       "              \n",
       "              \n",
       "                      [[[-1.9900e-02, -1.5152e-01, -2.7151e-04],\n",
       "                        [-1.0746e-01,  1.5184e-01, -4.6138e-02],\n",
       "                        [-1.9307e-01, -2.2633e-01,  2.2070e-01]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 6.3673e-02, -3.2861e-02, -1.9396e-01],\n",
       "                        [-3.5733e-02, -1.3116e-01,  1.5579e-01],\n",
       "                        [-4.0398e-02, -3.1531e-01,  2.2769e-01]]],\n",
       "              \n",
       "              \n",
       "                      [[[-1.6158e-01, -2.0562e-01,  1.9023e-01],\n",
       "                        [-3.1876e-01,  3.1825e-01, -2.4834e-01],\n",
       "                        [-2.3560e-01, -1.0541e-01,  2.6943e-01]]],\n",
       "              \n",
       "              \n",
       "                      [[[-4.4316e-02,  1.8733e-01, -2.7432e-01],\n",
       "                        [ 7.5300e-02, -4.6752e-02, -6.9406e-03],\n",
       "                        [-9.2101e-02, -1.7409e-01,  5.2968e-02]]],\n",
       "              \n",
       "              \n",
       "                      [[[-2.7936e-01, -7.5272e-02, -1.8219e-01],\n",
       "                        [-2.4624e-01,  2.2186e-01, -2.3804e-01],\n",
       "                        [-1.0014e-01, -2.4915e-01, -1.2150e-02]]],\n",
       "              \n",
       "              \n",
       "                      [[[-1.8760e-01,  8.1838e-02, -2.9730e-02],\n",
       "                        [-1.2093e-01, -1.7937e-01, -1.5450e-01],\n",
       "                        [ 2.3921e-01, -3.5262e-01, -1.9280e-01]]],\n",
       "              \n",
       "              \n",
       "                      [[[-1.2932e-02,  2.8988e-01, -2.1533e-01],\n",
       "                        [ 3.1490e-01, -7.6659e-02,  3.5767e-01],\n",
       "                        [ 2.1278e-01,  1.2103e-02, -7.5834e-02]]],\n",
       "              \n",
       "              \n",
       "                      [[[-8.8076e-03,  2.4756e-01,  7.4411e-02],\n",
       "                        [ 2.4075e-01, -2.3215e-01, -1.4931e-01],\n",
       "                        [ 2.6116e-01, -5.1098e-02,  2.0031e-01]]],\n",
       "              \n",
       "              \n",
       "                      [[[-2.8530e-01,  5.3085e-02, -1.0604e-02],\n",
       "                        [ 2.5546e-01, -1.6495e-01,  2.7657e-01],\n",
       "                        [ 6.7422e-02, -9.9146e-02, -2.3520e-01]]],\n",
       "              \n",
       "              \n",
       "                      [[[-1.2049e-01, -3.4075e-01, -1.7543e-01],\n",
       "                        [-2.2891e-01,  1.6130e-01,  2.7140e-01],\n",
       "                        [-1.5293e-01, -2.1899e-01,  1.5956e-01]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 1.5297e-01,  2.4653e-01,  3.2804e-01],\n",
       "                        [ 1.0276e-01, -6.8442e-02, -4.0938e-02],\n",
       "                        [ 3.1419e-01,  2.0032e-01, -1.7962e-01]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 1.8275e-01, -3.4498e-01, -5.1648e-02],\n",
       "                        [ 2.1788e-01,  1.4364e-01,  1.6448e-01],\n",
       "                        [-3.3190e-01, -7.8855e-02,  1.6765e-02]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 3.5331e-01, -1.7878e-01,  4.8220e-02],\n",
       "                        [ 2.9156e-01,  2.5254e-01, -1.0145e-01],\n",
       "                        [-3.2803e-02,  2.3545e-01,  3.1020e-01]]],\n",
       "              \n",
       "              \n",
       "                      [[[-2.8676e-02, -1.4280e-01,  1.0218e-02],\n",
       "                        [ 8.6207e-02,  2.0989e-01,  2.5872e-01],\n",
       "                        [-5.0558e-02, -3.0212e-01, -1.1247e-01]]],\n",
       "              \n",
       "              \n",
       "                      [[[-2.1216e-02, -1.7542e-01,  1.2237e-01],\n",
       "                        [ 2.7417e-01,  5.5076e-02,  5.9082e-02],\n",
       "                        [ 1.7375e-01,  2.1356e-01,  2.9982e-01]]],\n",
       "              \n",
       "              \n",
       "                      [[[-3.0889e-01, -1.1682e-02, -2.5077e-01],\n",
       "                        [ 2.5701e-01,  1.1139e-02, -2.9813e-01],\n",
       "                        [ 2.8987e-01, -3.1746e-01,  7.7986e-02]]],\n",
       "              \n",
       "              \n",
       "                      [[[-1.4301e-02, -2.3974e-01,  3.0849e-01],\n",
       "                        [-2.8735e-01,  1.8145e-01,  3.2432e-02],\n",
       "                        [-3.7407e-01, -3.0509e-01,  8.0130e-02]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 2.3639e-01, -2.4570e-01,  4.7371e-02],\n",
       "                        [-1.7890e-01,  2.8219e-01,  1.8110e-02],\n",
       "                        [ 1.4507e-01,  1.4808e-01,  2.3993e-01]]],\n",
       "              \n",
       "              \n",
       "                      [[[-2.4059e-01,  2.6170e-02, -1.3168e-01],\n",
       "                        [ 2.9137e-02,  8.7539e-02,  1.8310e-01],\n",
       "                        [ 3.1315e-01,  3.4920e-01, -1.0389e-01]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 2.1352e-01,  2.0213e-02, -2.9993e-02],\n",
       "                        [ 1.4877e-01,  8.2685e-02,  2.6684e-01],\n",
       "                        [ 8.8268e-02,  1.2411e-01,  2.7928e-01]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 5.5486e-03, -2.3158e-01,  2.8944e-03],\n",
       "                        [ 1.5449e-01, -8.6634e-02,  2.3331e-01],\n",
       "                        [ 1.4207e-01, -2.0750e-01,  3.5528e-02]]],\n",
       "              \n",
       "              \n",
       "                      [[[-3.7653e-02,  2.8862e-01, -1.6586e-01],\n",
       "                        [-2.1241e-02,  8.6253e-03, -2.5089e-01],\n",
       "                        [ 1.5027e-01, -3.1149e-01, -2.1212e-01]]],\n",
       "              \n",
       "              \n",
       "                      [[[-2.4079e-01, -3.4897e-01,  1.0124e-01],\n",
       "                        [-2.4036e-02, -8.3775e-02, -6.0893e-02],\n",
       "                        [-5.0548e-02,  2.1277e-01,  1.9552e-01]]],\n",
       "              \n",
       "              \n",
       "                      [[[-1.0883e-01,  6.2605e-02,  1.6608e-01],\n",
       "                        [ 1.9270e-01,  1.5926e-01,  2.9311e-01],\n",
       "                        [-2.3255e-01,  1.9281e-01, -1.5868e-01]]],\n",
       "              \n",
       "              \n",
       "                      [[[-7.6556e-02, -1.0994e-01,  8.2624e-02],\n",
       "                        [ 1.0743e-01,  1.9658e-01,  1.2980e-01],\n",
       "                        [ 2.3918e-01,  2.1553e-01, -7.7843e-02]]],\n",
       "              \n",
       "              \n",
       "                      [[[-2.6467e-01,  1.9780e-01,  1.3054e-01],\n",
       "                        [-1.4797e-01,  6.8103e-02,  1.3049e-01],\n",
       "                        [-3.0686e-01,  5.6981e-02,  2.3314e-01]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 2.5924e-01,  2.3697e-01, -1.3191e-01],\n",
       "                        [ 2.7357e-01,  2.8823e-01,  3.1673e-01],\n",
       "                        [-9.6567e-02, -1.6777e-01,  3.8120e-02]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 9.7562e-02,  2.3785e-01, -2.4206e-01],\n",
       "                        [-1.5647e-01,  3.0977e-01,  3.0276e-01],\n",
       "                        [ 2.4936e-01, -2.6212e-02,  2.0436e-02]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 1.9711e-01, -3.9818e-02, -2.3082e-01],\n",
       "                        [ 3.3429e-03,  6.6033e-02, -7.9473e-03],\n",
       "                        [-1.5928e-01,  3.3015e-01, -2.4352e-01]]],\n",
       "              \n",
       "              \n",
       "                      [[[-1.0623e-02,  2.9311e-01,  2.3530e-01],\n",
       "                        [-2.9928e-01, -2.0846e-01,  6.1943e-02],\n",
       "                        [ 1.4430e-01,  1.5550e-01, -1.5349e-01]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 3.0690e-02, -1.0533e-02, -8.6380e-02],\n",
       "                        [ 2.6767e-02, -2.8156e-01, -1.7619e-01],\n",
       "                        [ 1.9954e-02, -2.0697e-01, -2.7964e-01]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 3.7438e-02,  2.5098e-01, -6.6637e-02],\n",
       "                        [ 1.6146e-01,  1.4096e-01,  2.0678e-01],\n",
       "                        [ 2.8163e-01, -6.3377e-02,  2.3029e-01]]],\n",
       "              \n",
       "              \n",
       "                      [[[-1.2998e-01,  1.9431e-01,  2.0165e-01],\n",
       "                        [-1.4312e-01, -1.8045e-01, -1.8526e-01],\n",
       "                        [-2.1156e-01, -3.0760e-01,  2.8868e-01]]],\n",
       "              \n",
       "              \n",
       "                      [[[-2.8942e-02, -2.4394e-01, -2.3244e-01],\n",
       "                        [ 2.9379e-01, -1.2094e-01, -1.0152e-02],\n",
       "                        [ 3.3287e-02,  6.9551e-02,  1.0429e-01]]],\n",
       "              \n",
       "              \n",
       "                      [[[-6.1605e-02, -7.4263e-02,  2.1553e-01],\n",
       "                        [ 9.8374e-02,  2.8302e-01,  1.5019e-01],\n",
       "                        [ 6.1065e-02,  7.5650e-02,  2.5195e-01]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 8.3399e-02, -1.9590e-01,  1.4976e-02],\n",
       "                        [ 1.3355e-01, -1.0595e-01, -1.1594e-01],\n",
       "                        [-1.4060e-01, -5.1581e-02,  4.6727e-02]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 1.7396e-01,  2.8808e-01, -2.9265e-01],\n",
       "                        [ 5.1695e-02, -1.6484e-01, -5.7148e-02],\n",
       "                        [ 9.3220e-02, -2.0827e-01,  9.8124e-02]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 4.7943e-02,  2.5076e-02,  2.3182e-01],\n",
       "                        [ 2.7742e-02, -1.0579e-01,  1.8472e-01],\n",
       "                        [ 2.5902e-02,  3.2510e-01,  1.3047e-01]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 8.1067e-02,  2.0587e-01,  1.9724e-01],\n",
       "                        [ 9.2619e-02,  2.3105e-01, -3.1750e-01],\n",
       "                        [ 6.1236e-02, -2.0042e-01, -2.8949e-01]]],\n",
       "              \n",
       "              \n",
       "                      [[[-2.3415e-01, -2.6375e-01, -4.4841e-02],\n",
       "                        [-8.9979e-02, -4.6747e-02, -1.4465e-01],\n",
       "                        [-2.2517e-01,  2.0488e-01, -3.6362e-01]]]])),\n",
       "             ('module.classifier_vae.encoder.net.0.bias',\n",
       "              tensor([-0.2304, -0.1511, -0.1967, -0.3161, -0.3103,  0.2900, -0.2310, -0.0841,\n",
       "                      -0.0188,  0.2339,  0.0738, -0.0607, -0.2361, -0.0700,  0.0860,  0.2097,\n",
       "                      -0.3249,  0.0539,  0.1028, -0.2298,  0.2165, -0.0910, -0.3083,  0.2853,\n",
       "                      -0.2540, -0.0048, -0.1601,  0.2201,  0.0299, -0.0640,  0.2865,  0.1342,\n",
       "                      -0.1018, -0.3064, -0.2259, -0.2249, -0.2200,  0.0765,  0.0145,  0.2288,\n",
       "                      -0.0195, -0.1810, -0.1965, -0.3078, -0.1990,  0.1150,  0.2212, -0.1246,\n",
       "                      -0.1610,  0.0162,  0.1380, -0.3093,  0.0564,  0.3253, -0.2765, -0.0993,\n",
       "                      -0.0970,  0.3117,  0.1381,  0.2061,  0.0964, -0.1932,  0.2411, -0.2318])),\n",
       "             ('module.classifier_vae.encoder.net.1.weight',\n",
       "              tensor([1.0048, 1.0017, 0.9946, 0.9590, 1.0152, 0.9909, 0.9687, 0.9476, 1.0194,\n",
       "                      0.9940, 1.0529, 1.0396, 1.0068, 0.9897, 1.0339, 0.9905, 1.0134, 0.9807,\n",
       "                      0.9593, 0.9479, 0.9813, 0.9836, 1.1018, 1.1257, 1.0060, 1.0121, 1.1172,\n",
       "                      1.0014, 1.1385, 1.0383, 0.9819, 0.9793, 0.9837, 0.9985, 0.9749, 1.0072,\n",
       "                      1.0201, 1.0010, 0.9440, 1.0421, 0.9970, 0.9892, 0.9411, 1.0244, 1.0146,\n",
       "                      0.9830, 0.9653, 0.9723, 0.9942, 0.9465, 0.9948, 1.0462, 1.0018, 1.0252,\n",
       "                      1.1271, 1.0577, 0.9564, 0.9401, 1.0058, 1.0855, 0.9731, 0.9920, 0.9379,\n",
       "                      1.0845])),\n",
       "             ('module.classifier_vae.encoder.net.1.bias',\n",
       "              tensor([ 0.0023,  0.0780,  0.0877, -0.0461,  0.0510,  0.0225, -0.0161,  0.0002,\n",
       "                       0.0251,  0.0348,  0.0802, -0.0651, -0.0059,  0.0394,  0.0587, -0.0361,\n",
       "                      -0.0010, -0.0447, -0.0320, -0.0639, -0.0107,  0.0780,  0.1029,  0.0978,\n",
       "                       0.0074,  0.0453,  0.0838,  0.0396,  0.1213,  0.0451,  0.0534,  0.0277,\n",
       "                      -0.0568,  0.0057,  0.1040, -0.0522,  0.0914, -0.0749,  0.0383,  0.0986,\n",
       "                       0.0366,  0.0635,  0.0558,  0.0857, -0.0127,  0.0064, -0.0594,  0.0265,\n",
       "                       0.0469, -0.0099,  0.0750,  0.0704, -0.0192,  0.0455,  0.0894,  0.0846,\n",
       "                      -0.0156, -0.0376,  0.0904,  0.1091, -0.0181,  0.0605, -0.0119,  0.0523])),\n",
       "             ('module.classifier_vae.encoder.net.1.running_mean',\n",
       "              tensor([-0.2587, -0.0745, -0.1069, -0.3306, -0.2618,  0.2535, -0.2408, -0.0725,\n",
       "                      -0.0706,  0.2653,  0.1489, -0.0755, -0.2337, -0.1217,  0.1844,  0.1975,\n",
       "                      -0.3201,  0.0435,  0.0850, -0.2486,  0.1713, -0.0277, -0.3422,  0.2331,\n",
       "                      -0.2911, -0.0356, -0.2067,  0.1898, -0.0706, -0.1390,  0.3539,  0.1870,\n",
       "                      -0.1063, -0.3648, -0.1335, -0.2323, -0.1183,  0.0696,  0.1004,  0.1782,\n",
       "                      -0.0734, -0.1208, -0.1523, -0.2059, -0.1919,  0.0668,  0.1895, -0.0777,\n",
       "                      -0.1011,  0.0210,  0.2225, -0.2438,  0.0483,  0.3475, -0.3581,  0.0011,\n",
       "                      -0.1382,  0.2961,  0.2228,  0.1797,  0.0916, -0.1168,  0.2448, -0.3361])),\n",
       "             ('module.classifier_vae.encoder.net.1.running_var',\n",
       "              tensor([0.0099, 0.0320, 0.0370, 0.0067, 0.0160, 0.0091, 0.0062, 0.0144, 0.0326,\n",
       "                      0.0162, 0.0320, 0.0137, 0.0029, 0.0307, 0.0404, 0.0052, 0.0033, 0.0145,\n",
       "                      0.0147, 0.0218, 0.0275, 0.0186, 0.0107, 0.0156, 0.0098, 0.0098, 0.0225,\n",
       "                      0.0056, 0.0394, 0.0308, 0.0247, 0.0147, 0.0048, 0.0273, 0.0411, 0.0075,\n",
       "                      0.0470, 0.0064, 0.0376, 0.0263, 0.0344, 0.0193, 0.0243, 0.0438, 0.0062,\n",
       "                      0.0232, 0.0199, 0.0204, 0.0221, 0.0252, 0.0431, 0.0237, 0.0071, 0.0106,\n",
       "                      0.0395, 0.0428, 0.0231, 0.0133, 0.0371, 0.0059, 0.0094, 0.0271, 0.0222,\n",
       "                      0.0431])),\n",
       "             ('module.classifier_vae.encoder.net.1.num_batches_tracked',\n",
       "              tensor(0)),\n",
       "             ('module.classifier_vae.encoder.net.3.weight',\n",
       "              tensor([[[[ 1.1387e-03,  1.6479e-02, -3.6189e-03],\n",
       "                        [-2.2122e-02, -3.2335e-02,  2.9781e-02],\n",
       "                        [-2.6217e-03, -2.5331e-02,  2.4332e-02]],\n",
       "              \n",
       "                       [[-3.3373e-02, -4.2184e-02,  1.5867e-04],\n",
       "                        [-3.5423e-02,  2.1733e-02, -5.5788e-02],\n",
       "                        [ 2.2485e-02, -9.7577e-03, -2.4920e-03]],\n",
       "              \n",
       "                       [[-5.6337e-02,  5.7255e-03, -1.2344e-02],\n",
       "                        [-9.6217e-03,  3.1144e-02,  2.2430e-02],\n",
       "                        [ 2.3948e-02, -2.0210e-02,  2.6139e-02]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-4.7983e-03, -2.5284e-02,  1.5283e-02],\n",
       "                        [ 1.8729e-02, -7.4242e-03, -1.1250e-02],\n",
       "                        [ 2.2763e-02,  3.5172e-02,  6.2224e-03]],\n",
       "              \n",
       "                       [[ 4.8888e-02, -3.4323e-02,  2.8149e-02],\n",
       "                        [ 9.8016e-03, -2.3978e-02,  2.1127e-02],\n",
       "                        [ 1.0199e-03, -2.5833e-02, -2.5976e-02]],\n",
       "              \n",
       "                       [[ 3.9324e-02,  1.4961e-02, -8.7740e-03],\n",
       "                        [ 2.6561e-02, -1.7995e-02,  4.7943e-02],\n",
       "                        [-3.1661e-02, -9.7803e-03,  3.4613e-02]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 4.2098e-03, -9.9149e-03, -4.3156e-02],\n",
       "                        [-3.8261e-02, -6.9935e-02, -1.2722e-02],\n",
       "                        [-2.3483e-02, -5.2441e-02, -1.4843e-02]],\n",
       "              \n",
       "                       [[ 4.5861e-02,  5.8330e-02,  5.1467e-02],\n",
       "                        [ 2.3664e-02, -5.5200e-03,  2.3501e-02],\n",
       "                        [ 2.4152e-03, -2.3056e-03,  2.7009e-02]],\n",
       "              \n",
       "                       [[ 2.8950e-02,  7.1089e-02,  9.8836e-05],\n",
       "                        [ 1.3554e-02,  2.3795e-02,  3.5339e-02],\n",
       "                        [ 4.2583e-02, -1.2785e-02, -2.7400e-02]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 4.4668e-03,  1.4874e-02,  9.3848e-03],\n",
       "                        [ 9.8857e-03,  5.3233e-02,  1.0095e-02],\n",
       "                        [-8.4942e-03, -8.0989e-03,  2.1418e-02]],\n",
       "              \n",
       "                       [[ 3.7926e-02, -3.2397e-02,  2.6712e-02],\n",
       "                        [-1.7808e-02,  3.5739e-02,  1.7607e-02],\n",
       "                        [ 3.1598e-02, -2.1810e-02, -1.2994e-02]],\n",
       "              \n",
       "                       [[-3.6542e-02, -4.0054e-02, -5.7671e-02],\n",
       "                        [-1.5593e-02, -6.5580e-03, -2.2608e-02],\n",
       "                        [ 1.7267e-02, -2.5499e-02, -1.0525e-01]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 1.4723e-02, -2.1767e-02, -6.4091e-03],\n",
       "                        [ 7.6496e-03,  3.3278e-02,  2.2615e-03],\n",
       "                        [ 1.0220e-02, -2.5964e-02, -3.2953e-02]],\n",
       "              \n",
       "                       [[-1.3561e-02, -4.5707e-02,  9.3122e-03],\n",
       "                        [-4.9328e-02, -1.5272e-02, -4.2454e-02],\n",
       "                        [-3.9950e-02,  7.7892e-03,  2.5149e-02]],\n",
       "              \n",
       "                       [[-1.2100e-02, -1.5695e-02, -2.8139e-03],\n",
       "                        [ 6.4475e-04,  3.6845e-02, -2.9481e-02],\n",
       "                        [-4.8695e-02, -1.1393e-02, -7.1761e-03]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-1.5073e-02, -2.8230e-02, -2.5943e-03],\n",
       "                        [ 1.6058e-02, -2.6763e-02, -3.2328e-03],\n",
       "                        [-1.1286e-02, -1.7258e-03, -2.4596e-02]],\n",
       "              \n",
       "                       [[ 2.3434e-02,  3.3484e-02,  9.8007e-03],\n",
       "                        [-2.2704e-02, -4.6120e-02,  3.7974e-02],\n",
       "                        [-2.4643e-02, -1.3852e-02, -4.4521e-02]],\n",
       "              \n",
       "                       [[ 6.6956e-03, -2.0224e-02,  3.2281e-02],\n",
       "                        [-1.5731e-02, -3.4463e-02, -4.2345e-03],\n",
       "                        [ 4.0719e-02,  1.3132e-02,  1.9730e-02]]],\n",
       "              \n",
       "              \n",
       "                      ...,\n",
       "              \n",
       "              \n",
       "                      [[[ 3.6736e-02, -1.9921e-02, -4.7634e-02],\n",
       "                        [-2.7174e-02, -3.3008e-02, -2.6126e-02],\n",
       "                        [ 1.5032e-02,  1.2620e-02,  1.6214e-02]],\n",
       "              \n",
       "                       [[-3.2567e-02,  1.1113e-02, -5.7204e-03],\n",
       "                        [-3.9173e-02, -4.1071e-03, -4.7250e-03],\n",
       "                        [ 3.0851e-02, -2.5938e-02, -9.2718e-03]],\n",
       "              \n",
       "                       [[-3.3286e-02,  3.8716e-02, -2.2303e-02],\n",
       "                        [ 2.0504e-04,  1.8109e-02,  2.9227e-03],\n",
       "                        [-3.4350e-02,  2.8002e-02,  1.4930e-02]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 3.0250e-02, -3.1457e-02, -3.0439e-02],\n",
       "                        [-2.9220e-02, -4.7476e-02,  1.4464e-03],\n",
       "                        [ 9.1739e-03,  3.1423e-02, -2.5014e-02]],\n",
       "              \n",
       "                       [[ 7.6391e-03, -5.0317e-02,  1.0082e-02],\n",
       "                        [-4.1341e-02, -3.9975e-02, -3.2438e-02],\n",
       "                        [-2.7929e-02, -3.1570e-02, -1.9958e-02]],\n",
       "              \n",
       "                       [[-2.5766e-02,  5.6921e-02,  4.9147e-02],\n",
       "                        [ 1.2506e-03, -2.3047e-02,  1.7719e-02],\n",
       "                        [ 5.6105e-03, -4.9693e-04, -2.6388e-02]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 4.0628e-02,  1.6084e-02,  5.0257e-02],\n",
       "                        [-3.3043e-04, -1.4908e-02, -4.0250e-02],\n",
       "                        [ 4.4559e-02,  2.4435e-02,  2.5379e-02]],\n",
       "              \n",
       "                       [[ 1.7805e-02,  3.4637e-03,  7.3567e-03],\n",
       "                        [ 1.0918e-02,  4.0402e-03, -4.3443e-02],\n",
       "                        [ 4.5701e-02,  2.7508e-02, -4.9643e-02]],\n",
       "              \n",
       "                       [[-3.1242e-02, -4.5171e-03, -2.6179e-02],\n",
       "                        [ 7.3806e-03,  3.6359e-03,  8.2197e-03],\n",
       "                        [-3.6202e-02,  3.2185e-02, -2.6679e-03]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-3.3825e-02, -2.5899e-02, -3.8523e-02],\n",
       "                        [ 1.3845e-02, -5.1715e-02,  2.6085e-02],\n",
       "                        [-8.2111e-03, -1.1099e-02, -5.2212e-03]],\n",
       "              \n",
       "                       [[-1.6957e-02, -5.7082e-03, -3.9188e-02],\n",
       "                        [ 2.0621e-02,  6.0739e-03,  2.7757e-02],\n",
       "                        [ 2.7207e-02, -4.8305e-02,  1.2206e-02]],\n",
       "              \n",
       "                       [[ 3.8818e-02,  4.5098e-02,  5.6644e-02],\n",
       "                        [-4.1858e-02, -4.0171e-02,  3.7274e-02],\n",
       "                        [-1.3621e-02,  2.7853e-02, -2.6585e-03]]],\n",
       "              \n",
       "              \n",
       "                      [[[-4.1529e-02,  1.3936e-02,  2.4598e-02],\n",
       "                        [-4.0483e-02, -3.7831e-02, -4.2711e-02],\n",
       "                        [ 4.0304e-02,  5.3528e-03, -4.7631e-02]],\n",
       "              \n",
       "                       [[-2.8625e-04,  1.9914e-03, -1.1417e-02],\n",
       "                        [ 1.1451e-02,  1.2588e-02,  2.5518e-03],\n",
       "                        [-2.8937e-02,  5.9725e-02,  8.3050e-03]],\n",
       "              \n",
       "                       [[-7.9296e-03,  4.2232e-03,  3.7042e-02],\n",
       "                        [ 5.0230e-02,  1.3567e-02, -3.7778e-02],\n",
       "                        [-2.7182e-02, -3.5711e-03, -2.7755e-02]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 8.0850e-04,  6.4190e-02,  3.4086e-02],\n",
       "                        [ 2.4672e-02,  6.6909e-02,  3.0580e-02],\n",
       "                        [ 2.9833e-02, -6.7696e-03, -3.5156e-03]],\n",
       "              \n",
       "                       [[ 1.3297e-02, -1.2163e-02, -1.9866e-02],\n",
       "                        [ 4.7771e-02,  3.4102e-02, -4.0796e-02],\n",
       "                        [-3.3221e-02, -3.0094e-03,  1.7996e-02]],\n",
       "              \n",
       "                       [[-1.3678e-03, -2.1380e-03, -3.1550e-03],\n",
       "                        [-6.0184e-02, -2.8583e-02,  1.5801e-02],\n",
       "                        [-3.1291e-03, -5.9871e-02, -2.6427e-02]]]])),\n",
       "             ('module.classifier_vae.encoder.net.3.bias',\n",
       "              tensor([ 2.7176e-02, -3.4194e-02,  1.0693e-02,  3.6821e-02, -3.4906e-02,\n",
       "                       3.5735e-02, -1.9253e-02,  1.8647e-02,  3.0278e-02, -1.2479e-02,\n",
       "                       3.9684e-02, -4.2972e-03,  2.2713e-02, -3.3628e-02, -2.6231e-02,\n",
       "                       3.7600e-02, -7.8995e-03, -2.1948e-02,  1.9120e-02,  3.7623e-02,\n",
       "                       3.2550e-03,  3.9413e-02, -2.4834e-02,  2.5425e-02, -4.1857e-02,\n",
       "                       4.1749e-02, -2.6608e-02,  3.8691e-02,  2.8391e-02,  2.6438e-02,\n",
       "                      -6.7514e-03, -4.0535e-02,  3.1986e-02,  1.0599e-03, -3.2878e-02,\n",
       "                       1.5202e-02,  2.5430e-04,  1.0743e-02,  1.2151e-02, -8.2486e-03,\n",
       "                      -5.1155e-03,  3.0481e-02,  4.2143e-03, -2.3304e-02, -3.1340e-02,\n",
       "                       3.3700e-02, -2.4137e-02,  2.2887e-02, -2.0957e-02,  3.1397e-02,\n",
       "                      -2.8205e-05, -4.0341e-02, -3.3997e-02, -3.9260e-02, -3.1368e-02,\n",
       "                      -2.4188e-02,  3.8964e-03, -7.3650e-03,  4.2740e-02,  9.0570e-03,\n",
       "                      -3.4935e-03,  1.3137e-02,  1.1870e-02,  3.5754e-02])),\n",
       "             ('module.classifier_vae.encoder.net.4.weight',\n",
       "              tensor([1.0006, 0.9609, 1.0287, 1.0413, 0.9825, 0.9946, 0.9819, 1.0194, 0.9311,\n",
       "                      1.1102, 0.9702, 0.9921, 1.0827, 1.0455, 1.0038, 0.9658, 1.0282, 0.9636,\n",
       "                      0.9726, 0.9795, 1.0099, 1.0680, 1.1096, 0.9840, 0.9618, 0.9837, 1.0428,\n",
       "                      0.9869, 1.0452, 1.0155, 1.0385, 0.9616, 1.0045, 1.0035, 1.0164, 0.9833,\n",
       "                      0.9881, 1.0605, 0.9944, 0.9598, 0.9727, 1.0027, 1.0302, 1.0357, 1.0448,\n",
       "                      1.0410, 1.0467, 1.0561, 0.9773, 1.0024, 0.9736, 0.9600, 0.9651, 1.0833,\n",
       "                      1.0413, 0.9949, 0.9660, 0.9787, 0.9749, 0.9733, 1.0567, 1.0269, 1.0282,\n",
       "                      0.9629])),\n",
       "             ('module.classifier_vae.encoder.net.4.bias',\n",
       "              tensor([ 0.0392, -0.0196,  0.0581,  0.0990, -0.0214, -0.0113, -0.0221,  0.0672,\n",
       "                      -0.0431,  0.1216, -0.0154,  0.0074,  0.1037,  0.1383,  0.0186, -0.0520,\n",
       "                       0.0592, -0.0102, -0.0303, -0.0458,  0.0410,  0.1612,  0.1207,  0.0117,\n",
       "                      -0.0018,  0.0093,  0.0734, -0.0399,  0.0926,  0.0885,  0.1007, -0.0373,\n",
       "                      -0.0246, -0.0071,  0.0065, -0.0392, -0.0121,  0.0948, -0.0005,  0.0136,\n",
       "                      -0.0402,  0.0193,  0.0508,  0.0792,  0.0805,  0.0813,  0.0960,  0.1190,\n",
       "                      -0.0292,  0.0407,  0.0213, -0.0074, -0.0250,  0.0977,  0.0718, -0.0444,\n",
       "                      -0.0256, -0.0340, -0.0252,  0.0104,  0.0887,  0.0213,  0.0914, -0.0463])),\n",
       "             ('module.classifier_vae.encoder.net.4.running_mean',\n",
       "              tensor([-0.6898,  0.6029, -0.6702,  0.3382,  0.9353,  0.7999,  0.5289, -0.5456,\n",
       "                       0.6132, -0.4774,  0.7931,  0.6814, -0.2792, -0.7116,  0.7189,  0.5106,\n",
       "                      -0.4228,  0.5008,  0.8628,  0.5523,  1.0834, -0.0025,  0.0311,  0.8042,\n",
       "                       0.9027, -0.5709,  0.4520,  0.2305, -0.7001, -0.4819, -0.5909,  0.4315,\n",
       "                       0.9617,  0.1702, -1.2881,  0.7328,  0.7501, -0.5939,  0.7540,  0.7945,\n",
       "                       0.4293, -0.6242, -0.6774,  0.3427, -0.4224,  0.0042, -0.0837, -0.0910,\n",
       "                       0.2332, -0.4525, -0.6358,  0.4944,  0.4167,  0.3605, -0.3519,  0.7223,\n",
       "                       0.6379,  0.4926,  0.8734,  0.6829, -0.5835,  0.3814,  0.6422,  0.6364])),\n",
       "             ('module.classifier_vae.encoder.net.4.running_var',\n",
       "              tensor([3.0602, 6.8254, 1.2284, 0.7215, 8.1617, 6.1700, 8.5341, 1.2356, 5.3547,\n",
       "                      1.3614, 3.8222, 1.9446, 1.3803, 1.9354, 7.5699, 6.3512, 1.1575, 6.6888,\n",
       "                      6.8974, 5.4900, 4.0771, 2.8160, 1.1046, 7.6775, 5.9737, 1.0720, 1.5257,\n",
       "                      3.6755, 4.2878, 1.6355, 1.9362, 4.5444, 7.0293, 1.9675, 6.7037, 6.0282,\n",
       "                      7.9641, 1.8310, 8.4109, 3.8308, 6.0050, 2.1395, 3.1340, 0.9533, 1.6189,\n",
       "                      0.9622, 1.6177, 1.4163, 5.3849, 1.1348, 1.3194, 3.3987, 5.6034, 1.5108,\n",
       "                      1.3040, 8.8236, 6.2842, 4.6068, 5.1308, 1.8856, 3.0303, 1.9714, 1.0423,\n",
       "                      5.1387])),\n",
       "             ('module.classifier_vae.encoder.net.4.num_batches_tracked',\n",
       "              tensor(0)),\n",
       "             ('module.classifier_vae.encoder.net.6.weight',\n",
       "              tensor([[[[-0.0331,  0.0250,  0.0053],\n",
       "                        [ 0.0419, -0.0072, -0.0447],\n",
       "                        [ 0.0268,  0.0007, -0.0387]],\n",
       "              \n",
       "                       [[ 0.0564, -0.0101, -0.0342],\n",
       "                        [ 0.0531,  0.0405,  0.0057],\n",
       "                        [-0.0252, -0.0181, -0.0242]],\n",
       "              \n",
       "                       [[ 0.0311, -0.0069,  0.0213],\n",
       "                        [-0.0204, -0.0082, -0.0431],\n",
       "                        [ 0.0021,  0.0096,  0.0126]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-0.0227,  0.0271,  0.0038],\n",
       "                        [ 0.0117,  0.0091, -0.0153],\n",
       "                        [ 0.0490, -0.0359, -0.0438]],\n",
       "              \n",
       "                       [[ 0.0327,  0.0347, -0.0570],\n",
       "                        [ 0.0129, -0.0192,  0.0390],\n",
       "                        [ 0.0531, -0.0243, -0.0019]],\n",
       "              \n",
       "                       [[ 0.0068,  0.0556,  0.0177],\n",
       "                        [-0.0088,  0.0091,  0.0337],\n",
       "                        [ 0.0699,  0.0518,  0.0261]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.0391, -0.0090, -0.0271],\n",
       "                        [-0.0142,  0.0122,  0.0108],\n",
       "                        [-0.0400,  0.0057, -0.0201]],\n",
       "              \n",
       "                       [[-0.0177, -0.0220, -0.0348],\n",
       "                        [ 0.0320,  0.0573, -0.0393],\n",
       "                        [-0.0036,  0.0413,  0.0127]],\n",
       "              \n",
       "                       [[ 0.0300,  0.0235, -0.0381],\n",
       "                        [-0.0471,  0.0513, -0.0008],\n",
       "                        [ 0.0202,  0.0090,  0.0157]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-0.0254, -0.0414,  0.0088],\n",
       "                        [ 0.0077,  0.0002, -0.0482],\n",
       "                        [-0.0171,  0.0257, -0.0021]],\n",
       "              \n",
       "                       [[-0.0303,  0.0225, -0.0275],\n",
       "                        [ 0.0226, -0.0349,  0.0179],\n",
       "                        [ 0.0233, -0.0018,  0.0242]],\n",
       "              \n",
       "                       [[-0.0320, -0.0153, -0.0112],\n",
       "                        [ 0.0437,  0.0326, -0.0061],\n",
       "                        [ 0.0256,  0.0046,  0.0234]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.0366, -0.0387, -0.0525],\n",
       "                        [ 0.0143,  0.0140, -0.0069],\n",
       "                        [ 0.0117, -0.0213, -0.0117]],\n",
       "              \n",
       "                       [[-0.0080, -0.0146, -0.0244],\n",
       "                        [ 0.0078,  0.0202, -0.0071],\n",
       "                        [-0.0155,  0.0043, -0.0222]],\n",
       "              \n",
       "                       [[-0.0334, -0.0013,  0.0226],\n",
       "                        [-0.0146, -0.0432, -0.0236],\n",
       "                        [ 0.0011, -0.0105, -0.0456]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 0.0447, -0.0150, -0.0165],\n",
       "                        [-0.0153, -0.0303,  0.0163],\n",
       "                        [ 0.0287,  0.0185, -0.0220]],\n",
       "              \n",
       "                       [[-0.0245,  0.0251,  0.0036],\n",
       "                        [ 0.0426,  0.0360, -0.0250],\n",
       "                        [-0.0089, -0.0040,  0.0072]],\n",
       "              \n",
       "                       [[-0.0123,  0.0060, -0.0317],\n",
       "                        [-0.0038, -0.0157, -0.0559],\n",
       "                        [ 0.0074, -0.0469, -0.0574]]],\n",
       "              \n",
       "              \n",
       "                      ...,\n",
       "              \n",
       "              \n",
       "                      [[[ 0.0180,  0.0074,  0.0631],\n",
       "                        [ 0.0137, -0.0147,  0.0558],\n",
       "                        [ 0.0390,  0.0045, -0.0223]],\n",
       "              \n",
       "                       [[-0.0554,  0.0214, -0.0365],\n",
       "                        [-0.0153,  0.0168,  0.0376],\n",
       "                        [ 0.0049,  0.0019,  0.0165]],\n",
       "              \n",
       "                       [[ 0.0180,  0.0240,  0.0720],\n",
       "                        [-0.0219, -0.0055, -0.0077],\n",
       "                        [ 0.0562,  0.0181,  0.0098]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 0.0046, -0.0479,  0.0520],\n",
       "                        [ 0.0433,  0.0058,  0.0532],\n",
       "                        [ 0.0066, -0.0051,  0.0104]],\n",
       "              \n",
       "                       [[ 0.0360, -0.0426,  0.0455],\n",
       "                        [-0.0168, -0.0248,  0.0085],\n",
       "                        [-0.0142, -0.0403, -0.0128]],\n",
       "              \n",
       "                       [[-0.0110, -0.0282, -0.0120],\n",
       "                        [ 0.0220,  0.0360,  0.0301],\n",
       "                        [-0.0033,  0.0223,  0.0073]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.0236,  0.0138, -0.0210],\n",
       "                        [ 0.0175,  0.0242, -0.0197],\n",
       "                        [ 0.0293, -0.0207, -0.0115]],\n",
       "              \n",
       "                       [[-0.0410, -0.0500,  0.0211],\n",
       "                        [-0.0504, -0.0188,  0.0424],\n",
       "                        [ 0.0268, -0.0084,  0.0119]],\n",
       "              \n",
       "                       [[ 0.0182,  0.0147, -0.0228],\n",
       "                        [-0.0308,  0.0111,  0.0091],\n",
       "                        [ 0.0057,  0.0174, -0.0140]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 0.0137,  0.0270,  0.0554],\n",
       "                        [-0.0031, -0.0480,  0.0058],\n",
       "                        [-0.0217,  0.0279,  0.0464]],\n",
       "              \n",
       "                       [[ 0.0136,  0.0657,  0.0122],\n",
       "                        [ 0.0041, -0.0313, -0.0487],\n",
       "                        [ 0.0035,  0.0331,  0.0256]],\n",
       "              \n",
       "                       [[ 0.0230, -0.0465,  0.0034],\n",
       "                        [-0.0041,  0.0208,  0.0236],\n",
       "                        [-0.0080,  0.0281,  0.0493]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.0350, -0.0301, -0.0231],\n",
       "                        [-0.0265, -0.0080, -0.0493],\n",
       "                        [-0.0175, -0.0605,  0.0170]],\n",
       "              \n",
       "                       [[-0.0290, -0.0165, -0.0538],\n",
       "                        [ 0.0066, -0.0109, -0.0391],\n",
       "                        [ 0.0122, -0.0086, -0.0242]],\n",
       "              \n",
       "                       [[ 0.0568,  0.0136, -0.0063],\n",
       "                        [ 0.0269,  0.0243,  0.0110],\n",
       "                        [ 0.0356, -0.0121,  0.0115]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 0.0326, -0.0157,  0.0079],\n",
       "                        [-0.0163,  0.0187, -0.0267],\n",
       "                        [-0.0248,  0.0044,  0.0301]],\n",
       "              \n",
       "                       [[ 0.0055,  0.0036,  0.0457],\n",
       "                        [ 0.0323,  0.0261,  0.0004],\n",
       "                        [-0.0580, -0.0143, -0.0127]],\n",
       "              \n",
       "                       [[ 0.0345, -0.0237, -0.0098],\n",
       "                        [-0.0494, -0.0367,  0.0102],\n",
       "                        [ 0.0181,  0.0169, -0.0523]]]])),\n",
       "             ('module.classifier_vae.encoder.net.6.bias',\n",
       "              tensor([-0.0060, -0.0104, -0.0091, -0.0305, -0.0429, -0.0385,  0.0181, -0.0040,\n",
       "                      -0.0205, -0.0199, -0.0304, -0.0104, -0.0045, -0.0354, -0.0129, -0.0352,\n",
       "                      -0.0021, -0.0221, -0.0327,  0.0203, -0.0187,  0.0163, -0.0180,  0.0345,\n",
       "                       0.0006,  0.0128, -0.0295,  0.0309, -0.0167, -0.0393, -0.0111,  0.0362,\n",
       "                      -0.0216,  0.0440, -0.0109,  0.0193,  0.0280, -0.0352, -0.0022, -0.0323,\n",
       "                      -0.0166, -0.0348,  0.0255,  0.0466,  0.0332, -0.0257, -0.0221,  0.0214,\n",
       "                       0.0190,  0.0082, -0.0047, -0.0368, -0.0243,  0.0085,  0.0282,  0.0170,\n",
       "                       0.0024,  0.0014, -0.0226, -0.0051,  0.0348,  0.0044,  0.0420, -0.0337])),\n",
       "             ('module.classifier_vae.encoder.net.7.weight',\n",
       "              tensor([0.9493, 1.0413, 1.0382, 0.9948, 1.0270, 0.9729, 0.9802, 0.9585, 1.0639,\n",
       "                      1.0057, 1.0367, 1.0462, 0.9917, 1.0199, 0.9901, 1.0156, 1.0090, 0.9559,\n",
       "                      1.0004, 0.9810, 0.9306, 1.0114, 1.0104, 1.0122, 1.0052, 1.0212, 1.0644,\n",
       "                      0.9238, 0.9765, 1.0380, 0.9602, 0.9546, 1.0243, 1.0552, 1.0052, 1.0575,\n",
       "                      1.0045, 1.0070, 1.0305, 0.9991, 0.9839, 1.0562, 1.0143, 1.0236, 0.9878,\n",
       "                      0.9833, 0.9444, 1.0145, 0.9286, 1.0818, 0.9833, 0.9296, 1.0274, 1.0001,\n",
       "                      0.9648, 1.0411, 0.9753, 1.0095, 1.0124, 0.9987, 1.0242, 0.9689, 0.9956,\n",
       "                      1.0361])),\n",
       "             ('module.classifier_vae.encoder.net.7.bias',\n",
       "              tensor([-0.0530,  0.0065,  0.0893, -0.0241,  0.0807, -0.0595, -0.0665, -0.0597,\n",
       "                       0.1205, -0.0037,  0.0181,  0.0497,  0.0015,  0.0484, -0.0297,  0.0079,\n",
       "                       0.0155, -0.0294,  0.0080, -0.0684, -0.1130,  0.0515, -0.0659,  0.0158,\n",
       "                      -0.0068,  0.0301,  0.0275, -0.0525, -0.0456,  0.0011, -0.0642, -0.1227,\n",
       "                       0.0458, -0.0110,  0.0511,  0.0701, -0.0024,  0.0918,  0.0634,  0.0280,\n",
       "                      -0.0422,  0.1103,  0.0291,  0.0428, -0.0661, -0.0385, -0.0491,  0.0597,\n",
       "                      -0.0510,  0.0020, -0.0717, -0.0534,  0.0252, -0.0063, -0.0406,  0.0736,\n",
       "                      -0.1237,  0.0274,  0.0139, -0.0684,  0.0172,  0.0140,  0.0038,  0.0646])),\n",
       "             ('module.classifier_vae.encoder.net.7.running_mean',\n",
       "              tensor([ 0.9705, -1.5900, -0.8367,  2.5081, -0.9022,  1.9052, -0.1800,  1.4808,\n",
       "                       1.1357, -0.7783,  1.2904, -1.8877, -0.4062, -0.9394,  0.1088,  0.2007,\n",
       "                      -1.2625,  2.0526, -0.5382,  1.3822,  0.9372, -1.1553,  0.5995,  0.3709,\n",
       "                      -2.1496,  2.4457, -0.8096,  0.3950,  0.2335, -0.6138,  1.7173,  0.6823,\n",
       "                      -2.3019, -0.5749, -0.1457,  1.9504, -0.6225, -0.9703, -1.0052,  1.4536,\n",
       "                       0.8030, -1.0378, -1.0802,  0.7742,  1.2072, -0.0581,  1.7743, -0.2180,\n",
       "                       1.8068, -1.0121,  0.8876,  0.6093, -1.0940, -0.7744, -0.3773, -0.0235,\n",
       "                       1.2385, -1.1612, -0.4302,  0.8934, -1.7126,  1.8456,  0.9305, -0.6130])),\n",
       "             ('module.classifier_vae.encoder.net.7.running_var',\n",
       "              tensor([ 3.3296, 10.3625,  1.2783,  2.8345,  2.2426,  4.6293,  1.8830,  3.2106,\n",
       "                       7.7467,  6.6320,  2.7181,  4.4920,  2.5029,  1.2433,  4.4969,  2.6888,\n",
       "                       2.0540,  3.8374,  1.7997,  4.2393,  2.8703,  1.2860,  4.2818,  1.4695,\n",
       "                       1.8121,  2.0408,  1.0615,  2.2724,  2.6286,  2.2856,  4.2953,  3.6896,\n",
       "                      15.3348,  3.4679,  1.8643,  3.4834,  3.5614,  2.6287, 14.3781,  2.2996,\n",
       "                       3.1526,  8.8929,  2.0009,  7.4524,  4.0197,  2.7660,  2.8412,  2.7682,\n",
       "                       2.9527,  2.6491,  2.3794,  1.5224,  2.8802,  1.4099,  1.7686,  1.8353,\n",
       "                       3.4672,  2.8467,  1.6844,  2.7156, 13.0648,  2.8738,  2.0431,  1.5973])),\n",
       "             ('module.classifier_vae.encoder.net.7.num_batches_tracked',\n",
       "              tensor(0)),\n",
       "             ('module.classifier_vae.encoder.net.9.weight',\n",
       "              tensor([[[[-0.0201,  0.0016,  0.0341],\n",
       "                        [-0.0255, -0.0151, -0.0298],\n",
       "                        [ 0.0072, -0.0400,  0.0280]],\n",
       "              \n",
       "                       [[-0.0364, -0.0036,  0.0264],\n",
       "                        [ 0.0100,  0.0292,  0.0409],\n",
       "                        [ 0.0247, -0.0252, -0.0292]],\n",
       "              \n",
       "                       [[-0.0688, -0.0234,  0.0303],\n",
       "                        [-0.0409, -0.0338, -0.0351],\n",
       "                        [-0.0138, -0.0339,  0.0434]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-0.0362, -0.0057,  0.0059],\n",
       "                        [-0.0309,  0.0030, -0.0141],\n",
       "                        [ 0.0343, -0.0149,  0.0174]],\n",
       "              \n",
       "                       [[-0.0487,  0.0093,  0.0279],\n",
       "                        [-0.0427,  0.0143,  0.0365],\n",
       "                        [ 0.0186, -0.0598,  0.0183]],\n",
       "              \n",
       "                       [[-0.0313,  0.0449,  0.0006],\n",
       "                        [ 0.0228, -0.0108, -0.0416],\n",
       "                        [ 0.0077, -0.0610, -0.0049]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.0010, -0.0187,  0.0127],\n",
       "                        [-0.0077, -0.0290,  0.0277],\n",
       "                        [ 0.0042,  0.0362, -0.0265]],\n",
       "              \n",
       "                       [[ 0.0207, -0.0024,  0.0358],\n",
       "                        [ 0.0181,  0.0102, -0.0048],\n",
       "                        [-0.0107, -0.0539, -0.0312]],\n",
       "              \n",
       "                       [[ 0.0087, -0.0213, -0.0665],\n",
       "                        [-0.0083,  0.0082, -0.0321],\n",
       "                        [-0.0494,  0.0334,  0.0266]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 0.0004, -0.0180,  0.0180],\n",
       "                        [-0.0266,  0.0221,  0.0154],\n",
       "                        [-0.0120, -0.0166,  0.0209]],\n",
       "              \n",
       "                       [[ 0.0224, -0.0514, -0.0157],\n",
       "                        [-0.0221, -0.0355, -0.0039],\n",
       "                        [ 0.0284, -0.0400,  0.0113]],\n",
       "              \n",
       "                       [[-0.0420, -0.0282, -0.0669],\n",
       "                        [-0.0558, -0.0084, -0.0518],\n",
       "                        [-0.0307,  0.0165, -0.0265]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.0151,  0.0178,  0.0364],\n",
       "                        [-0.0126, -0.0250,  0.0144],\n",
       "                        [ 0.0173, -0.0137, -0.0303]],\n",
       "              \n",
       "                       [[ 0.0138, -0.0225, -0.0419],\n",
       "                        [-0.0022,  0.0071,  0.0052],\n",
       "                        [-0.0416,  0.0083, -0.0199]],\n",
       "              \n",
       "                       [[-0.0076, -0.0239,  0.0101],\n",
       "                        [-0.0576,  0.0001, -0.0451],\n",
       "                        [-0.0066, -0.0387, -0.0074]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-0.0405,  0.0094,  0.0231],\n",
       "                        [-0.0269,  0.0222, -0.0390],\n",
       "                        [ 0.0176,  0.0098, -0.0038]],\n",
       "              \n",
       "                       [[ 0.0014, -0.0109, -0.0215],\n",
       "                        [-0.0374, -0.0328,  0.0119],\n",
       "                        [ 0.0119, -0.0400,  0.0573]],\n",
       "              \n",
       "                       [[ 0.0406,  0.0120,  0.0102],\n",
       "                        [-0.0250, -0.0490, -0.0326],\n",
       "                        [-0.0338,  0.0089, -0.0408]]],\n",
       "              \n",
       "              \n",
       "                      ...,\n",
       "              \n",
       "              \n",
       "                      [[[-0.0087,  0.0104,  0.0138],\n",
       "                        [-0.0138, -0.0421, -0.0218],\n",
       "                        [-0.0295,  0.0305,  0.0442]],\n",
       "              \n",
       "                       [[ 0.0187,  0.0013,  0.0240],\n",
       "                        [-0.0043, -0.0373,  0.0148],\n",
       "                        [ 0.0634,  0.0095,  0.0317]],\n",
       "              \n",
       "                       [[-0.0093, -0.0284, -0.0533],\n",
       "                        [ 0.0049,  0.0143, -0.0077],\n",
       "                        [-0.0537, -0.0579, -0.0415]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-0.0522, -0.0007,  0.0156],\n",
       "                        [ 0.0231, -0.0411,  0.0252],\n",
       "                        [-0.0479,  0.0507,  0.0067]],\n",
       "              \n",
       "                       [[-0.0620, -0.0296,  0.0303],\n",
       "                        [-0.0037, -0.0212, -0.0338],\n",
       "                        [ 0.0420, -0.0033,  0.0380]],\n",
       "              \n",
       "                       [[-0.0083, -0.0089, -0.0202],\n",
       "                        [-0.0058,  0.0072, -0.0039],\n",
       "                        [-0.0050, -0.0107,  0.0112]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.0383, -0.0140,  0.0130],\n",
       "                        [-0.0431,  0.0018, -0.0041],\n",
       "                        [ 0.0353, -0.0137,  0.0245]],\n",
       "              \n",
       "                       [[-0.0156, -0.0156,  0.0448],\n",
       "                        [-0.0048,  0.0484, -0.0059],\n",
       "                        [-0.0323, -0.0381, -0.0464]],\n",
       "              \n",
       "                       [[ 0.0161, -0.0177,  0.0103],\n",
       "                        [-0.0356,  0.0318, -0.0170],\n",
       "                        [-0.0159, -0.0815, -0.0207]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-0.0441, -0.0530, -0.0395],\n",
       "                        [ 0.0620,  0.0074, -0.0396],\n",
       "                        [-0.0237, -0.0237,  0.0226]],\n",
       "              \n",
       "                       [[-0.0136,  0.0159, -0.0289],\n",
       "                        [-0.0504, -0.0102,  0.0021],\n",
       "                        [-0.0190, -0.0453, -0.0280]],\n",
       "              \n",
       "                       [[-0.0330, -0.0187,  0.0071],\n",
       "                        [-0.0100,  0.0029, -0.0017],\n",
       "                        [-0.0425, -0.0224, -0.0448]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.0502, -0.0230, -0.0200],\n",
       "                        [-0.0136, -0.0135, -0.0385],\n",
       "                        [-0.0280, -0.0259, -0.0087]],\n",
       "              \n",
       "                       [[-0.0363, -0.0083, -0.0166],\n",
       "                        [ 0.0146, -0.0019,  0.0036],\n",
       "                        [-0.0701,  0.0074,  0.0306]],\n",
       "              \n",
       "                       [[ 0.0120,  0.0121, -0.0297],\n",
       "                        [ 0.0589, -0.0190,  0.0274],\n",
       "                        [ 0.0017, -0.0009,  0.0297]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-0.0091, -0.0667, -0.0127],\n",
       "                        [ 0.0217, -0.0208,  0.0278],\n",
       "                        [-0.0009,  0.0096, -0.0273]],\n",
       "              \n",
       "                       [[-0.0559,  0.0051,  0.0202],\n",
       "                        [-0.0328,  0.0011,  0.0036],\n",
       "                        [ 0.0101, -0.0506,  0.0259]],\n",
       "              \n",
       "                       [[-0.0133,  0.0192, -0.0466],\n",
       "                        [ 0.0493,  0.0066,  0.0269],\n",
       "                        [ 0.0286, -0.0145, -0.0164]]]])),\n",
       "             ('module.classifier_vae.encoder.net.9.bias',\n",
       "              tensor([ 0.0493, -0.0149, -0.0151,  0.0328,  0.0423,  0.0399,  0.0150,  0.0222,\n",
       "                      -0.0001,  0.0009,  0.0087,  0.0066,  0.0398,  0.0294, -0.0057, -0.0337,\n",
       "                       0.0081, -0.0219,  0.0068,  0.0212,  0.0376,  0.0104, -0.0324,  0.0160,\n",
       "                      -0.0381, -0.0165,  0.0104,  0.0254,  0.0041,  0.0065,  0.0161, -0.0067,\n",
       "                       0.0072,  0.0313, -0.0171,  0.0388,  0.0098,  0.0147, -0.0259,  0.0040,\n",
       "                      -0.0267,  0.0036, -0.0274, -0.0326, -0.0244,  0.0309,  0.0119,  0.0090,\n",
       "                       0.0164, -0.0222,  0.0273,  0.0015, -0.0279,  0.0352, -0.0091, -0.0175,\n",
       "                      -0.0028,  0.0150,  0.0014,  0.0391, -0.0052,  0.0402, -0.0398,  0.0298])),\n",
       "             ('module.classifier_vae.encoder.net.10.weight',\n",
       "              tensor([0.9412, 0.9264, 0.9372, 0.9673, 0.9491, 0.8852, 0.8838, 0.9248, 0.9301,\n",
       "                      0.8796, 0.9052, 0.8402, 0.9077, 0.8987, 0.9431, 0.8468, 0.9228, 0.9687,\n",
       "                      0.9656, 0.8504, 0.9369, 0.9405, 0.9378, 0.9607, 0.9588, 0.9683, 0.9174,\n",
       "                      0.9162, 0.9621, 0.9390, 0.9375, 0.8911, 0.9870, 0.9060, 0.9156, 0.9207,\n",
       "                      0.9347, 0.8843, 0.9032, 0.9246, 0.9363, 0.9199, 0.9751, 0.9156, 0.9699,\n",
       "                      0.9318, 0.9080, 0.9002, 0.9168, 0.8975, 0.8680, 0.9231, 0.9497, 0.9654,\n",
       "                      0.9149, 0.9553, 0.8860, 0.9759, 0.9200, 0.9276, 0.8691, 0.9334, 0.9389,\n",
       "                      0.9424])),\n",
       "             ('module.classifier_vae.encoder.net.10.bias',\n",
       "              tensor([-0.1209, -0.1224, -0.1263, -0.1027, -0.0872, -0.1350, -0.1650, -0.1266,\n",
       "                      -0.1344, -0.1140, -0.1364, -0.1533, -0.1458, -0.1967, -0.0835, -0.1686,\n",
       "                      -0.1553, -0.0762, -0.1304, -0.1403, -0.1234, -0.1026, -0.0882, -0.1053,\n",
       "                      -0.0887, -0.1302, -0.1166, -0.0630, -0.0492, -0.1448, -0.0981, -0.1363,\n",
       "                       0.0350, -0.1277, -0.0926, -0.0960, -0.1291, -0.1414, -0.1155, -0.0791,\n",
       "                      -0.0372, -0.1057, -0.1120, -0.1135, -0.0737, -0.0613, -0.0866, -0.1158,\n",
       "                      -0.1693, -0.1497, -0.1394, -0.1120, -0.0784, -0.0762, -0.0938, -0.1387,\n",
       "                      -0.0650, -0.0855, -0.1095, -0.1151, -0.1306, -0.1303, -0.1432, -0.0687])),\n",
       "             ('module.classifier_vae.encoder.net.10.running_mean',\n",
       "              tensor([-0.9513, -1.2612, -1.5580, -1.5224, -1.1812, -0.2450, -1.2592, -1.0422,\n",
       "                      -1.3454,  1.6941, -1.3944,  0.9130, -1.2340,  1.7032, -1.5996, -0.9814,\n",
       "                      -1.5028, -1.7022, -1.5453,  1.3265, -1.0791,  0.1250, -1.7158, -2.1200,\n",
       "                      -1.6222, -0.9734, -1.1017, -1.3841, -2.1319, -1.8767, -1.3782, -1.1660,\n",
       "                       0.0111, -0.8277, -1.1184, -0.9132, -1.1773, -0.9961, -0.9419,  1.4206,\n",
       "                       1.8309,  1.9462, -3.3155, -1.1280, -1.2673,  2.4500,  2.1480,  0.1782,\n",
       "                      -1.1034, -0.5683,  1.5380, -1.0650,  0.0876, -1.3850, -1.1720, -1.3964,\n",
       "                       0.5107, -2.0771, -1.0266, -0.8481, -1.1654, -0.8882, -1.8810, -1.6902])),\n",
       "             ('module.classifier_vae.encoder.net.10.running_var',\n",
       "              tensor([ 7.9165,  6.8296,  6.9091, 10.7433,  7.1575,  3.8311,  4.0531,  5.3350,\n",
       "                       8.3518, 19.7680,  5.8863,  2.1200,  9.2930,  3.8940,  5.7914,  5.6289,\n",
       "                       6.8391,  8.4422,  6.7703,  2.7132,  7.9037,  8.2364,  8.7142,  7.0329,\n",
       "                       6.0619,  8.2147,  3.6981,  3.1419,  6.9795,  6.2240, 10.3015,  5.8887,\n",
       "                       8.9411,  6.8563,  5.1531,  5.4178,  9.5348,  7.8628,  9.7630, 33.7220,\n",
       "                      23.5512,  4.3297, 81.0624,  5.8214,  7.6564,  6.5757, 10.1985,  6.7326,\n",
       "                      12.1329,  4.4074,  3.1751,  6.8884, 11.1172,  9.7367,  8.6415,  6.2413,\n",
       "                      14.6343,  4.1927,  6.9420,  4.7420,  2.8850,  7.8000,  6.5764,  7.6217])),\n",
       "             ('module.classifier_vae.encoder.net.10.num_batches_tracked',\n",
       "              tensor(0)),\n",
       "             ('module.classifier_vae.encoder.h1.weight',\n",
       "              tensor([[-0.0554, -0.0809,  0.0408,  ..., -0.0525,  0.0381, -0.0403],\n",
       "                      [ 0.0078,  0.0480,  0.0137,  ...,  0.1214, -0.0086,  0.0423],\n",
       "                      [-0.0153, -0.0693, -0.0142,  ..., -0.0993,  0.0425,  0.0283],\n",
       "                      ...,\n",
       "                      [-0.0037, -0.0120,  0.0315,  ..., -0.0259, -0.0313,  0.0211],\n",
       "                      [-0.0416, -0.0305,  0.0997,  ..., -0.0201,  0.0686,  0.0383],\n",
       "                      [ 0.0167, -0.0672,  0.0413,  ..., -0.0168, -0.0206, -0.0108]])),\n",
       "             ('module.classifier_vae.encoder.h1.bias',\n",
       "              tensor([-0.0293,  0.0303, -0.0141,  0.0695,  0.0299, -0.0314,  0.0174,  0.0841,\n",
       "                      -0.0310, -0.0466,  0.0194,  0.0182, -0.0537, -0.0561,  0.0266,  0.0446,\n",
       "                       0.0117, -0.0334,  0.0041, -0.0854, -0.0225, -0.0792,  0.0202,  0.0089,\n",
       "                      -0.0809, -0.0101,  0.0861, -0.0169,  0.0588,  0.0511,  0.0905,  0.0672,\n",
       "                       0.0708,  0.0036, -0.0146, -0.0459, -0.0158,  0.0036, -0.0361,  0.0623,\n",
       "                      -0.0684,  0.0186,  0.0471, -0.0087, -0.0189, -0.0329, -0.0490, -0.0663,\n",
       "                      -0.0882, -0.0023, -0.0068, -0.0374, -0.0078,  0.0433,  0.0138,  0.0774,\n",
       "                       0.1130, -0.0194, -0.0322,  0.0657, -0.0077, -0.0423,  0.0155, -0.0525])),\n",
       "             ('module.classifier_vae.encoder.h2.weight',\n",
       "              tensor([[-0.0250,  0.0413,  0.0404,  ...,  0.0813, -0.0063, -0.0601],\n",
       "                      [ 0.0681, -0.0430, -0.0429,  ..., -0.0551, -0.0187, -0.0193],\n",
       "                      [ 0.0264, -0.0091, -0.0445,  ..., -0.0248,  0.0261, -0.0633],\n",
       "                      ...,\n",
       "                      [ 0.0258,  0.0090, -0.0312,  ...,  0.0169, -0.0035, -0.0550],\n",
       "                      [ 0.0475,  0.0338,  0.0520,  ...,  0.0173, -0.0177,  0.0488],\n",
       "                      [ 0.0057,  0.0370, -0.0177,  ..., -0.0389,  0.0153,  0.0381]])),\n",
       "             ('module.classifier_vae.encoder.h2.bias',\n",
       "              tensor([ 0.0004,  0.0067,  0.0302, -0.0226, -0.0420, -0.0287,  0.0035,  0.0222,\n",
       "                      -0.0309,  0.0061, -0.0055, -0.0292,  0.0336,  0.0132,  0.0302,  0.0136,\n",
       "                      -0.0077,  0.0263,  0.0129, -0.0235, -0.0409, -0.0201, -0.0231,  0.0201,\n",
       "                       0.0252, -0.0153, -0.0635,  0.0030,  0.0186, -0.0471, -0.0789, -0.0059,\n",
       "                      -0.0496,  0.0146, -0.0137, -0.0021, -0.0447, -0.0271, -0.0199, -0.0373,\n",
       "                      -0.0275, -0.0114,  0.0231, -0.0510, -0.0139, -0.0256, -0.0132, -0.0419,\n",
       "                       0.0023, -0.0087, -0.0082, -0.0479,  0.0263, -0.0259,  0.0416, -0.0352,\n",
       "                      -0.0552,  0.0366,  0.0133, -0.0509, -0.0295, -0.0254, -0.0210, -0.0313])),\n",
       "             ('module.classifier_vae.classifier.0.weight',\n",
       "              tensor([[ 0.1471, -0.1044,  0.2203,  ...,  0.1450,  0.1142,  0.0989],\n",
       "                      [ 0.0540,  0.1872, -0.1953,  ..., -0.1300, -0.0105, -0.1183],\n",
       "                      [-0.0134, -0.0238, -0.0941,  ..., -0.0720,  0.0275, -0.0309],\n",
       "                      ...,\n",
       "                      [ 0.0702, -0.0079,  0.0282,  ...,  0.1055,  0.0271, -0.0763],\n",
       "                      [-0.0561, -0.0302,  0.0350,  ..., -0.0539, -0.1318,  0.0682],\n",
       "                      [ 0.0975, -0.0254, -0.0381,  ...,  0.0103, -0.0510,  0.0541]])),\n",
       "             ('module.classifier_vae.classifier.0.bias',\n",
       "              tensor([ 0.4163,  0.5024,  0.1209, -0.0991,  0.2938,  0.1149,  0.0355,  0.5039,\n",
       "                       0.0247,  0.0645,  0.1126,  0.1808, -0.0851,  0.1430,  0.1172,  0.1376,\n",
       "                      -0.0256, -0.0766,  0.0110,  0.0793,  0.2094,  0.1325,  0.4272, -0.1022,\n",
       "                       0.2645,  0.2364,  0.4955,  0.3639,  0.1763, -0.0291,  0.4640, -0.0194])),\n",
       "             ('module.classifier_vae.classifier.2.weight',\n",
       "              tensor([[ 0.0427,  0.0029, -0.0073,  0.0670, -0.0850, -0.0381, -0.0293,  0.0245,\n",
       "                       -0.0143,  0.0021,  0.1204,  0.0216, -0.0441, -0.0335, -0.0341, -0.0383,\n",
       "                       -0.0288, -0.0188,  0.0005, -0.0201, -0.0035,  0.0491, -0.0026,  0.0724,\n",
       "                        0.0412,  0.0778, -0.0185, -0.0222,  0.0398,  0.0714, -0.0175, -0.0357],\n",
       "                      [ 0.0371,  0.0165,  0.0269,  0.0241, -0.0623, -0.0516, -0.0345,  0.0143,\n",
       "                       -0.0224, -0.0071,  0.1072,  0.0433, -0.0203, -0.0127,  0.0253, -0.0418,\n",
       "                        0.0116, -0.0392,  0.0046, -0.0216, -0.0054,  0.0550,  0.0334,  0.0854,\n",
       "                        0.0165,  0.0543, -0.0290, -0.0350,  0.0125,  0.0875, -0.0352, -0.0330],\n",
       "                      [ 0.0348,  0.0210, -0.0234,  0.0793, -0.0758, -0.0605, -0.0634,  0.0281,\n",
       "                       -0.0341,  0.0126,  0.1066,  0.0322, -0.0358, -0.0258, -0.0096, -0.0644,\n",
       "                        0.0245,  0.0112, -0.0220, -0.0176, -0.0268,  0.0328,  0.0312,  0.0645,\n",
       "                        0.0354,  0.0574, -0.0224, -0.0263,  0.0398,  0.1077, -0.0253, -0.0301],\n",
       "                      [ 0.0354,  0.0238, -0.0113,  0.0518, -0.0560, -0.0182, -0.0412,  0.0304,\n",
       "                       -0.0142, -0.0260,  0.0888,  0.0414, -0.0240, -0.0073, -0.0039, -0.0451,\n",
       "                       -0.0473, -0.0301, -0.0321, -0.0270, -0.0070,  0.0388, -0.0189,  0.0887,\n",
       "                        0.0312,  0.0766, -0.0222, -0.0507,  0.0316,  0.0628, -0.0491, -0.0359],\n",
       "                      [ 0.0481,  0.0124, -0.0423,  0.0557, -0.0751, -0.0165, -0.0489,  0.0298,\n",
       "                       -0.0003, -0.0289,  0.1339,  0.0436,  0.0167, -0.0051, -0.0344, -0.0446,\n",
       "                       -0.0459, -0.0290, -0.0248, -0.0247, -0.0081,  0.0676, -0.0077,  0.0700,\n",
       "                        0.0292,  0.0652, -0.0084, -0.0460,  0.0230,  0.0854, -0.0216, -0.0321]])),\n",
       "             ('module.classifier_vae.classifier.2.bias',\n",
       "              tensor([ 0.0411,  0.0160, -0.0111,  0.0591,  0.0626]))])"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learner.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('module.encoder.net.0.weight',\n",
       "              tensor([[[[-0.3048, -0.3246,  0.0990],\n",
       "                        [ 0.1896,  0.1979, -0.2656],\n",
       "                        [-0.2700, -0.1848, -0.1144]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.1525,  0.2682, -0.0513],\n",
       "                        [-0.1824,  0.0755, -0.0448],\n",
       "                        [ 0.0886,  0.3032, -0.2761]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.0053, -0.2915, -0.0807],\n",
       "                        [-0.2234, -0.1336,  0.1740],\n",
       "                        [-0.2694, -0.2438,  0.1925]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.3307, -0.0475,  0.2044],\n",
       "                        [ 0.2647,  0.0161, -0.1718],\n",
       "                        [ 0.2601,  0.0647, -0.2202]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.2269,  0.0561, -0.3050],\n",
       "                        [-0.1633, -0.0991,  0.0612],\n",
       "                        [-0.2267, -0.1750, -0.2032]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.2091, -0.0584, -0.0250],\n",
       "                        [-0.2091, -0.2407, -0.2598],\n",
       "                        [-0.0479, -0.1641, -0.3006]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.1807,  0.0349, -0.2644],\n",
       "                        [-0.3239, -0.2977, -0.1466],\n",
       "                        [-0.1723, -0.0628,  0.2128]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.0824,  0.0493, -0.3232],\n",
       "                        [ 0.3299,  0.3312,  0.2295],\n",
       "                        [-0.1254,  0.2935,  0.2061]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.2407, -0.2328,  0.1649],\n",
       "                        [ 0.1510, -0.0107,  0.0916],\n",
       "                        [ 0.3192, -0.3028,  0.2057]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.1404, -0.2607, -0.3097],\n",
       "                        [-0.1699, -0.0546,  0.0972],\n",
       "                        [ 0.0024, -0.0592,  0.1151]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.1974,  0.2653,  0.0505],\n",
       "                        [ 0.2592, -0.1528, -0.2074],\n",
       "                        [-0.2441,  0.0053,  0.2725]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.2805,  0.0133,  0.2504],\n",
       "                        [-0.2529,  0.1378, -0.2619],\n",
       "                        [-0.2140,  0.0756, -0.0282]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.1857, -0.2491,  0.0606],\n",
       "                        [-0.0742, -0.2514,  0.1836],\n",
       "                        [-0.1462,  0.0443, -0.2553]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.0815, -0.3272,  0.1715],\n",
       "                        [ 0.2487, -0.1299,  0.2773],\n",
       "                        [ 0.2964,  0.0686,  0.2392]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.2990, -0.1360,  0.0594],\n",
       "                        [-0.0081, -0.0827,  0.1781],\n",
       "                        [ 0.2457,  0.2507, -0.1789]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.0460, -0.0422,  0.0559],\n",
       "                        [ 0.2121,  0.3364, -0.0501],\n",
       "                        [ 0.0474,  0.2599, -0.0224]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.2919, -0.2926,  0.0283],\n",
       "                        [-0.0496, -0.1627, -0.2360],\n",
       "                        [ 0.1393,  0.0795, -0.2259]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.1302, -0.0438, -0.1044],\n",
       "                        [-0.2993,  0.0469,  0.3063],\n",
       "                        [-0.2076,  0.2351,  0.2889]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.2641, -0.0919, -0.0108],\n",
       "                        [-0.0454, -0.2144,  0.2198],\n",
       "                        [ 0.1030, -0.0767, -0.0768]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.1696,  0.0756,  0.0777],\n",
       "                        [ 0.3190, -0.2736, -0.1602],\n",
       "                        [-0.1459, -0.2855,  0.2915]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.1758,  0.2024, -0.2242],\n",
       "                        [ 0.1311,  0.0807, -0.1546],\n",
       "                        [-0.0462, -0.0135, -0.3133]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.0706,  0.2312, -0.2684],\n",
       "                        [-0.1377, -0.2766,  0.3306],\n",
       "                        [ 0.2358,  0.0492, -0.1715]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.3338,  0.1478, -0.3151],\n",
       "                        [ 0.3027, -0.2103,  0.2785],\n",
       "                        [-0.1515, -0.0986, -0.1233]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.1489, -0.1520,  0.1248],\n",
       "                        [ 0.1222,  0.3005,  0.2915],\n",
       "                        [-0.1586,  0.1922,  0.2613]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.0045,  0.0295,  0.2175],\n",
       "                        [ 0.1914,  0.2646,  0.0138],\n",
       "                        [-0.3175, -0.0791, -0.0217]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.0729, -0.1990,  0.1203],\n",
       "                        [-0.1999,  0.0087,  0.0755],\n",
       "                        [-0.2243, -0.3306,  0.1835]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.1700,  0.0776,  0.3010],\n",
       "                        [ 0.2798, -0.2119, -0.3327],\n",
       "                        [-0.1123, -0.0200,  0.1808]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.1567,  0.2781, -0.0381],\n",
       "                        [ 0.3233, -0.0907,  0.2563],\n",
       "                        [-0.2775,  0.2134, -0.0620]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.3343, -0.1497, -0.2270],\n",
       "                        [-0.1028,  0.0643, -0.1313],\n",
       "                        [-0.2035,  0.1541,  0.1822]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.1201, -0.1942,  0.0519],\n",
       "                        [ 0.2532, -0.0262, -0.2109],\n",
       "                        [ 0.2519,  0.2321,  0.3319]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.2147,  0.1643, -0.2682],\n",
       "                        [-0.2282,  0.0413, -0.3331],\n",
       "                        [-0.0491,  0.2110,  0.3042]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.0340, -0.0558,  0.0071],\n",
       "                        [-0.1149,  0.2054, -0.0682],\n",
       "                        [-0.2662,  0.3213,  0.2586]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.0466,  0.0778, -0.3235],\n",
       "                        [ 0.2929, -0.1384,  0.2795],\n",
       "                        [ 0.1430, -0.1813, -0.2894]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.2894, -0.0829,  0.2303],\n",
       "                        [-0.0539,  0.2211, -0.0099],\n",
       "                        [ 0.2628, -0.0663,  0.2994]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.1679,  0.3029, -0.1353],\n",
       "                        [ 0.3040, -0.2210,  0.1856],\n",
       "                        [ 0.0369,  0.1919, -0.0700]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.2633, -0.0771, -0.1897],\n",
       "                        [-0.0827, -0.1577, -0.0134],\n",
       "                        [-0.3132,  0.1564, -0.1328]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.2539, -0.0126,  0.3179],\n",
       "                        [ 0.2819,  0.1603, -0.0621],\n",
       "                        [ 0.0177,  0.0690, -0.2674]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.0433,  0.2551,  0.0606],\n",
       "                        [ 0.1227, -0.2277, -0.2721],\n",
       "                        [ 0.0198,  0.0142, -0.0703]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.0820, -0.1877,  0.0894],\n",
       "                        [ 0.0329, -0.3360,  0.2270],\n",
       "                        [ 0.3095, -0.1036, -0.1270]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.2017, -0.0504, -0.2870],\n",
       "                        [-0.3117,  0.0603, -0.0166],\n",
       "                        [-0.0991, -0.2278,  0.2081]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.1479,  0.0783, -0.0614],\n",
       "                        [-0.1452, -0.0664, -0.2859],\n",
       "                        [ 0.0377, -0.2258,  0.0409]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.2094, -0.0279,  0.3142],\n",
       "                        [ 0.3040,  0.2345, -0.1253],\n",
       "                        [ 0.3044, -0.0893, -0.0490]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.1700,  0.2533, -0.0231],\n",
       "                        [-0.3122,  0.2969,  0.2280],\n",
       "                        [-0.1200, -0.1158, -0.1388]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.3212, -0.2039,  0.2434],\n",
       "                        [ 0.2566, -0.0030, -0.2724],\n",
       "                        [-0.2069,  0.0360, -0.2328]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.1596,  0.1515,  0.0826],\n",
       "                        [-0.0263,  0.1161, -0.2643],\n",
       "                        [-0.0472, -0.3218,  0.1185]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.1483, -0.0599,  0.1369],\n",
       "                        [ 0.2997,  0.1907,  0.1162],\n",
       "                        [ 0.0983, -0.0555,  0.1289]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.1661,  0.2917,  0.2644],\n",
       "                        [ 0.2419, -0.0848, -0.0042],\n",
       "                        [-0.2892, -0.3146,  0.2866]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.1860,  0.2332,  0.0368],\n",
       "                        [-0.0644,  0.0293, -0.0784],\n",
       "                        [ 0.1757, -0.1883,  0.0149]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.1065, -0.2457, -0.0944],\n",
       "                        [-0.1981,  0.0251, -0.1887],\n",
       "                        [ 0.1934,  0.2098,  0.1244]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.1118, -0.1971, -0.2347],\n",
       "                        [ 0.3161, -0.0022,  0.0214],\n",
       "                        [ 0.2228, -0.0701,  0.0477]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.2486,  0.1487, -0.2497],\n",
       "                        [ 0.2305,  0.2999,  0.2740],\n",
       "                        [-0.2643, -0.2661, -0.2644]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.1471,  0.1945, -0.3366],\n",
       "                        [-0.0187, -0.2644,  0.3279],\n",
       "                        [-0.1467, -0.2203,  0.2946]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.1384, -0.0967, -0.0567],\n",
       "                        [ 0.0120,  0.2378,  0.2766],\n",
       "                        [ 0.1622,  0.0154, -0.2603]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.1261, -0.2702,  0.3083],\n",
       "                        [-0.3403,  0.2697, -0.2829],\n",
       "                        [ 0.0301, -0.0251,  0.0229]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.0662, -0.2866,  0.1040],\n",
       "                        [ 0.0058,  0.0055,  0.3184],\n",
       "                        [ 0.1889, -0.3019,  0.0144]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.3026,  0.2698, -0.1485],\n",
       "                        [-0.1426, -0.2765,  0.0105],\n",
       "                        [ 0.1520,  0.1925, -0.2448]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.2912, -0.1079,  0.1935],\n",
       "                        [ 0.1400,  0.1681,  0.1271],\n",
       "                        [-0.2032,  0.1996, -0.1440]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.2498,  0.0958, -0.0834],\n",
       "                        [-0.0561, -0.2745, -0.2340],\n",
       "                        [-0.2765,  0.1374, -0.1550]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.2551,  0.2711, -0.1506],\n",
       "                        [-0.3161,  0.2988, -0.3047],\n",
       "                        [ 0.2524,  0.1620,  0.0765]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.1786, -0.1224,  0.1429],\n",
       "                        [ 0.2278,  0.0087,  0.1962],\n",
       "                        [-0.1502, -0.2701,  0.2099]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.1180,  0.2298,  0.0467],\n",
       "                        [ 0.0935, -0.2726, -0.1611],\n",
       "                        [ 0.1707, -0.0096,  0.3092]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.1154, -0.2868, -0.2347],\n",
       "                        [ 0.0129, -0.2664, -0.2809],\n",
       "                        [ 0.0567,  0.0544, -0.1846]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.0881,  0.0852, -0.3221],\n",
       "                        [-0.1222, -0.0467,  0.1397],\n",
       "                        [ 0.1487,  0.2561, -0.0671]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.0961, -0.2018, -0.3247],\n",
       "                        [ 0.1313, -0.0361,  0.0476],\n",
       "                        [-0.1149,  0.2479,  0.2982]]]])),\n",
       "             ('module.encoder.net.0.bias',\n",
       "              tensor([-0.2070, -0.1486,  0.2517,  0.2893, -0.2616,  0.0265, -0.0996,  0.1476,\n",
       "                      -0.2088, -0.1319, -0.2589,  0.2874,  0.2921, -0.0794, -0.1275, -0.2786,\n",
       "                       0.2239, -0.1820,  0.1837, -0.2108,  0.2914, -0.1093, -0.0829,  0.2505,\n",
       "                      -0.3259,  0.2944, -0.1029, -0.1952, -0.1647, -0.1430,  0.0411, -0.1047,\n",
       "                       0.0312, -0.2828, -0.0292,  0.1894, -0.3114, -0.1672, -0.2809,  0.3209,\n",
       "                      -0.0795, -0.2942,  0.2382, -0.0576, -0.2366,  0.2045, -0.3161,  0.1060,\n",
       "                      -0.2129, -0.0763,  0.3349, -0.2869,  0.0284,  0.0449, -0.1564,  0.2865,\n",
       "                      -0.0195, -0.1686,  0.0640,  0.0940, -0.1729, -0.1604,  0.0609, -0.0285])),\n",
       "             ('module.encoder.net.1.weight',\n",
       "              tensor([1.0382, 0.9996, 0.9997, 0.9966, 1.0252, 1.0330, 1.0025, 1.0121, 1.0051,\n",
       "                      1.0073, 0.9899, 0.9964, 1.0133, 0.9919, 0.9954, 1.0000, 1.0464, 0.9854,\n",
       "                      0.9940, 0.9942, 0.9959, 0.9970, 1.0113, 1.0000, 1.0093, 1.0060, 0.9852,\n",
       "                      1.0004, 1.0082, 0.9976, 0.9968, 0.9922, 0.9970, 0.9899, 1.0158, 1.0669,\n",
       "                      0.9984, 0.9974, 0.9878, 1.0087, 1.0948, 1.0010, 1.0068, 0.9890, 0.9985,\n",
       "                      1.0010, 1.0046, 0.9949, 0.9921, 1.0070, 0.9998, 0.9900, 1.0070, 0.9900,\n",
       "                      0.9945, 1.0085, 1.0001, 0.9967, 1.0134, 0.9818, 1.0059, 1.0286, 0.9928,\n",
       "                      0.9653])),\n",
       "             ('module.encoder.net.1.bias',\n",
       "              tensor([ 0.0387, -0.0061,  0.0078,  0.0049,  0.0275,  0.0511,  0.0069,  0.0203,\n",
       "                       0.0119,  0.0332,  0.0162,  0.0021,  0.0211, -0.0051,  0.0069,  0.0076,\n",
       "                       0.0537,  0.0047,  0.0046,  0.0194, -0.0125, -0.0252,  0.0249,  0.0104,\n",
       "                       0.0120,  0.0144, -0.0042,  0.0154,  0.0129,  0.0064,  0.0214, -0.0018,\n",
       "                       0.0015,  0.0108,  0.0245,  0.0688,  0.0208, -0.0043, -0.0038,  0.0155,\n",
       "                       0.0816,  0.0169, -0.0031, -0.0166,  0.0066,  0.0146,  0.0134,  0.0072,\n",
       "                      -0.0009,  0.0055,  0.0048, -0.0068,  0.0193, -0.0029, -0.0032,  0.0235,\n",
       "                       0.0068,  0.0111,  0.0006,  0.0198,  0.0143,  0.0307, -0.0100, -0.0002])),\n",
       "             ('module.encoder.net.1.running_mean',\n",
       "              tensor([-0.2946, -0.1453,  0.1753,  0.3528, -0.3753, -0.0723, -0.1740,  0.2424,\n",
       "                      -0.1550, -0.1761, -0.2515,  0.2861,  0.2138,  0.0012, -0.0708, -0.2046,\n",
       "                       0.1332, -0.1501,  0.1887, -0.2371,  0.2777, -0.1125, -0.1284,  0.3523,\n",
       "                      -0.2988,  0.2485, -0.1014, -0.1536, -0.2315, -0.0920,  0.0472, -0.0732,\n",
       "                       0.0160, -0.2385,  0.0390,  0.0951, -0.2886, -0.1723, -0.2997,  0.2393,\n",
       "                      -0.1476, -0.2363,  0.2345, -0.0634, -0.2721,  0.2667, -0.2947,  0.1367,\n",
       "                      -0.2368, -0.0840,  0.3070, -0.3162,  0.0654,  0.0222, -0.1474,  0.2398,\n",
       "                      -0.0104, -0.2225,  0.0678,  0.1310, -0.1276, -0.2700,  0.0556, -0.0162])),\n",
       "             ('module.encoder.net.1.running_var',\n",
       "              tensor([0.0307, 0.0085, 0.0389, 0.0324, 0.0481, 0.0505, 0.0349, 0.0556, 0.0164,\n",
       "                      0.0151, 0.0044, 0.0089, 0.0288, 0.0345, 0.0159, 0.0306, 0.0415, 0.0213,\n",
       "                      0.0040, 0.0099, 0.0190, 0.0039, 0.0105, 0.0508, 0.0143, 0.0219, 0.0049,\n",
       "                      0.0129, 0.0298, 0.0268, 0.0136, 0.0144, 0.0118, 0.0190, 0.0232, 0.0361,\n",
       "                      0.0112, 0.0087, 0.0107, 0.0333, 0.0214, 0.0222, 0.0143, 0.0149, 0.0083,\n",
       "                      0.0204, 0.0192, 0.0091, 0.0181, 0.0136, 0.0179, 0.0147, 0.0123, 0.0060,\n",
       "                      0.0080, 0.0148, 0.0095, 0.0219, 0.0159, 0.0148, 0.0118, 0.0601, 0.0080,\n",
       "                      0.0161])),\n",
       "             ('module.encoder.net.1.num_batches_tracked', tensor(0)),\n",
       "             ('module.encoder.net.3.weight',\n",
       "              tensor([[[[ 1.6459e-02, -1.9059e-02,  3.3600e-02],\n",
       "                        [-4.0648e-02, -1.0401e-03, -8.0253e-03],\n",
       "                        [ 2.6134e-02,  8.3181e-03, -2.3878e-02]],\n",
       "              \n",
       "                       [[-3.3299e-02,  3.8649e-02,  1.9250e-02],\n",
       "                        [-3.6596e-02, -1.2689e-02, -2.2459e-02],\n",
       "                        [ 2.4771e-02, -2.6010e-02,  2.0212e-02]],\n",
       "              \n",
       "                       [[-2.1497e-02, -1.4662e-02,  8.4233e-04],\n",
       "                        [ 1.4797e-02,  9.5170e-03, -1.0768e-02],\n",
       "                        [ 4.8357e-03, -5.8560e-03, -3.2699e-02]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-2.3983e-02,  2.3009e-02,  3.7953e-02],\n",
       "                        [ 1.0271e-02, -4.4912e-02,  2.2615e-02],\n",
       "                        [ 4.0933e-02, -1.9024e-02, -2.1952e-02]],\n",
       "              \n",
       "                       [[ 4.1025e-02, -2.9316e-03,  1.0683e-03],\n",
       "                        [ 2.1281e-02,  2.5206e-02,  6.8350e-03],\n",
       "                        [-9.4442e-03,  4.7486e-02,  2.5457e-02]],\n",
       "              \n",
       "                       [[-3.7878e-03, -9.5362e-03, -2.1404e-02],\n",
       "                        [-1.5908e-02, -2.3608e-02, -2.2763e-02],\n",
       "                        [-1.0757e-02, -2.4115e-02,  4.0388e-03]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 3.1156e-02, -1.4677e-02,  2.8345e-02],\n",
       "                        [-1.6712e-02, -6.8418e-03, -7.4294e-03],\n",
       "                        [-2.1750e-02,  1.9912e-02, -3.9453e-03]],\n",
       "              \n",
       "                       [[ 2.5668e-02, -4.0596e-03,  2.9718e-02],\n",
       "                        [ 3.3208e-02,  9.9584e-03,  3.3475e-02],\n",
       "                        [ 3.6819e-02, -1.9795e-02,  1.3812e-02]],\n",
       "              \n",
       "                       [[ 1.8973e-02, -3.9696e-02,  1.8218e-02],\n",
       "                        [-2.5750e-02,  1.7079e-02,  3.2400e-02],\n",
       "                        [ 6.6068e-03,  2.3505e-02,  4.0770e-02]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 8.7131e-03, -3.4566e-02, -1.3344e-03],\n",
       "                        [ 2.1238e-02, -2.2963e-02,  3.5212e-02],\n",
       "                        [ 2.2786e-02, -4.3979e-02,  4.2891e-03]],\n",
       "              \n",
       "                       [[ 4.0536e-02, -1.4693e-02,  2.4356e-02],\n",
       "                        [ 4.0767e-02,  2.5900e-02,  4.7965e-02],\n",
       "                        [ 2.4137e-02,  3.0470e-02,  3.3985e-02]],\n",
       "              \n",
       "                       [[-1.1114e-02, -3.9844e-03,  2.9721e-02],\n",
       "                        [ 3.4441e-02,  2.5291e-02,  2.5166e-02],\n",
       "                        [-1.1655e-02,  3.2186e-04, -9.0710e-03]]],\n",
       "              \n",
       "              \n",
       "                      [[[-6.2864e-03,  2.3361e-02,  4.5011e-02],\n",
       "                        [ 4.7087e-02, -2.7166e-02,  4.0284e-02],\n",
       "                        [-1.6336e-02,  4.4183e-02,  3.8265e-02]],\n",
       "              \n",
       "                       [[ 3.3715e-02, -2.1075e-02,  3.8401e-02],\n",
       "                        [-3.3877e-02, -3.7475e-02,  2.4958e-02],\n",
       "                        [-1.9032e-02, -2.7140e-02,  1.8163e-02]],\n",
       "              \n",
       "                       [[ 3.9709e-02, -1.5941e-02, -1.8327e-03],\n",
       "                        [ 1.6632e-02, -1.0098e-02, -3.2897e-02],\n",
       "                        [ 3.6682e-02, -2.7025e-02, -1.3396e-02]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-5.0142e-03,  3.1645e-02,  3.2989e-02],\n",
       "                        [ 2.1450e-03,  4.5208e-02,  1.6314e-02],\n",
       "                        [ 3.3705e-02,  1.6648e-02,  1.8825e-02]],\n",
       "              \n",
       "                       [[-2.5113e-02, -2.0685e-02, -1.4772e-02],\n",
       "                        [ 9.1680e-03,  4.5814e-03, -1.0437e-02],\n",
       "                        [ 1.4785e-02,  3.0231e-02,  2.7059e-02]],\n",
       "              \n",
       "                       [[-1.2299e-02,  7.1070e-03, -4.2341e-02],\n",
       "                        [ 1.6848e-02,  1.0266e-02, -3.2788e-02],\n",
       "                        [ 2.6942e-02,  2.7358e-02, -3.0577e-02]]],\n",
       "              \n",
       "              \n",
       "                      ...,\n",
       "              \n",
       "              \n",
       "                      [[[ 2.7007e-02,  4.3203e-03, -4.0098e-02],\n",
       "                        [ 7.3773e-03, -3.6827e-03, -3.4564e-02],\n",
       "                        [ 2.0303e-02, -1.1539e-02, -2.7190e-03]],\n",
       "              \n",
       "                       [[ 2.4534e-03,  2.3001e-02, -4.0787e-03],\n",
       "                        [-3.9367e-02,  4.2061e-03, -1.3634e-02],\n",
       "                        [-1.6989e-02,  3.3516e-02,  1.2760e-02]],\n",
       "              \n",
       "                       [[ 1.8197e-02, -2.0955e-02,  2.1441e-02],\n",
       "                        [ 2.4812e-02, -2.3712e-02, -1.2697e-02],\n",
       "                        [ 2.4007e-02, -1.6260e-02,  1.7030e-02]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 4.2056e-02, -1.6388e-02,  3.0547e-02],\n",
       "                        [ 2.7552e-02,  2.2064e-02, -2.9289e-02],\n",
       "                        [ 9.3745e-03, -4.4040e-02, -2.6089e-02]],\n",
       "              \n",
       "                       [[-2.5873e-02,  1.2733e-02, -3.7663e-02],\n",
       "                        [-1.7488e-02,  2.9101e-02, -4.9285e-03],\n",
       "                        [ 3.6630e-03,  4.1807e-02, -3.5511e-02]],\n",
       "              \n",
       "                       [[ 3.5993e-02,  2.6721e-02, -9.7076e-03],\n",
       "                        [-2.1760e-02, -2.7542e-02, -4.1441e-02],\n",
       "                        [ 2.0669e-02,  2.7003e-02,  3.3566e-02]]],\n",
       "              \n",
       "              \n",
       "                      [[[-1.9919e-02,  3.3447e-02,  1.6505e-02],\n",
       "                        [ 1.1800e-02,  1.3377e-02, -2.9931e-02],\n",
       "                        [ 3.2697e-02,  2.0249e-02, -2.9170e-03]],\n",
       "              \n",
       "                       [[-2.8094e-02,  9.8082e-03, -4.2440e-02],\n",
       "                        [-3.0075e-02,  6.0475e-03, -2.7372e-02],\n",
       "                        [ 1.0844e-02, -2.2456e-02, -9.5750e-03]],\n",
       "              \n",
       "                       [[ 1.3702e-02, -9.3012e-03, -2.8410e-02],\n",
       "                        [-4.1672e-02,  4.3054e-02, -1.1056e-02],\n",
       "                        [ 3.5805e-02, -2.5287e-02,  1.8148e-02]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 4.1921e-02, -2.8062e-02, -3.5575e-02],\n",
       "                        [-9.6655e-03,  2.0034e-02, -6.7542e-03],\n",
       "                        [-3.1613e-04,  3.5341e-02, -3.6157e-03]],\n",
       "              \n",
       "                       [[-3.3454e-02, -3.7618e-02,  2.1823e-02],\n",
       "                        [ 6.0980e-03,  2.3400e-02,  3.7408e-02],\n",
       "                        [-2.5761e-02, -3.0904e-02, -3.7276e-03]],\n",
       "              \n",
       "                       [[ 2.6402e-02, -3.9293e-02,  2.3046e-02],\n",
       "                        [ 1.1521e-02, -1.8633e-02, -3.5864e-02],\n",
       "                        [ 2.6152e-02, -6.0387e-03,  2.4094e-02]]],\n",
       "              \n",
       "              \n",
       "                      [[[-1.7197e-02,  9.9533e-03,  4.2981e-03],\n",
       "                        [-1.7822e-02, -3.2461e-03, -1.5958e-03],\n",
       "                        [ 1.0050e-02,  9.9803e-03, -4.7592e-02]],\n",
       "              \n",
       "                       [[ 2.7133e-02, -2.8513e-03,  1.1077e-02],\n",
       "                        [ 5.1875e-02, -4.4350e-03, -1.1480e-02],\n",
       "                        [-2.2298e-02,  4.0941e-02,  8.6952e-05]],\n",
       "              \n",
       "                       [[ 1.0537e-02,  7.8316e-03, -3.3632e-02],\n",
       "                        [-1.5644e-02,  4.6105e-03, -3.9087e-02],\n",
       "                        [-2.6474e-02, -6.5675e-03,  3.6897e-02]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-1.9685e-02, -2.4872e-02, -4.3806e-02],\n",
       "                        [-1.9299e-02, -4.1078e-02,  2.5132e-03],\n",
       "                        [ 1.8644e-02, -4.7315e-02,  2.8706e-02]],\n",
       "              \n",
       "                       [[ 8.8644e-03, -4.0231e-02,  2.8056e-02],\n",
       "                        [ 8.5113e-03,  4.4804e-02,  9.0547e-03],\n",
       "                        [-1.2087e-02, -1.8040e-02,  3.7529e-02]],\n",
       "              \n",
       "                       [[ 4.4462e-02,  6.0812e-03,  2.1907e-02],\n",
       "                        [-3.0413e-03, -2.9392e-02,  2.5633e-02],\n",
       "                        [-1.4616e-02,  2.0270e-03,  4.9902e-02]]]])),\n",
       "             ('module.encoder.net.3.bias',\n",
       "              tensor([-0.0368,  0.0003,  0.0301,  0.0456, -0.0299,  0.0039,  0.0066,  0.0296,\n",
       "                      -0.0109, -0.0377, -0.0402, -0.0197,  0.0082, -0.0324, -0.0376,  0.0020,\n",
       "                       0.0119, -0.0206,  0.0382,  0.0313, -0.0078, -0.0144, -0.0012,  0.0045,\n",
       "                      -0.0254, -0.0080,  0.0297,  0.0252,  0.0163,  0.0197,  0.0164,  0.0316,\n",
       "                      -0.0118,  0.0257, -0.0215, -0.0130,  0.0410,  0.0327, -0.0296, -0.0365,\n",
       "                      -0.0110, -0.0353, -0.0050, -0.0356,  0.0105,  0.0030, -0.0344,  0.0172,\n",
       "                       0.0201,  0.0122,  0.0154, -0.0045, -0.0243, -0.0028, -0.0313,  0.0018,\n",
       "                      -0.0033,  0.0008, -0.0093, -0.0361,  0.0068,  0.0221,  0.0199,  0.0331])),\n",
       "             ('module.encoder.net.4.weight',\n",
       "              tensor([0.9900, 0.9905, 1.0110, 1.0355, 1.0295, 1.0447, 0.9962, 0.9878, 0.9767,\n",
       "                      1.0239, 1.0238, 1.0524, 0.9825, 0.9938, 0.9835, 0.9947, 1.0015, 1.0266,\n",
       "                      1.0171, 0.9923, 1.0136, 1.0135, 1.0443, 1.0124, 0.9845, 1.1110, 0.9828,\n",
       "                      1.0553, 1.0032, 0.9890, 1.0101, 1.0698, 1.0024, 0.9982, 0.9905, 1.0302,\n",
       "                      0.9885, 0.9959, 0.9870, 0.9904, 0.9929, 0.9999, 0.9851, 0.9649, 1.0077,\n",
       "                      0.9655, 0.9956, 0.9687, 1.0148, 1.0204, 1.0042, 1.0080, 0.9798, 0.9959,\n",
       "                      1.0156, 0.9719, 1.0131, 1.0159, 0.9843, 0.9906, 1.0028, 0.9853, 1.0138,\n",
       "                      0.9845])),\n",
       "             ('module.encoder.net.4.bias',\n",
       "              tensor([ 0.0330,  0.0362,  0.0735,  0.0491,  0.0591,  0.0385,  0.0197,  0.0141,\n",
       "                       0.0428,  0.0511,  0.1449,  0.0763, -0.0010,  0.0129,  0.0314,  0.0141,\n",
       "                       0.0304,  0.0349,  0.0764, -0.0026,  0.0181,  0.0819,  0.0513,  0.0259,\n",
       "                       0.0268,  0.0850,  0.0076,  0.0545,  0.0303,  0.0087,  0.0573,  0.0408,\n",
       "                       0.0269,  0.0497,  0.0206,  0.0318,  0.0165,  0.0160,  0.0133,  0.0174,\n",
       "                       0.0192,  0.0158,  0.0077,  0.0190,  0.0590,  0.0074,  0.0223, -0.0091,\n",
       "                       0.0364,  0.0275,  0.0315,  0.0300,  0.0132,  0.0205,  0.0793,  0.0194,\n",
       "                       0.0346,  0.0313,  0.0059,  0.0224,  0.0276,  0.0023,  0.0618,  0.0027])),\n",
       "             ('module.encoder.net.4.running_mean',\n",
       "              tensor([ 0.9170,  0.5561, -0.2867,  0.4353, -0.6764, -0.4907,  0.0773,  0.5084,\n",
       "                       0.6217,  0.1478, -0.1306, -0.6361,  0.5236,  0.4013,  0.6715,  0.3384,\n",
       "                      -0.4333, -0.1821, -0.6335,  0.2760,  0.0783, -0.1285, -0.0707, -0.3698,\n",
       "                       0.7321,  0.5030,  0.1355,  0.4025,  0.3550,  0.5404,  0.2194, -0.3576,\n",
       "                       0.1532,  0.3284,  0.3149, -0.4173,  0.6068, -0.2958,  0.1366, -0.4271,\n",
       "                      -0.6093,  0.4590,  0.7189,  0.6258, -0.2236,  0.3482,  0.4596,  0.4021,\n",
       "                      -0.3207,  0.2899, -0.4984,  0.1836,  0.3498,  1.0073, -0.4782,  0.2736,\n",
       "                      -0.3475, -0.7934,  0.4883,  0.0717,  0.5315,  0.4466, -0.3675,  0.4681])),\n",
       "             ('module.encoder.net.4.running_var',\n",
       "              tensor([0.9995, 1.2786, 1.1791, 0.6157, 1.7023, 2.4535, 0.7973, 1.1078, 1.8041,\n",
       "                      0.4699, 0.9706, 0.8848, 0.5412, 0.5854, 2.0825, 0.7199, 0.9647, 0.6768,\n",
       "                      1.8350, 1.0936, 1.0832, 1.1898, 1.0441, 0.6445, 1.4077, 0.6814, 0.3467,\n",
       "                      0.5621, 0.8543, 0.7813, 0.8912, 0.5520, 0.3355, 0.4404, 1.1437, 1.0424,\n",
       "                      1.4859, 1.0420, 0.5817, 0.8929, 1.5622, 1.4186, 1.3934, 0.9714, 1.1257,\n",
       "                      0.7146, 1.5080, 0.7598, 0.9463, 0.4571, 0.8108, 0.4102, 1.0838, 2.0395,\n",
       "                      0.7071, 0.9132, 1.1486, 0.8759, 0.9827, 0.5053, 1.6582, 0.7489, 0.9095,\n",
       "                      1.2424])),\n",
       "             ('module.encoder.net.4.num_batches_tracked', tensor(0)),\n",
       "             ('module.encoder.net.6.weight',\n",
       "              tensor([[[[ 4.1249e-02,  1.5640e-02, -2.0453e-02],\n",
       "                        [ 2.5343e-02,  4.5625e-02, -1.6843e-02],\n",
       "                        [ 3.3529e-03, -1.6803e-02, -1.9373e-02]],\n",
       "              \n",
       "                       [[ 9.7169e-03,  1.9865e-02,  1.6556e-02],\n",
       "                        [-1.0356e-02,  2.7486e-02, -5.8626e-03],\n",
       "                        [-1.9536e-02,  3.0616e-02,  4.6027e-02]],\n",
       "              \n",
       "                       [[-1.4752e-02,  3.6710e-02, -7.5501e-03],\n",
       "                        [ 1.3403e-02,  3.1583e-03, -3.2705e-03],\n",
       "                        [-9.2099e-03,  9.8453e-03, -1.1870e-02]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 9.6799e-05,  4.8429e-02,  3.7554e-02],\n",
       "                        [ 2.7128e-02,  2.8597e-02, -1.9672e-02],\n",
       "                        [ 2.7514e-02,  3.5519e-03, -4.0518e-03]],\n",
       "              \n",
       "                       [[-1.5755e-02,  2.3156e-02,  2.3147e-02],\n",
       "                        [ 2.0081e-02,  2.0930e-02, -2.6088e-02],\n",
       "                        [-1.1482e-02, -2.9061e-02,  4.8385e-02]],\n",
       "              \n",
       "                       [[ 3.3027e-02,  9.8396e-03,  3.3631e-02],\n",
       "                        [-3.4186e-02,  1.7303e-02, -1.8685e-02],\n",
       "                        [ 3.7608e-02,  2.7328e-02, -1.8662e-03]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 8.4086e-03,  2.4190e-02, -2.1767e-02],\n",
       "                        [-3.8054e-03,  3.2438e-02, -1.4202e-02],\n",
       "                        [-1.6208e-02,  9.8660e-04,  1.5514e-02]],\n",
       "              \n",
       "                       [[ 1.4367e-02,  2.9213e-02, -3.8129e-02],\n",
       "                        [-3.4583e-03, -9.0316e-03, -1.2050e-02],\n",
       "                        [ 2.8369e-02,  1.6894e-02,  2.8605e-02]],\n",
       "              \n",
       "                       [[-4.7573e-02, -5.5183e-03,  2.3488e-02],\n",
       "                        [-1.6613e-02,  2.8780e-02,  1.2366e-02],\n",
       "                        [-1.1663e-02, -9.5768e-03, -1.1231e-03]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 2.1595e-02, -4.1750e-02, -4.9761e-03],\n",
       "                        [-1.9532e-02,  2.6782e-03, -1.3938e-02],\n",
       "                        [ 5.9479e-03,  1.4843e-02,  2.9837e-03]],\n",
       "              \n",
       "                       [[ 1.9697e-02,  5.3979e-03, -3.7053e-02],\n",
       "                        [ 2.5240e-02,  4.5070e-02,  1.4353e-02],\n",
       "                        [-3.4167e-02,  4.4431e-02,  3.1509e-02]],\n",
       "              \n",
       "                       [[-2.3442e-02, -2.6453e-02, -2.6464e-02],\n",
       "                        [ 1.2974e-02, -4.1285e-02,  8.3370e-03],\n",
       "                        [ 2.7510e-03,  2.8201e-02,  8.8505e-03]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 1.2922e-02, -2.1977e-02, -8.2199e-03],\n",
       "                        [ 4.0239e-02, -3.0815e-03,  1.2055e-02],\n",
       "                        [ 3.9597e-02,  3.9198e-02, -1.8322e-02]],\n",
       "              \n",
       "                       [[-2.4283e-02, -1.0516e-02,  9.1864e-03],\n",
       "                        [-1.6003e-02, -1.1118e-03,  2.1776e-02],\n",
       "                        [ 4.5875e-02, -1.6058e-02,  4.3021e-02]],\n",
       "              \n",
       "                       [[ 2.2182e-02,  1.0419e-02,  3.7815e-02],\n",
       "                        [-2.0837e-03,  1.2920e-02, -1.8981e-02],\n",
       "                        [-1.3888e-02, -2.2968e-02, -2.5098e-03]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 1.7163e-02,  2.4364e-02,  2.5244e-02],\n",
       "                        [ 4.6251e-02,  2.8997e-02,  2.9972e-03],\n",
       "                        [ 2.3234e-03, -1.7611e-02,  7.4759e-04]],\n",
       "              \n",
       "                       [[ 3.2329e-02,  2.4764e-02,  1.6960e-02],\n",
       "                        [ 1.0207e-02,  3.6582e-02, -1.9847e-02],\n",
       "                        [-3.1357e-03, -1.1866e-02,  2.7873e-02]],\n",
       "              \n",
       "                       [[-2.4841e-02,  7.8399e-03,  1.0117e-02],\n",
       "                        [ 3.5873e-02,  1.5022e-02, -1.3320e-02],\n",
       "                        [ 4.5608e-03,  2.5579e-02,  4.9539e-02]]],\n",
       "              \n",
       "              \n",
       "                      ...,\n",
       "              \n",
       "              \n",
       "                      [[[ 1.0379e-02,  4.0356e-03, -4.9341e-02],\n",
       "                        [-1.4956e-02,  1.6185e-02, -2.3246e-02],\n",
       "                        [ 2.2827e-02,  1.1862e-02, -1.3751e-02]],\n",
       "              \n",
       "                       [[-2.7686e-03,  2.8022e-02,  1.9151e-02],\n",
       "                        [ 2.9619e-02,  4.4093e-02,  2.9879e-02],\n",
       "                        [-2.8373e-02, -1.3691e-02, -4.7038e-03]],\n",
       "              \n",
       "                       [[ 9.3094e-03,  2.7671e-02, -5.9556e-03],\n",
       "                        [ 3.6744e-02, -9.9867e-03, -9.9340e-03],\n",
       "                        [ 4.8096e-02,  3.9905e-02, -2.7361e-02]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 2.7969e-02, -3.2239e-02,  9.8357e-03],\n",
       "                        [ 1.6095e-02,  1.9821e-02, -1.3429e-02],\n",
       "                        [ 3.1920e-03,  2.8269e-02, -3.8267e-02]],\n",
       "              \n",
       "                       [[ 8.5563e-03,  2.6857e-02, -2.4929e-02],\n",
       "                        [ 6.1748e-03, -1.9616e-02,  2.3695e-02],\n",
       "                        [-2.8182e-02,  1.5925e-03,  2.8970e-02]],\n",
       "              \n",
       "                       [[-1.1483e-02, -3.8056e-02, -9.5976e-03],\n",
       "                        [ 8.5221e-03,  2.7333e-02,  1.1232e-02],\n",
       "                        [ 2.6113e-02,  1.0574e-02,  4.0188e-02]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 1.8971e-02,  8.3767e-03,  4.6899e-03],\n",
       "                        [ 1.3040e-04, -4.1920e-02, -1.4876e-03],\n",
       "                        [ 1.1923e-02,  2.6275e-02, -2.1965e-02]],\n",
       "              \n",
       "                       [[-1.2523e-02, -1.7132e-02,  2.7247e-02],\n",
       "                        [ 4.1886e-02,  2.2468e-02, -3.3416e-02],\n",
       "                        [-6.2060e-03,  2.0969e-02,  2.5442e-02]],\n",
       "              \n",
       "                       [[-2.7719e-02,  1.1698e-02,  2.6042e-02],\n",
       "                        [ 4.9902e-04, -3.0284e-02, -3.4233e-03],\n",
       "                        [-2.8542e-02, -7.3153e-03,  1.3729e-02]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 9.3472e-03, -4.8269e-02, -4.8437e-02],\n",
       "                        [ 2.9853e-02, -2.8804e-02,  2.0537e-03],\n",
       "                        [-9.3731e-03, -1.7123e-02, -3.2242e-02]],\n",
       "              \n",
       "                       [[-2.6663e-02, -2.9229e-02, -1.4745e-02],\n",
       "                        [-1.8580e-02,  1.1460e-02,  3.5004e-02],\n",
       "                        [-1.0994e-02, -1.3764e-02,  4.8864e-03]],\n",
       "              \n",
       "                       [[ 4.6454e-04,  3.1923e-02, -3.4723e-02],\n",
       "                        [ 3.1314e-02,  3.1216e-02, -3.1348e-02],\n",
       "                        [ 3.9242e-02, -1.2596e-03, -8.9703e-03]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 1.5595e-02,  1.3334e-02, -2.2912e-02],\n",
       "                        [-4.1346e-03, -1.0358e-02,  1.5183e-02],\n",
       "                        [ 1.0054e-02, -6.1172e-03, -2.6488e-02]],\n",
       "              \n",
       "                       [[ 2.1118e-03,  3.5196e-02,  1.9220e-02],\n",
       "                        [-1.5185e-02, -8.0745e-03, -4.6582e-03],\n",
       "                        [ 3.1082e-02, -2.0457e-02, -3.3443e-02]],\n",
       "              \n",
       "                       [[ 5.8924e-02,  1.7816e-02,  1.4810e-02],\n",
       "                        [-1.2410e-02, -3.3915e-02, -3.7698e-02],\n",
       "                        [ 4.3663e-02, -1.8148e-03,  6.0374e-03]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 1.0431e-02,  4.4988e-02, -4.6628e-03],\n",
       "                        [-1.2770e-02, -1.9644e-03, -2.8374e-02],\n",
       "                        [ 2.7623e-02, -3.9303e-04, -3.9519e-02]],\n",
       "              \n",
       "                       [[ 2.8182e-02, -5.5704e-03, -2.7398e-02],\n",
       "                        [ 1.8559e-02, -1.7923e-03, -1.4393e-02],\n",
       "                        [ 3.6477e-02, -6.7156e-03,  2.5404e-02]],\n",
       "              \n",
       "                       [[ 2.5658e-02,  1.9987e-02,  2.8298e-02],\n",
       "                        [-1.5334e-02,  2.6240e-03,  2.2125e-02],\n",
       "                        [-2.1868e-03, -1.1598e-02, -8.3110e-03]]]])),\n",
       "             ('module.encoder.net.6.bias',\n",
       "              tensor([-0.0171,  0.0068, -0.0408, -0.0129,  0.0211,  0.0223,  0.0162,  0.0238,\n",
       "                      -0.0218, -0.0345, -0.0177,  0.0196,  0.0214,  0.0342,  0.0374,  0.0120,\n",
       "                      -0.0410,  0.0281,  0.0332,  0.0113,  0.0095, -0.0376, -0.0064, -0.0156,\n",
       "                      -0.0043,  0.0211,  0.0250,  0.0362, -0.0262,  0.0039,  0.0358, -0.0079,\n",
       "                       0.0173, -0.0408, -0.0370,  0.0068, -0.0045, -0.0168, -0.0402, -0.0290,\n",
       "                      -0.0433, -0.0036, -0.0087, -0.0370, -0.0168, -0.0011,  0.0073, -0.0378,\n",
       "                       0.0168,  0.0125, -0.0246, -0.0014, -0.0333, -0.0312, -0.0250,  0.0233,\n",
       "                      -0.0175, -0.0352, -0.0323, -0.0252,  0.0369, -0.0053, -0.0417, -0.0115])),\n",
       "             ('module.encoder.net.7.weight',\n",
       "              tensor([0.9866, 1.0312, 1.0152, 1.0152, 0.9739, 1.0103, 0.9854, 0.9908, 1.0215,\n",
       "                      1.0205, 0.9665, 0.9984, 1.0128, 0.9921, 0.9923, 0.9838, 1.0090, 0.9771,\n",
       "                      0.9921, 1.0238, 0.9818, 1.0588, 0.9539, 0.9950, 0.9875, 1.0204, 0.9912,\n",
       "                      1.0050, 1.0165, 0.9893, 0.9953, 0.9988, 0.9986, 1.0293, 1.0014, 1.0383,\n",
       "                      1.0150, 1.0151, 0.9847, 0.9953, 1.0151, 1.0186, 1.0154, 0.9969, 1.0059,\n",
       "                      0.9999, 0.9987, 1.0017, 0.9915, 1.0169, 0.9705, 1.0001, 1.0198, 0.9931,\n",
       "                      1.0210, 1.0001, 1.0205, 0.9953, 1.0026, 0.9824, 0.9747, 1.0157, 1.0005,\n",
       "                      0.9908])),\n",
       "             ('module.encoder.net.7.bias',\n",
       "              tensor([ 0.0150,  0.0319,  0.0350,  0.0288,  0.0199,  0.0228,  0.0037, -0.0023,\n",
       "                       0.0272,  0.0471, -0.0297,  0.0084,  0.0358,  0.0178,  0.0027,  0.0069,\n",
       "                       0.0106, -0.0057,  0.0356,  0.0049,  0.0233,  0.0453,  0.0033,  0.0548,\n",
       "                       0.0317,  0.1069,  0.0138,  0.0395,  0.0391, -0.0021,  0.0303, -0.0093,\n",
       "                       0.0438,  0.0199,  0.0177,  0.0381,  0.0381,  0.0434, -0.0066,  0.0047,\n",
       "                       0.0478,  0.0479,  0.0429,  0.0241,  0.0186,  0.0039,  0.0677,  0.0317,\n",
       "                       0.0068,  0.0250,  0.0228,  0.0544,  0.0226,  0.0481,  0.0381,  0.0244,\n",
       "                       0.0327,  0.0367,  0.0322,  0.0021,  0.0154,  0.0197,  0.0119,  0.0070])),\n",
       "             ('module.encoder.net.7.running_mean',\n",
       "              tensor([ 0.7731, -1.2117,  1.2399, -0.3603,  0.4885,  1.0604, -0.3466, -0.8015,\n",
       "                      -0.8086,  1.5267,  0.4957, -0.4608, -0.4171,  1.4272,  0.3159, -0.3671,\n",
       "                      -0.5332,  0.7774,  1.3416, -0.6515,  1.2348, -0.6176, -0.0713,  1.4991,\n",
       "                       1.0853,  1.3622, -0.5942, -0.8773, -0.7347, -0.5075,  1.0194, -0.2814,\n",
       "                       1.3270, -1.0617, -0.7836, -0.9134, -0.5555,  1.3411, -0.4885, -0.7810,\n",
       "                       0.7708,  0.9576,  1.1776,  1.1792, -0.6570, -0.7984,  0.9980,  1.3570,\n",
       "                      -0.8025, -0.6067,  1.0248,  1.5439, -0.6361,  1.1221,  1.1310, -0.1584,\n",
       "                      -0.6373,  0.2485,  1.3867,  0.5611,  0.8624, -0.3334, -0.7217,  1.1548])),\n",
       "             ('module.encoder.net.7.running_var',\n",
       "              tensor([0.8320, 0.7771, 1.2784, 0.9927, 0.3565, 1.6866, 1.1056, 0.4980, 0.7099,\n",
       "                      1.4829, 0.8699, 0.9683, 1.2895, 1.1815, 0.5555, 0.5412, 2.0802, 1.8480,\n",
       "                      0.9541, 0.7158, 0.7961, 0.6715, 0.5435, 0.8364, 2.1184, 1.1976, 0.9512,\n",
       "                      0.7749, 0.7638, 0.6549, 0.7918, 0.6414, 1.2440, 0.6994, 0.6980, 0.4507,\n",
       "                      0.4827, 1.9939, 0.6598, 0.6809, 1.0473, 0.7884, 1.1639, 1.1842, 0.5215,\n",
       "                      0.8723, 0.9271, 1.5678, 0.8429, 2.1452, 0.7121, 1.1472, 0.6543, 1.6096,\n",
       "                      1.1786, 0.8482, 0.6487, 0.8431, 1.1390, 0.9945, 0.8618, 0.8143, 0.8528,\n",
       "                      1.2239])),\n",
       "             ('module.encoder.net.7.num_batches_tracked', tensor(0)),\n",
       "             ('module.encoder.net.9.weight',\n",
       "              tensor([[[[ 2.8775e-02,  3.2289e-02,  8.9009e-03],\n",
       "                        [ 3.3892e-02,  5.3125e-03, -2.8465e-02],\n",
       "                        [-9.5763e-03,  1.9904e-02, -2.5187e-02]],\n",
       "              \n",
       "                       [[-1.7624e-02, -3.7212e-03, -1.1353e-02],\n",
       "                        [-2.2411e-02, -4.7173e-02,  2.8981e-02],\n",
       "                        [-3.2582e-02,  7.3780e-03,  3.4643e-03]],\n",
       "              \n",
       "                       [[ 7.2329e-03,  3.7520e-02, -2.6042e-02],\n",
       "                        [ 6.2077e-03, -5.6502e-03, -4.3230e-02],\n",
       "                        [ 3.5876e-02, -8.0056e-03, -1.6720e-02]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 2.8091e-02,  2.9245e-02,  3.3565e-03],\n",
       "                        [ 2.2829e-02, -3.7380e-02,  6.2794e-04],\n",
       "                        [-2.4542e-02,  9.3717e-03,  2.5862e-02]],\n",
       "              \n",
       "                       [[-2.6377e-02,  6.0469e-03,  2.4200e-02],\n",
       "                        [ 2.3289e-02, -2.5095e-02, -3.6224e-03],\n",
       "                        [ 1.5277e-02,  6.0812e-03,  8.3493e-03]],\n",
       "              \n",
       "                       [[ 1.7508e-02,  1.0729e-02,  8.3477e-03],\n",
       "                        [ 4.6449e-02,  8.9776e-03, -3.1335e-02],\n",
       "                        [-2.4699e-02, -4.9947e-04,  1.9754e-02]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 1.0894e-02, -4.8827e-03,  3.0318e-02],\n",
       "                        [ 1.0200e-02,  9.3003e-03, -1.2158e-02],\n",
       "                        [ 3.7928e-03,  2.6626e-02, -3.5117e-02]],\n",
       "              \n",
       "                       [[-1.3557e-02,  4.2017e-02,  2.1266e-02],\n",
       "                        [ 2.0017e-02,  2.7025e-02, -2.5030e-02],\n",
       "                        [ 2.0521e-02, -3.7342e-02,  1.8067e-02]],\n",
       "              \n",
       "                       [[ 2.2045e-02, -6.9101e-03,  1.3915e-02],\n",
       "                        [ 3.9085e-02, -2.0887e-02, -3.7198e-02],\n",
       "                        [-1.5696e-02,  3.8094e-03, -3.6675e-02]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 7.1390e-03,  7.1276e-03,  1.7840e-02],\n",
       "                        [ 8.8015e-03, -2.1957e-02,  4.7432e-03],\n",
       "                        [-2.1753e-02, -4.6927e-03,  1.8200e-02]],\n",
       "              \n",
       "                       [[-6.9086e-03, -2.3520e-02, -2.5983e-02],\n",
       "                        [-2.1445e-02,  2.3913e-02,  4.0157e-02],\n",
       "                        [-9.9444e-03, -2.5422e-03,  4.7124e-02]],\n",
       "              \n",
       "                       [[-2.6308e-02,  3.0520e-02,  3.4504e-02],\n",
       "                        [ 4.5276e-02,  1.3461e-02, -7.7417e-03],\n",
       "                        [ 2.5921e-02,  2.0806e-02, -2.7775e-02]]],\n",
       "              \n",
       "              \n",
       "                      [[[-3.4502e-04,  4.0327e-02, -1.8585e-02],\n",
       "                        [-3.5871e-03,  5.1055e-02, -2.9963e-02],\n",
       "                        [ 4.0068e-02, -1.7263e-02, -2.4047e-02]],\n",
       "              \n",
       "                       [[ 1.2914e-02, -3.0969e-04, -3.9973e-03],\n",
       "                        [-3.9515e-02, -1.8008e-03, -1.0095e-02],\n",
       "                        [ 4.2590e-02, -4.0056e-02,  6.1244e-03]],\n",
       "              \n",
       "                       [[ 1.0332e-02, -1.4873e-03, -4.0363e-02],\n",
       "                        [ 3.7446e-02,  9.7038e-03,  2.9486e-04],\n",
       "                        [-3.3440e-02, -2.7803e-02,  1.1923e-02]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 1.4343e-02, -3.4702e-02,  3.0878e-02],\n",
       "                        [-4.0612e-02, -3.8353e-02,  1.7819e-02],\n",
       "                        [ 1.8596e-02, -3.5529e-03,  9.1306e-03]],\n",
       "              \n",
       "                       [[ 9.0020e-03,  3.1460e-02,  2.0650e-02],\n",
       "                        [-1.9972e-02,  1.3991e-02, -2.1686e-02],\n",
       "                        [-3.4989e-03,  3.3487e-02,  5.7223e-02]],\n",
       "              \n",
       "                       [[ 1.0584e-02,  7.8540e-03,  2.7868e-02],\n",
       "                        [ 2.7970e-02,  2.8152e-03, -4.1016e-02],\n",
       "                        [ 1.3685e-02, -5.3728e-02, -1.6311e-02]]],\n",
       "              \n",
       "              \n",
       "                      ...,\n",
       "              \n",
       "              \n",
       "                      [[[-1.7115e-03,  2.3622e-02, -5.5959e-04],\n",
       "                        [ 2.5601e-02,  1.9447e-02,  5.6948e-03],\n",
       "                        [-1.7906e-02,  2.6202e-02, -6.7835e-04]],\n",
       "              \n",
       "                       [[-1.4657e-02, -1.6489e-02,  1.1739e-02],\n",
       "                        [-3.4417e-02,  1.2480e-02,  1.1695e-02],\n",
       "                        [-3.6527e-02, -7.7342e-04, -1.9175e-02]],\n",
       "              \n",
       "                       [[-2.3317e-02, -5.1969e-02,  1.1183e-02],\n",
       "                        [-2.5908e-02,  1.9695e-02,  1.8087e-02],\n",
       "                        [-2.8872e-02, -7.5040e-03,  1.8610e-02]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-3.0701e-02, -1.8779e-02, -6.7772e-04],\n",
       "                        [ 3.7071e-02,  3.0015e-02, -2.6206e-02],\n",
       "                        [-3.9974e-02, -1.1292e-02, -2.6545e-02]],\n",
       "              \n",
       "                       [[-1.0533e-03,  4.5381e-03,  2.8470e-03],\n",
       "                        [ 3.0636e-02, -1.3075e-02, -2.1177e-02],\n",
       "                        [-1.4211e-03,  2.1108e-03, -5.0004e-02]],\n",
       "              \n",
       "                       [[ 1.9559e-02, -3.8785e-02, -1.8676e-02],\n",
       "                        [-2.3612e-02,  7.0915e-03,  1.3672e-02],\n",
       "                        [-2.7634e-02, -1.1029e-02,  1.6849e-02]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 8.1296e-06, -2.7242e-02,  1.0959e-02],\n",
       "                        [ 3.2075e-02,  1.4044e-02,  3.0460e-02],\n",
       "                        [-2.3514e-02,  2.6889e-02, -1.3003e-02]],\n",
       "              \n",
       "                       [[-3.3938e-02, -6.5948e-03,  2.5984e-02],\n",
       "                        [-1.5349e-02,  3.6189e-02,  2.9421e-02],\n",
       "                        [ 1.4879e-03,  2.1860e-02, -4.3606e-02]],\n",
       "              \n",
       "                       [[-3.7224e-02, -1.9601e-02,  1.1994e-02],\n",
       "                        [ 3.0728e-02,  2.7798e-02,  2.7070e-03],\n",
       "                        [ 1.5528e-02,  1.8650e-02,  3.8125e-02]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 2.3475e-02,  1.0713e-02, -1.7210e-02],\n",
       "                        [ 2.0505e-02, -1.3624e-02,  2.5684e-02],\n",
       "                        [ 2.0517e-02, -2.6761e-04,  6.6411e-03]],\n",
       "              \n",
       "                       [[ 5.1207e-02,  4.3733e-05, -3.4620e-02],\n",
       "                        [-1.1537e-02,  3.5554e-02, -1.2818e-02],\n",
       "                        [ 2.5399e-02, -2.3057e-02,  1.8964e-02]],\n",
       "              \n",
       "                       [[ 1.1868e-02,  2.6557e-02,  2.2095e-02],\n",
       "                        [ 2.5826e-03, -1.5932e-02,  4.3271e-03],\n",
       "                        [ 2.2548e-02,  2.4338e-02, -1.0578e-02]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 1.1080e-02,  6.0809e-03,  8.1043e-03],\n",
       "                        [ 1.0633e-03,  1.2925e-02,  3.7778e-02],\n",
       "                        [ 3.7629e-03, -4.1086e-03, -1.6074e-02]],\n",
       "              \n",
       "                       [[ 3.4119e-02,  1.0342e-02,  3.0726e-02],\n",
       "                        [ 1.6627e-02, -3.4956e-02, -4.4753e-02],\n",
       "                        [-2.8306e-02, -7.8222e-03,  2.8623e-02]],\n",
       "              \n",
       "                       [[ 2.4020e-02, -7.0145e-03, -4.6940e-02],\n",
       "                        [-2.6681e-02, -4.0383e-02,  2.6039e-02],\n",
       "                        [ 2.9702e-02,  1.8755e-02,  2.5530e-02]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 3.5686e-02,  2.6232e-02, -5.1410e-03],\n",
       "                        [ 2.1926e-02, -8.7397e-03, -3.7342e-02],\n",
       "                        [-3.1427e-02,  3.1091e-02,  1.7682e-02]],\n",
       "              \n",
       "                       [[ 2.5717e-02, -1.2786e-02, -5.6393e-03],\n",
       "                        [-3.1166e-02, -4.2689e-02,  1.5165e-02],\n",
       "                        [ 4.3712e-02, -7.3231e-03,  2.8494e-02]],\n",
       "              \n",
       "                       [[ 2.9559e-02,  1.8097e-02, -3.9207e-02],\n",
       "                        [ 2.0400e-02,  2.3981e-02,  1.9366e-02],\n",
       "                        [ 3.5576e-02,  4.5887e-02,  1.9424e-02]]]])),\n",
       "             ('module.encoder.net.9.bias',\n",
       "              tensor([-0.0087, -0.0386,  0.0067, -0.0279,  0.0193, -0.0449, -0.0049, -0.0021,\n",
       "                      -0.0258, -0.0179, -0.0047,  0.0185,  0.0156, -0.0394, -0.0118, -0.0375,\n",
       "                       0.0236, -0.0142, -0.0194,  0.0230, -0.0095, -0.0216,  0.0248,  0.0240,\n",
       "                       0.0356, -0.0103,  0.0119,  0.0346,  0.0287,  0.0347, -0.0451,  0.0426,\n",
       "                       0.0130,  0.0321,  0.0470,  0.0258, -0.0342, -0.0013,  0.0357,  0.0303,\n",
       "                       0.0445,  0.0376,  0.0329, -0.0263,  0.0367, -0.0071,  0.0163, -0.0225,\n",
       "                      -0.0382,  0.0338,  0.0109, -0.0304,  0.0125,  0.0102, -0.0299,  0.0426,\n",
       "                      -0.0442, -0.0119, -0.0312,  0.0274, -0.0373,  0.0428, -0.0305, -0.0252])),\n",
       "             ('module.encoder.net.10.weight',\n",
       "              tensor([0.9837, 0.9923, 0.9982, 0.9899, 1.0025, 1.0040, 0.9900, 0.9863, 0.9960,\n",
       "                      0.9974, 0.9838, 0.9865, 0.9738, 0.9765, 0.9972, 0.9987, 0.9835, 0.9946,\n",
       "                      0.9833, 0.9932, 0.9863, 0.9988, 0.9832, 0.9840, 0.9892, 0.9884, 0.9884,\n",
       "                      0.9799, 1.0045, 0.9947, 0.9902, 0.9841, 0.9865, 0.9849, 0.9907, 1.0086,\n",
       "                      0.9795, 0.9904, 0.9986, 0.9871, 1.0327, 1.0042, 0.9724, 0.9879, 0.9837,\n",
       "                      1.0076, 1.0082, 1.0159, 0.9800, 1.0024, 1.0073, 0.9919, 0.9901, 0.9960,\n",
       "                      0.9975, 0.9955, 1.0041, 0.9970, 0.9946, 0.9912, 1.0062, 0.9859, 0.9816,\n",
       "                      0.9948])),\n",
       "             ('module.encoder.net.10.bias',\n",
       "              tensor([ 0.0022,  0.0183,  0.0081, -0.0115, -0.0044,  0.0110, -0.0067, -0.0107,\n",
       "                       0.0180,  0.0044,  0.0033, -0.0069, -0.0128, -0.0056,  0.0275, -0.0028,\n",
       "                      -0.0169,  0.0215,  0.0025, -0.0078, -0.0268, -0.0060, -0.0234, -0.0053,\n",
       "                       0.0125, -0.0047,  0.0060, -0.0003,  0.0090, -0.0035,  0.0052, -0.0138,\n",
       "                      -0.0026, -0.0201, -0.0004, -0.0042, -0.0019,  0.0178, -0.0181, -0.0219,\n",
       "                       0.0097, -0.0126, -0.0091, -0.0020, -0.0171, -0.0037, -0.0092, -0.0096,\n",
       "                      -0.0170, -0.0150,  0.0271, -0.0008, -0.0163, -0.0029,  0.0047,  0.0081,\n",
       "                      -0.0007,  0.0374,  0.0134, -0.0033, -0.0024,  0.0061, -0.0064,  0.0128])),\n",
       "             ('module.encoder.net.10.running_mean',\n",
       "              tensor([ 0.6067,  0.5918,  0.5890, -0.5612, -0.9514, -0.4275, -0.7567, -0.7873,\n",
       "                      -0.7452,  0.4145,  0.5916, -0.6619, -0.3423,  0.5501,  0.4930, -0.6787,\n",
       "                      -0.7166,  0.9054,  0.3686, -0.5500, -0.5506, -0.4213, -0.6130, -0.2184,\n",
       "                      -0.3588, -0.1248, -0.3594, -0.5894, -0.5618, -0.4800,  0.0976, -0.7085,\n",
       "                      -0.3141, -0.3724, -0.5604, -0.1330, -0.5574, -0.7970, -0.4487, -0.8548,\n",
       "                      -0.6738, -0.7145, -0.6781, -0.6736, -0.7961, -0.3533, -0.4159, -0.3155,\n",
       "                      -0.3757, -0.3288,  0.6591, -0.6306, -0.6374, -0.8051, -0.8910, -0.5585,\n",
       "                      -0.1906,  0.7334,  0.3058, -0.7493, -1.0605, -0.7237, -0.5352,  0.7313])),\n",
       "             ('module.encoder.net.10.running_var',\n",
       "              tensor([ 4.1394,  4.8753,  5.3948,  3.2927,  3.7360,  4.6818,  3.3398,  4.6512,\n",
       "                       6.9752,  7.0799,  3.5498,  4.3558,  4.6601,  3.0326,  5.9265,  6.9123,\n",
       "                       4.2075,  4.1767,  4.7462,  5.7205,  3.7364,  5.1382,  5.9339,  4.9635,\n",
       "                       5.0581,  3.8080,  6.9971,  4.2673,  4.4643,  8.4175,  5.1286,  3.7809,\n",
       "                       4.4347,  3.8024,  4.9809,  6.9810,  3.4528,  7.0445,  7.0478,  4.7475,\n",
       "                       8.2163,  4.0086,  4.3529,  4.9970,  4.0572,  7.8957,  4.3216,  5.6180,\n",
       "                       3.4018,  5.9676,  6.3159,  5.1869,  3.1992,  5.1559,  6.1554, 12.7297,\n",
       "                       6.5535, 24.2174,  4.8226,  4.6734,  4.4423,  6.3735,  6.8938,  6.9703])),\n",
       "             ('module.encoder.net.10.num_batches_tracked', tensor(0)),\n",
       "             ('module.encoder.h1.weight',\n",
       "              tensor([[-0.0164, -0.0385,  0.0377,  ..., -0.0682, -0.0609,  0.0636],\n",
       "                      [-0.0427,  0.0247, -0.0067,  ..., -0.0081,  0.0567,  0.0285],\n",
       "                      [ 0.0017,  0.0214,  0.0733,  ..., -0.0449, -0.0605, -0.0407],\n",
       "                      ...,\n",
       "                      [ 0.0206,  0.0221,  0.0384,  ...,  0.0300,  0.0087,  0.0389],\n",
       "                      [-0.0512,  0.0276,  0.0661,  ..., -0.0227, -0.0623, -0.0338],\n",
       "                      [ 0.0320,  0.0282, -0.0178,  ...,  0.0215, -0.0499, -0.0178]])),\n",
       "             ('module.encoder.h1.bias',\n",
       "              tensor([-0.0115, -0.0099, -0.0604, -0.0082,  0.0115,  0.0384, -0.0008, -0.0051,\n",
       "                      -0.0271,  0.0129, -0.0285,  0.0202, -0.0229, -0.0434, -0.0065, -0.0308,\n",
       "                      -0.0183,  0.0318,  0.0050, -0.0309, -0.0216, -0.0037, -0.0246, -0.0302,\n",
       "                      -0.0489,  0.0128, -0.0203,  0.0075, -0.0171, -0.0286,  0.0180, -0.0162,\n",
       "                      -0.0195, -0.0028, -0.0025,  0.0196, -0.0182, -0.0347,  0.0362, -0.0457,\n",
       "                      -0.0277, -0.0001,  0.0143, -0.0054,  0.0226, -0.0433,  0.0266,  0.0396,\n",
       "                      -0.0435,  0.0466,  0.0327, -0.0443, -0.0221, -0.0534,  0.0321, -0.0192,\n",
       "                       0.0147, -0.0352,  0.0107, -0.0276, -0.0019,  0.0016,  0.0434,  0.0572])),\n",
       "             ('module.encoder.h2.weight',\n",
       "              tensor([[-0.0576, -0.0414,  0.0067,  ...,  0.0289,  0.0152, -0.0118],\n",
       "                      [ 0.0460,  0.0485, -0.0568,  ..., -0.0609,  0.0099,  0.0339],\n",
       "                      [ 0.0449, -0.0125,  0.0420,  ...,  0.0181, -0.0329,  0.0328],\n",
       "                      ...,\n",
       "                      [ 0.0121,  0.0253,  0.0231,  ...,  0.0217, -0.0278,  0.0157],\n",
       "                      [-0.0119, -0.0602, -0.0260,  ..., -0.0023, -0.0785,  0.0082],\n",
       "                      [-0.0424,  0.0478,  0.0060,  ..., -0.0290, -0.0095,  0.0060]])),\n",
       "             ('module.encoder.h2.bias',\n",
       "              tensor([-0.0258, -0.0190, -0.0440,  0.0060, -0.0422,  0.0454,  0.0521,  0.0131,\n",
       "                       0.0277, -0.0329, -0.0485,  0.0077, -0.0187,  0.0343,  0.0366, -0.0369,\n",
       "                      -0.0342,  0.0276, -0.0156, -0.0358, -0.0251,  0.0163,  0.0215, -0.0492,\n",
       "                       0.0208, -0.0438, -0.0367,  0.0127, -0.0400,  0.0576, -0.0235,  0.0403,\n",
       "                       0.0168, -0.0426, -0.0249, -0.0512,  0.0415, -0.0515,  0.0433, -0.0337,\n",
       "                       0.0109,  0.0316, -0.0111,  0.0330, -0.0427,  0.0596,  0.0210,  0.0305,\n",
       "                       0.0441, -0.0171, -0.0324,  0.0006, -0.0130, -0.0410,  0.0343, -0.0329,\n",
       "                       0.0269,  0.0376, -0.0225, -0.0349, -0.0192,  0.0647,  0.0139,  0.0657])),\n",
       "             ('module.decoder.linear.0.weight',\n",
       "              tensor([[-0.0457,  0.0642, -0.0232,  ..., -0.0486, -0.1019,  0.0375],\n",
       "                      [ 0.0214,  0.0489,  0.0535,  ...,  0.0890,  0.0316, -0.0004],\n",
       "                      [ 0.0560, -0.0645,  0.0546,  ...,  0.0341, -0.0817,  0.0466],\n",
       "                      ...,\n",
       "                      [-0.0455,  0.0443, -0.0604,  ..., -0.0100,  0.0871, -0.0161],\n",
       "                      [ 0.0062, -0.0108,  0.0495,  ...,  0.0771,  0.0676,  0.0323],\n",
       "                      [-0.0925,  0.0548,  0.0555,  ...,  0.0395, -0.0866, -0.0830]])),\n",
       "             ('module.decoder.linear.0.bias',\n",
       "              tensor([ 0.1576,  0.1175,  0.0288,  0.0124,  0.0214, -0.0345,  0.0734,  0.1571,\n",
       "                      -0.0442, -0.0477, -0.1103,  0.1218,  0.0372, -0.0331,  0.0835,  0.0663,\n",
       "                       0.0346, -0.0950,  0.0789,  0.0105, -0.0613, -0.0271,  0.1667,  0.0774,\n",
       "                       0.0723,  0.0843,  0.0273,  0.1128,  0.0610, -0.1182, -0.0827,  0.0719,\n",
       "                       0.0174,  0.0979,  0.1024, -0.0878,  0.0332,  0.0059, -0.0040,  0.0445,\n",
       "                       0.0781,  0.0422, -0.0663, -0.0798,  0.0131, -0.0453,  0.0563,  0.0091,\n",
       "                      -0.0355,  0.0126,  0.0756, -0.1259, -0.0556,  0.0720,  0.0609,  0.1143,\n",
       "                      -0.0036,  0.0610,  0.1045,  0.0806, -0.0478,  0.1423,  0.1162, -0.0982,\n",
       "                       0.0603, -0.0193,  0.0034, -0.0221,  0.0753, -0.0269,  0.0131,  0.0841,\n",
       "                       0.1064, -0.0477,  0.1154,  0.0129,  0.0616,  0.0141,  0.0527,  0.0924,\n",
       "                      -0.0684,  0.0424,  0.1228, -0.0594,  0.0058,  0.0602, -0.0257, -0.0733,\n",
       "                       0.0955, -0.1220, -0.0421,  0.1040,  0.0830,  0.1046,  0.0514,  0.0090,\n",
       "                      -0.0113,  0.0516, -0.1023,  0.0600,  0.0656, -0.0281,  0.0982, -0.0498,\n",
       "                       0.0789,  0.0429,  0.0485,  0.0585,  0.0353, -0.0402,  0.1007,  0.0604,\n",
       "                       0.0583,  0.0899, -0.0735,  0.0607,  0.0626,  0.0594,  0.1219,  0.0329,\n",
       "                       0.0468,  0.1310,  0.0886, -0.0170,  0.0071,  0.0771,  0.0646, -0.0830,\n",
       "                       0.0969,  0.0504,  0.0229, -0.0924,  0.0463, -0.0063,  0.0749,  0.1060,\n",
       "                       0.0612, -0.0484,  0.0954,  0.0247, -0.0040, -0.0331,  0.1009,  0.0544,\n",
       "                       0.0141,  0.1313, -0.0009,  0.0837,  0.1152,  0.0062,  0.1049,  0.1072,\n",
       "                       0.1139, -0.0934, -0.0850, -0.0047, -0.0487,  0.0191,  0.0398, -0.1251,\n",
       "                       0.0248,  0.0616,  0.0581,  0.0996, -0.0017, -0.0729,  0.0855,  0.1211,\n",
       "                       0.0362, -0.0031,  0.0280,  0.0347, -0.0828,  0.0526, -0.0540,  0.0706,\n",
       "                       0.1387,  0.0130,  0.0552,  0.0410, -0.0269, -0.0251,  0.0733,  0.0445,\n",
       "                       0.0691,  0.0824, -0.0710,  0.0927,  0.1570,  0.1312, -0.0613, -0.0714,\n",
       "                      -0.0617,  0.0912,  0.0078, -0.0461, -0.0419,  0.1575,  0.0757,  0.0274,\n",
       "                       0.0565,  0.0575,  0.0810, -0.0078, -0.0342,  0.0022,  0.1147, -0.0192,\n",
       "                      -0.0038, -0.0525, -0.0312, -0.0538, -0.0773, -0.0040,  0.0130, -0.0090,\n",
       "                       0.0399, -0.0932,  0.1033, -0.1252, -0.0500, -0.0010, -0.0090,  0.0793,\n",
       "                      -0.0480, -0.1229,  0.0553,  0.1108, -0.0556,  0.0906,  0.0085,  0.0695,\n",
       "                      -0.0747, -0.1325,  0.0706,  0.0810, -0.0250, -0.0267, -0.0282,  0.0116,\n",
       "                       0.0340,  0.0142,  0.0205,  0.0553,  0.1071,  0.0652,  0.0547,  0.0389,\n",
       "                       0.0223,  0.1014,  0.0817,  0.0418,  0.1360, -0.0472, -0.0649,  0.0802])),\n",
       "             ('module.decoder.net.1.weight',\n",
       "              tensor([[[[ 3.4523e-02, -9.9705e-03,  3.5472e-02],\n",
       "                        [-6.5575e-02,  3.1363e-02,  6.3586e-03],\n",
       "                        [-4.2482e-02, -1.5903e-02,  2.0620e-02]],\n",
       "              \n",
       "                       [[ 1.3794e-02,  4.8657e-02,  4.6650e-02],\n",
       "                        [-2.6840e-02,  2.3613e-03, -1.2028e-02],\n",
       "                        [-1.1596e-02, -2.8653e-03, -3.6741e-02]],\n",
       "              \n",
       "                       [[ 5.0146e-02, -2.6739e-02,  2.9757e-02],\n",
       "                        [ 1.3674e-02, -3.3778e-02, -1.3625e-02],\n",
       "                        [-4.2959e-02, -5.7351e-03, -2.9578e-02]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-3.8250e-02,  1.1261e-02, -1.1411e-02],\n",
       "                        [ 2.5217e-02, -2.3404e-02, -1.1588e-02],\n",
       "                        [ 2.9508e-02, -4.2398e-02, -4.0919e-02]],\n",
       "              \n",
       "                       [[ 6.8206e-03,  7.9429e-03,  4.1251e-02],\n",
       "                        [ 3.5561e-02,  4.3251e-02, -1.9995e-02],\n",
       "                        [ 2.6414e-02,  2.7364e-02, -4.2435e-03]],\n",
       "              \n",
       "                       [[ 3.6541e-02, -2.8985e-02, -1.2412e-02],\n",
       "                        [-3.1765e-02, -5.4651e-03,  3.1369e-02],\n",
       "                        [-6.7292e-03, -3.0088e-02, -9.0536e-03]]],\n",
       "              \n",
       "              \n",
       "                      [[[-3.3946e-02,  3.5780e-02,  7.8340e-03],\n",
       "                        [-2.0098e-02, -2.7570e-02, -3.9041e-02],\n",
       "                        [-2.4780e-02, -3.4976e-02, -4.8000e-03]],\n",
       "              \n",
       "                       [[-2.2794e-02, -2.8774e-02, -3.0111e-02],\n",
       "                        [-3.6048e-03, -5.9028e-03, -4.7184e-02],\n",
       "                        [-2.2569e-02,  1.1756e-02, -1.8651e-02]],\n",
       "              \n",
       "                       [[-1.8964e-02,  5.2054e-03,  7.0036e-03],\n",
       "                        [ 2.2333e-02,  3.8569e-02, -1.8273e-02],\n",
       "                        [ 2.0825e-02,  1.8398e-02, -1.2199e-02]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-1.1563e-02, -2.7815e-02, -6.8720e-03],\n",
       "                        [ 1.7921e-03,  7.1816e-03,  4.5710e-02],\n",
       "                        [ 5.9728e-03, -2.4319e-03, -2.7865e-02]],\n",
       "              \n",
       "                       [[-3.1017e-02,  8.2703e-03,  1.4799e-02],\n",
       "                        [ 3.5517e-02, -6.9534e-03,  3.4124e-03],\n",
       "                        [ 2.0720e-02, -1.9477e-02, -2.0087e-02]],\n",
       "              \n",
       "                       [[-3.4920e-02,  2.4966e-02, -2.3249e-02],\n",
       "                        [-2.9150e-02, -8.2036e-03,  1.1971e-02],\n",
       "                        [ 1.4975e-02, -2.7239e-02, -3.5947e-02]]],\n",
       "              \n",
       "              \n",
       "                      [[[-1.0651e-03,  3.8346e-02, -6.9434e-03],\n",
       "                        [-4.4403e-02, -1.2547e-02,  3.7194e-02],\n",
       "                        [ 5.6434e-03, -3.2855e-02,  8.8637e-03]],\n",
       "              \n",
       "                       [[ 4.5131e-02, -5.2523e-02,  3.2712e-02],\n",
       "                        [ 3.7725e-02, -3.1229e-03,  1.4130e-02],\n",
       "                        [ 4.3543e-02,  2.6064e-02,  3.3913e-02]],\n",
       "              \n",
       "                       [[-1.4074e-02, -4.2769e-02,  3.0729e-02],\n",
       "                        [-2.2112e-02, -4.6586e-02, -1.1797e-02],\n",
       "                        [-1.7732e-03,  3.3596e-02,  6.6807e-02]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-7.0363e-03,  2.6884e-02,  4.4185e-02],\n",
       "                        [-6.6648e-03,  1.2831e-02, -5.8055e-03],\n",
       "                        [ 5.1317e-02, -4.0863e-02,  5.5566e-03]],\n",
       "              \n",
       "                       [[ 3.1754e-02, -3.0128e-02, -1.3757e-02],\n",
       "                        [-9.2029e-04, -1.3676e-02,  7.0326e-03],\n",
       "                        [ 3.2194e-02,  3.6447e-02, -1.7748e-04]],\n",
       "              \n",
       "                       [[ 5.5734e-02,  6.9963e-03,  3.8267e-02],\n",
       "                        [-3.3100e-02, -7.2912e-02,  5.8175e-03],\n",
       "                        [ 1.9732e-02, -3.5927e-02,  1.9700e-02]]],\n",
       "              \n",
       "              \n",
       "                      ...,\n",
       "              \n",
       "              \n",
       "                      [[[ 1.2213e-03, -1.3460e-02, -4.6948e-03],\n",
       "                        [ 1.3788e-02, -4.1904e-02, -3.2379e-02],\n",
       "                        [ 2.4313e-02,  3.4344e-02,  7.0290e-03]],\n",
       "              \n",
       "                       [[-7.2872e-03,  2.9656e-02,  2.8753e-02],\n",
       "                        [-4.2681e-02,  2.9060e-02,  8.6520e-03],\n",
       "                        [-7.4703e-06,  2.6602e-02,  4.7398e-02]],\n",
       "              \n",
       "                       [[-5.1753e-03,  1.7595e-02,  9.7381e-03],\n",
       "                        [-4.8499e-02,  8.2215e-03, -3.1343e-02],\n",
       "                        [-3.0721e-02,  3.0122e-02, -2.7695e-02]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-6.8380e-03, -3.2461e-02, -2.2469e-03],\n",
       "                        [ 1.1446e-02, -3.0790e-02,  5.4446e-03],\n",
       "                        [-2.2110e-02, -1.0674e-02, -4.3542e-02]],\n",
       "              \n",
       "                       [[ 1.5819e-02, -8.4038e-03,  2.6544e-02],\n",
       "                        [ 2.8141e-02,  9.4491e-03,  3.6666e-02],\n",
       "                        [-1.7722e-02,  3.5260e-02,  2.7961e-02]],\n",
       "              \n",
       "                       [[-2.9601e-02,  1.9070e-02,  3.3559e-02],\n",
       "                        [ 1.9029e-02, -2.7188e-02,  6.5712e-03],\n",
       "                        [-4.9468e-03,  4.1413e-03,  4.1812e-02]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 1.5970e-02,  2.7033e-02,  2.6748e-02],\n",
       "                        [-2.6749e-02, -2.2809e-02,  2.8598e-02],\n",
       "                        [ 2.0191e-03,  5.9275e-02,  2.9322e-02]],\n",
       "              \n",
       "                       [[-3.7463e-02,  5.0399e-02,  2.8771e-03],\n",
       "                        [ 2.5572e-02,  1.1513e-02,  3.4678e-02],\n",
       "                        [-7.4528e-03,  1.1921e-03, -1.1987e-05]],\n",
       "              \n",
       "                       [[-2.7118e-02, -1.4546e-02, -2.7362e-02],\n",
       "                        [-3.0194e-02, -1.0510e-02,  1.0052e-02],\n",
       "                        [ 2.6945e-02,  9.7365e-03,  5.1038e-02]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 9.9432e-03, -7.1542e-03, -2.4729e-02],\n",
       "                        [ 4.9789e-02, -3.2431e-02,  1.5378e-02],\n",
       "                        [-1.0546e-02,  1.0647e-02, -2.6541e-02]],\n",
       "              \n",
       "                       [[ 1.8242e-02,  5.0070e-03, -2.7687e-02],\n",
       "                        [ 2.4156e-02,  3.6073e-02, -1.5701e-02],\n",
       "                        [ 3.4127e-02, -7.6035e-03,  3.3759e-02]],\n",
       "              \n",
       "                       [[-1.7858e-02,  2.7159e-02, -3.1511e-02],\n",
       "                        [-6.4252e-03, -1.4576e-02, -4.4880e-02],\n",
       "                        [-3.0279e-02,  3.1399e-02,  3.5090e-02]]],\n",
       "              \n",
       "              \n",
       "                      [[[-3.5055e-03,  3.4061e-02,  2.8318e-02],\n",
       "                        [ 3.1796e-03, -3.6350e-02, -3.6789e-02],\n",
       "                        [ 3.3851e-02,  3.8866e-02,  3.5512e-02]],\n",
       "              \n",
       "                       [[-1.4121e-02, -2.8256e-02, -2.6588e-02],\n",
       "                        [-3.1777e-02, -4.0612e-02, -3.7065e-03],\n",
       "                        [ 3.7422e-02, -2.9547e-03,  5.0356e-02]],\n",
       "              \n",
       "                       [[ 3.9359e-02,  2.0576e-02, -3.3424e-02],\n",
       "                        [ 2.5574e-02, -2.4931e-02, -1.5253e-02],\n",
       "                        [-9.5843e-04, -1.4002e-02, -1.6759e-02]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 5.0781e-02, -1.0080e-02,  2.4153e-02],\n",
       "                        [ 2.7696e-02, -1.6069e-02, -9.9441e-03],\n",
       "                        [ 4.4931e-02, -1.5412e-02,  4.9708e-03]],\n",
       "              \n",
       "                       [[ 3.6085e-02, -2.1508e-02, -5.0285e-03],\n",
       "                        [ 2.7850e-02,  5.7505e-03, -1.0954e-02],\n",
       "                        [ 2.4074e-02,  2.9341e-02, -2.2738e-03]],\n",
       "              \n",
       "                       [[ 2.6086e-02, -1.0071e-02, -3.1641e-02],\n",
       "                        [ 1.2304e-02,  2.2494e-03,  1.0693e-02],\n",
       "                        [ 1.8452e-02,  1.5394e-02,  1.7179e-02]]]])),\n",
       "             ('module.decoder.net.1.bias',\n",
       "              tensor([ 0.0034, -0.0213, -0.0245, -0.0307,  0.0314, -0.0186,  0.0483,  0.0231,\n",
       "                       0.0218,  0.0217, -0.0069, -0.0084, -0.0335,  0.0065,  0.0157,  0.0038,\n",
       "                       0.0216,  0.0104, -0.0404,  0.0392, -0.0405, -0.0289, -0.0142, -0.0323,\n",
       "                      -0.0367, -0.0352, -0.0199,  0.0035, -0.0189, -0.0440,  0.0481, -0.0049,\n",
       "                      -0.0308, -0.0060,  0.0339,  0.0113,  0.0305,  0.0104,  0.0124, -0.0161,\n",
       "                       0.0074,  0.0134,  0.0163,  0.0043,  0.0349,  0.0286, -0.0212,  0.0095,\n",
       "                       0.0273,  0.0278,  0.0113,  0.0127, -0.0340, -0.0374, -0.0077, -0.0312,\n",
       "                      -0.0304, -0.0042, -0.0077, -0.0268,  0.0014, -0.0237, -0.0069, -0.0342])),\n",
       "             ('module.decoder.net.2.weight',\n",
       "              tensor([0.9957, 1.0290, 1.0122, 0.9950, 1.0083, 0.9948, 1.0415, 0.9885, 0.9976,\n",
       "                      0.9941, 0.9641, 0.9708, 1.0520, 0.9764, 1.0702, 1.0151, 1.0274, 1.0071,\n",
       "                      0.9506, 1.0076, 0.9915, 0.9891, 0.9736, 0.9606, 1.0161, 0.9892, 1.0093,\n",
       "                      0.9724, 0.9836, 1.0964, 0.9622, 0.9753, 0.9759, 0.9640, 0.9708, 1.0139,\n",
       "                      1.0418, 1.0153, 0.9824, 1.0969, 0.9912, 1.0190, 1.0275, 1.0088, 0.9716,\n",
       "                      0.9792, 0.9900, 1.0167, 0.9691, 1.0310, 1.0145, 1.0438, 1.0093, 1.0203,\n",
       "                      0.9941, 0.9834, 1.0845, 0.9858, 0.9871, 1.0063, 0.9921, 0.9948, 0.9911,\n",
       "                      0.9851])),\n",
       "             ('module.decoder.net.2.bias',\n",
       "              tensor([ 0.0091, -0.0974,  0.0794,  0.0431,  0.0175,  0.0091, -0.1155,  0.0129,\n",
       "                       0.0529, -0.0149,  0.0459,  0.0130,  0.0742,  0.0406,  0.1034, -0.1832,\n",
       "                       0.0011,  0.0849, -0.0288,  0.0178,  0.0109, -0.1237,  0.0203,  0.0071,\n",
       "                      -0.0028,  0.0251,  0.0318, -0.0810, -0.0084, -0.1075, -0.0138,  0.0281,\n",
       "                       0.0502,  0.0366,  0.0185,  0.0653,  0.0539, -0.0009,  0.0602,  0.0765,\n",
       "                       0.0045,  0.0413, -0.0870, -0.0476, -0.0119, -0.0555,  0.0402,  0.0071,\n",
       "                       0.0231,  0.0199, -0.0519,  0.0520,  0.0084,  0.0842, -0.0521, -0.0157,\n",
       "                       0.1362,  0.0281, -0.0006,  0.0116, -0.0100,  0.0216,  0.0273,  0.0326])),\n",
       "             ('module.decoder.net.2.running_mean',\n",
       "              tensor([-0.6627, -0.0460,  0.2562, -0.5145, -0.3838,  0.1690,  0.0906,  0.0518,\n",
       "                       0.1036, -0.2043,  0.2566,  0.5046, -0.5136,  0.1923, -1.0307,  0.3383,\n",
       "                      -0.4887,  0.5774,  0.9010,  0.7888,  0.1748,  0.2171,  0.5024,  0.4992,\n",
       "                       0.5637, -0.5484, -1.0034,  0.3599, -0.0594,  0.0475,  0.9616, -0.2021,\n",
       "                       0.6215,  0.8015,  0.1297, -0.9312, -0.1522, -0.6895,  0.5031, -0.7273,\n",
       "                      -0.4586, -0.1332, -0.0851, -0.0411, -0.2932,  1.0002,  0.6038, -0.4651,\n",
       "                      -0.1007,  0.3140,  0.1866, -0.7098, -0.2641, -0.9581,  0.0805,  0.3840,\n",
       "                      -1.0674,  0.4164,  0.1482, -0.6736, -0.2068,  0.1224,  0.3265,  0.0298])),\n",
       "             ('module.decoder.net.2.running_var',\n",
       "              tensor([0.5697, 1.2608, 0.6788, 0.6461, 0.5941, 0.4386, 0.8261, 0.6764, 0.6454,\n",
       "                      0.7930, 0.9987, 0.6058, 0.8711, 0.4596, 1.2407, 1.2065, 0.8977, 1.0694,\n",
       "                      0.6066, 0.9074, 0.6951, 1.1145, 0.7192, 0.8190, 0.7415, 0.7984, 0.5569,\n",
       "                      0.6263, 0.5018, 0.8165, 0.7697, 0.7378, 0.9598, 0.6811, 0.4872, 0.9064,\n",
       "                      0.8578, 1.2081, 0.7640, 1.4188, 0.4436, 0.6201, 0.7532, 0.9256, 0.5426,\n",
       "                      0.7970, 0.6387, 1.2469, 0.3802, 0.9065, 0.9824, 0.7573, 0.6384, 0.7382,\n",
       "                      0.8032, 0.4514, 1.2078, 1.0139, 0.8238, 0.5906, 0.2976, 0.5767, 0.8043,\n",
       "                      0.6673])),\n",
       "             ('module.decoder.net.2.num_batches_tracked', tensor(0)),\n",
       "             ('module.decoder.net.5.weight',\n",
       "              tensor([[[[-4.4026e-02,  2.0590e-02, -3.9732e-02],\n",
       "                        [-7.6131e-03, -3.1535e-03, -1.1416e-02],\n",
       "                        [ 1.1645e-02, -3.5150e-02,  2.5421e-02]],\n",
       "              \n",
       "                       [[-5.1375e-03, -2.5705e-02,  1.9045e-02],\n",
       "                        [-3.3732e-02,  2.3594e-02,  2.4455e-02],\n",
       "                        [-3.7334e-03, -3.7240e-02,  4.1808e-02]],\n",
       "              \n",
       "                       [[ 2.7886e-02, -3.2955e-02,  1.1890e-02],\n",
       "                        [ 1.7915e-02, -3.1733e-03,  2.4759e-02],\n",
       "                        [-1.4631e-02, -3.4278e-02,  4.6138e-02]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 2.3807e-02,  3.4233e-02,  4.2940e-02],\n",
       "                        [-1.6285e-02,  3.7457e-02,  1.3460e-02],\n",
       "                        [ 3.0668e-03,  2.6866e-03, -1.5678e-02]],\n",
       "              \n",
       "                       [[ 4.3167e-02,  2.1705e-02,  1.9908e-02],\n",
       "                        [ 3.1260e-02,  1.2532e-02, -6.9328e-03],\n",
       "                        [ 5.1621e-02, -1.8172e-02, -1.5622e-02]],\n",
       "              \n",
       "                       [[-2.0158e-02, -1.9972e-04,  2.3648e-02],\n",
       "                        [ 2.4015e-02, -1.5574e-02,  1.8168e-03],\n",
       "                        [ 1.4601e-02,  3.4058e-02,  5.2513e-03]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 3.0670e-03, -1.5661e-03, -1.4808e-02],\n",
       "                        [-6.9717e-03,  3.4200e-02, -4.7448e-02],\n",
       "                        [-4.0671e-02, -3.3234e-02,  3.5313e-02]],\n",
       "              \n",
       "                       [[-1.3080e-02,  2.4508e-02,  4.4117e-02],\n",
       "                        [-2.2910e-02,  4.7165e-02,  2.1648e-02],\n",
       "                        [ 9.6767e-03, -1.4336e-02, -3.3095e-02]],\n",
       "              \n",
       "                       [[-6.0149e-03,  7.3602e-03,  1.8888e-02],\n",
       "                        [-1.6477e-02,  2.1631e-02,  3.3224e-03],\n",
       "                        [-1.2129e-02, -2.9095e-03, -2.5755e-02]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-3.0108e-02, -2.3100e-02,  2.3513e-02],\n",
       "                        [-4.3454e-03,  3.8563e-02, -9.5326e-03],\n",
       "                        [-2.7521e-03, -5.0152e-02, -2.7697e-02]],\n",
       "              \n",
       "                       [[ 1.3036e-02, -6.1027e-02, -2.9833e-02],\n",
       "                        [-1.7874e-02, -5.5836e-02, -2.4187e-03],\n",
       "                        [ 7.2588e-03, -3.0185e-02, -1.2481e-02]],\n",
       "              \n",
       "                       [[ 2.8150e-02,  1.1077e-04, -1.7408e-02],\n",
       "                        [ 1.0481e-02, -4.1531e-04,  3.3793e-02],\n",
       "                        [-2.9854e-02, -1.6611e-02,  3.9466e-03]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 3.0189e-02, -1.6489e-03,  7.2542e-03],\n",
       "                        [-1.3118e-02, -5.3028e-02,  2.0507e-02],\n",
       "                        [ 2.2972e-02, -4.4050e-02,  1.8151e-02]],\n",
       "              \n",
       "                       [[-3.3540e-02, -2.6623e-02,  3.4395e-03],\n",
       "                        [-3.4834e-02, -2.7517e-02,  3.5843e-02],\n",
       "                        [-1.8015e-02,  7.9831e-03,  2.1540e-02]],\n",
       "              \n",
       "                       [[-6.1483e-02,  2.3235e-02, -2.3970e-02],\n",
       "                        [ 1.3629e-02, -9.4705e-03, -2.2411e-02],\n",
       "                        [-3.0961e-03, -1.7849e-02, -3.2990e-03]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-5.8676e-02, -6.2909e-02, -2.4036e-02],\n",
       "                        [-5.2501e-02, -3.7417e-02,  2.7324e-02],\n",
       "                        [ 1.7372e-02, -3.5436e-02, -1.8778e-02]],\n",
       "              \n",
       "                       [[ 3.7097e-02, -2.1876e-02,  1.4486e-02],\n",
       "                        [ 3.8760e-03, -4.8297e-02,  1.4661e-02],\n",
       "                        [ 3.9645e-03, -8.2624e-03,  3.2447e-02]],\n",
       "              \n",
       "                       [[ 1.5947e-02,  3.5343e-02,  3.6436e-02],\n",
       "                        [-8.6469e-03, -1.3118e-02,  1.9387e-02],\n",
       "                        [-3.8212e-03, -1.1912e-02,  2.7737e-02]]],\n",
       "              \n",
       "              \n",
       "                      ...,\n",
       "              \n",
       "              \n",
       "                      [[[-1.7516e-02, -6.4810e-03, -1.3911e-02],\n",
       "                        [-3.1744e-02, -3.7877e-02, -3.9046e-02],\n",
       "                        [-1.5068e-02,  3.6082e-02,  6.5345e-03]],\n",
       "              \n",
       "                       [[ 4.9134e-02,  5.0512e-02, -5.8655e-03],\n",
       "                        [ 2.1930e-02,  1.8952e-02, -2.7563e-02],\n",
       "                        [ 6.7270e-02,  4.1235e-02,  3.2513e-02]],\n",
       "              \n",
       "                       [[-4.4939e-02, -4.7035e-05, -1.2877e-02],\n",
       "                        [ 6.8935e-03, -1.6632e-02,  3.5268e-02],\n",
       "                        [-7.3345e-03,  3.0724e-03,  2.8428e-02]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-1.6084e-02, -7.4709e-03, -3.0051e-02],\n",
       "                        [-5.3110e-03, -7.7674e-03,  2.5193e-03],\n",
       "                        [-9.9744e-03, -2.7159e-02, -1.0552e-02]],\n",
       "              \n",
       "                       [[-4.7533e-03,  7.9812e-04, -2.0608e-02],\n",
       "                        [-2.8794e-02,  5.6773e-03,  1.1128e-02],\n",
       "                        [-4.7611e-02,  1.9781e-03, -2.4542e-03]],\n",
       "              \n",
       "                       [[-3.3418e-02, -2.7509e-02,  3.4593e-02],\n",
       "                        [ 2.8667e-02, -2.7410e-02,  1.9769e-02],\n",
       "                        [ 2.7529e-02, -1.8497e-02, -1.5916e-02]]],\n",
       "              \n",
       "              \n",
       "                      [[[-1.5258e-02, -5.9266e-02,  1.8127e-02],\n",
       "                        [ 1.9560e-02, -3.1851e-02, -3.3074e-02],\n",
       "                        [ 7.7265e-03, -2.4273e-03,  3.1525e-02]],\n",
       "              \n",
       "                       [[ 7.5917e-02,  4.9825e-03,  3.6851e-02],\n",
       "                        [-2.4611e-03, -2.0948e-02,  1.1530e-02],\n",
       "                        [ 3.8274e-02,  5.7415e-02,  4.0774e-02]],\n",
       "              \n",
       "                       [[-8.1155e-03, -3.1376e-03,  2.0225e-02],\n",
       "                        [ 2.8574e-03, -4.9702e-02, -1.0678e-03],\n",
       "                        [-2.2995e-02, -3.9859e-02, -1.9396e-02]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-2.4049e-02,  8.5684e-04, -2.8464e-02],\n",
       "                        [-3.8792e-02,  2.7470e-02, -5.5849e-02],\n",
       "                        [ 4.5252e-02,  2.8372e-03, -1.7293e-02]],\n",
       "              \n",
       "                       [[ 1.9572e-02, -2.4832e-02, -2.8242e-02],\n",
       "                        [-4.4965e-02, -4.5766e-02, -8.0664e-04],\n",
       "                        [ 1.4493e-02, -2.5168e-02, -1.7014e-03]],\n",
       "              \n",
       "                       [[-3.2146e-02,  1.1694e-02, -1.1925e-02],\n",
       "                        [ 3.6263e-02, -2.6045e-02,  1.2201e-02],\n",
       "                        [ 4.2827e-02, -8.1173e-03, -3.7019e-02]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 3.1273e-02,  3.8092e-03, -4.2001e-02],\n",
       "                        [-3.8702e-02, -4.2983e-02, -8.7089e-03],\n",
       "                        [-3.1650e-02,  7.1249e-03, -7.5619e-03]],\n",
       "              \n",
       "                       [[-5.3834e-02, -3.6286e-04, -1.8795e-02],\n",
       "                        [-1.9773e-04,  4.6749e-02,  5.5061e-03],\n",
       "                        [-4.5963e-02,  4.3198e-02, -3.5799e-02]],\n",
       "              \n",
       "                       [[-1.8816e-02,  8.1213e-02,  5.2551e-02],\n",
       "                        [ 2.9428e-02, -2.5326e-02,  5.3513e-02],\n",
       "                        [ 2.5256e-02,  1.3353e-02,  4.2051e-02]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-3.1712e-02,  4.6880e-03, -7.9850e-03],\n",
       "                        [-2.7359e-02,  2.3869e-02, -1.2261e-02],\n",
       "                        [-4.9239e-02,  1.5897e-02, -3.5140e-03]],\n",
       "              \n",
       "                       [[-2.6865e-02, -3.4922e-02,  1.6252e-02],\n",
       "                        [-9.0093e-03, -3.4222e-03, -9.1070e-03],\n",
       "                        [ 2.6533e-02,  1.5503e-02, -3.4538e-02]],\n",
       "              \n",
       "                       [[ 2.0014e-02,  6.7900e-02,  4.9755e-02],\n",
       "                        [-7.8866e-03, -1.5334e-02, -2.2899e-02],\n",
       "                        [ 1.9434e-03,  4.3671e-02,  2.7272e-02]]]])),\n",
       "             ('module.decoder.net.5.bias',\n",
       "              tensor([ 0.0018,  0.0205, -0.0391, -0.0154,  0.0126,  0.0122, -0.0161, -0.0260,\n",
       "                      -0.0216, -0.0383,  0.0104,  0.0262,  0.0489,  0.0263, -0.0308,  0.0207,\n",
       "                      -0.0248, -0.0225,  0.0293, -0.0225,  0.0034, -0.0313,  0.0256, -0.0253,\n",
       "                      -0.0209,  0.0100,  0.0409,  0.0140, -0.0160,  0.0156, -0.0246, -0.0324,\n",
       "                       0.0131, -0.0264,  0.0144,  0.0173, -0.0078,  0.0359,  0.0177, -0.0397,\n",
       "                      -0.0039, -0.0314, -0.0017,  0.0296, -0.0234, -0.0186,  0.0075, -0.0219,\n",
       "                       0.0076, -0.0010, -0.0102,  0.0377,  0.0437,  0.0333, -0.0109,  0.0459,\n",
       "                      -0.0121,  0.0077,  0.0355, -0.0369,  0.0238,  0.0156, -0.0116,  0.0382])),\n",
       "             ('module.decoder.net.6.weight',\n",
       "              tensor([0.9807, 1.0047, 0.9756, 0.9783, 1.0304, 1.0820, 1.0014, 1.0037, 0.9422,\n",
       "                      0.9516, 1.0142, 1.0469, 1.0600, 1.0431, 1.0048, 1.0090, 0.9882, 0.9873,\n",
       "                      0.9447, 1.0174, 1.0393, 0.9889, 0.9959, 1.0071, 1.0185, 0.9775, 0.9837,\n",
       "                      1.0044, 1.0106, 0.9251, 1.0027, 1.0550, 1.0028, 0.9754, 1.0075, 0.9795,\n",
       "                      0.9579, 0.9511, 0.9921, 0.9917, 0.9828, 0.9965, 1.0064, 0.9628, 0.9917,\n",
       "                      1.0168, 0.9573, 0.9438, 1.0264, 0.9986, 0.9665, 0.9760, 0.9978, 0.9561,\n",
       "                      1.0630, 1.0579, 1.0261, 1.0623, 1.0052, 0.9899, 1.0093, 1.0216, 1.0016,\n",
       "                      0.9934])),\n",
       "             ('module.decoder.net.6.bias',\n",
       "              tensor([ 0.0046, -0.0166, -0.0152,  0.0068,  0.0140,  0.0505, -0.0302, -0.0186,\n",
       "                      -0.0275, -0.0142, -0.0264,  0.0847,  0.0106, -0.1962, -0.0054,  0.0078,\n",
       "                      -0.1705,  0.0060, -0.0221,  0.0328, -0.1801, -0.0084, -0.0561,  0.0448,\n",
       "                      -0.0207, -0.0232, -0.0162,  0.0611, -0.0291, -0.0383, -0.0044, -0.1896,\n",
       "                      -0.0265,  0.0649, -0.1355,  0.0253,  0.0018, -0.0375, -0.0392, -0.0054,\n",
       "                      -0.0106, -0.0423, -0.1624,  0.0031, -0.0296,  0.0175, -0.0064, -0.0037,\n",
       "                      -0.1863, -0.0259,  0.0018, -0.0730, -0.0020, -0.0153,  0.0068, -0.0084,\n",
       "                       0.0171,  0.0545, -0.0382, -0.0035,  0.0255, -0.1388, -0.1372, -0.0184])),\n",
       "             ('module.decoder.net.6.running_mean',\n",
       "              tensor([ 0.3793, -0.4397, -0.6626, -0.6310, -0.0961,  0.9450, -0.4909, -0.6196,\n",
       "                       0.0330,  0.5318, -0.0915, -0.5097, -0.3817,  0.1508, -0.3547, -0.4149,\n",
       "                      -0.0464,  0.2243,  0.3487, -0.7958,  0.1998,  0.2420,  0.4739, -0.3335,\n",
       "                      -0.4121, -1.0337, -0.5328, -0.4297,  0.2383,  0.2389, -0.3370, -0.0689,\n",
       "                      -0.7832, -1.1246, -0.8907,  0.0334, -0.1266, -0.2089, -0.3494, -0.4556,\n",
       "                      -1.0989,  0.5978, -0.2846, -0.5788, -1.4211, -0.4634, -0.4432, -0.5116,\n",
       "                       0.3924, -1.1932, -0.6440,  0.4415, -0.6545,  0.1705, -0.3487, -0.3101,\n",
       "                      -0.8747, -0.2818, -0.6304, -0.3615, -0.5330, -0.1790, -0.3134,  0.4608])),\n",
       "             ('module.decoder.net.6.running_var',\n",
       "              tensor([1.4656, 0.5753, 0.3441, 0.4273, 0.6249, 3.1866, 0.8968, 0.9924, 0.5608,\n",
       "                      1.3464, 1.8111, 1.1249, 1.1238, 0.9491, 0.6540, 0.9886, 0.5817, 1.3751,\n",
       "                      0.9482, 1.1843, 0.9382, 0.7058, 0.7555, 1.0079, 1.2902, 0.6203, 0.6529,\n",
       "                      1.1100, 0.5553, 0.5576, 2.0178, 0.7839, 0.6708, 1.1325, 0.8074, 1.3722,\n",
       "                      0.8867, 0.8465, 0.5089, 0.6417, 0.8971, 1.0523, 0.7914, 0.9362, 1.1564,\n",
       "                      0.9776, 0.5992, 0.8425, 0.5821, 0.9802, 0.6575, 0.5678, 0.5227, 0.5205,\n",
       "                      0.5815, 0.7538, 1.2245, 0.6958, 0.9535, 0.9784, 0.4586, 0.6980, 0.8287,\n",
       "                      0.6333])),\n",
       "             ('module.decoder.net.6.num_batches_tracked', tensor(0)),\n",
       "             ('module.decoder.net.9.weight',\n",
       "              tensor([[[[ 1.5641e-02, -2.2090e-02,  2.1615e-02],\n",
       "                        [ 5.4014e-03, -1.9961e-02,  3.7034e-03],\n",
       "                        [-5.1687e-02,  8.9170e-03,  1.0561e-02]],\n",
       "              \n",
       "                       [[ 5.9799e-03,  6.0812e-03,  2.6542e-02],\n",
       "                        [-7.8935e-03, -1.9420e-02, -6.0919e-03],\n",
       "                        [-2.6719e-03, -1.4806e-02, -3.7336e-02]],\n",
       "              \n",
       "                       [[-1.6846e-02, -1.1877e-02,  1.5376e-02],\n",
       "                        [-2.2386e-02, -5.3262e-03,  3.1640e-02],\n",
       "                        [ 1.1089e-04,  3.0131e-02,  1.2919e-02]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-7.0367e-03, -1.8531e-02,  2.6086e-02],\n",
       "                        [-1.7410e-03,  3.0120e-02, -2.0134e-02],\n",
       "                        [ 2.3000e-03, -1.2690e-02,  2.3706e-03]],\n",
       "              \n",
       "                       [[ 2.9203e-02,  6.7397e-02, -2.9402e-03],\n",
       "                        [ 2.8359e-02,  4.8927e-02,  4.4060e-02],\n",
       "                        [ 2.2209e-02, -2.1464e-02,  2.1905e-02]],\n",
       "              \n",
       "                       [[-3.7524e-03, -2.5438e-02, -1.4388e-02],\n",
       "                        [ 4.2045e-02,  1.0346e-02,  3.0261e-02],\n",
       "                        [-4.7863e-03,  1.3858e-03,  4.5834e-03]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 2.3737e-02, -3.3279e-02, -4.4828e-02],\n",
       "                        [-1.5032e-03, -5.4508e-02,  2.3505e-02],\n",
       "                        [-1.8642e-02,  8.4611e-03, -2.9226e-02]],\n",
       "              \n",
       "                       [[-2.6777e-02,  1.9774e-02, -7.9178e-03],\n",
       "                        [-4.3661e-02, -4.0670e-02, -3.5174e-02],\n",
       "                        [ 4.2080e-04,  1.6478e-02, -5.5203e-02]],\n",
       "              \n",
       "                       [[-3.4452e-03,  2.3428e-02,  5.3899e-03],\n",
       "                        [-1.5037e-02, -1.0693e-02, -5.4613e-03],\n",
       "                        [ 3.0267e-02, -4.6024e-02,  2.7123e-02]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-4.7588e-02, -6.2362e-02, -3.1889e-02],\n",
       "                        [ 9.0801e-03, -1.3052e-02, -4.0008e-03],\n",
       "                        [ 2.3667e-02,  4.9401e-02,  4.2726e-02]],\n",
       "              \n",
       "                       [[-6.9783e-03, -6.2045e-03, -1.5101e-02],\n",
       "                        [-3.3852e-03, -1.3845e-02, -1.7209e-02],\n",
       "                        [-5.8914e-03,  3.2362e-02,  6.7226e-04]],\n",
       "              \n",
       "                       [[ 2.2854e-02,  2.7361e-02, -2.6871e-02],\n",
       "                        [ 2.2411e-02, -9.4396e-03, -4.6756e-02],\n",
       "                        [ 3.0319e-02, -2.5494e-03, -3.2322e-02]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 3.3387e-02, -1.1705e-02,  3.5355e-02],\n",
       "                        [ 3.3018e-03,  4.5758e-02,  5.6682e-02],\n",
       "                        [ 2.9919e-02, -2.2832e-02,  5.2026e-02]],\n",
       "              \n",
       "                       [[-3.1782e-02, -1.5696e-02,  8.5581e-03],\n",
       "                        [-6.6184e-03, -3.5726e-02, -5.0180e-02],\n",
       "                        [ 2.3106e-03, -2.0457e-02, -5.8914e-03]],\n",
       "              \n",
       "                       [[ 9.3402e-03, -6.8070e-03, -2.8431e-02],\n",
       "                        [ 3.0464e-02,  2.5360e-03, -2.4654e-02],\n",
       "                        [ 5.4648e-02,  1.6426e-02,  2.2890e-02]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-4.4907e-02, -3.9988e-02,  4.3684e-02],\n",
       "                        [-2.4882e-02, -4.5224e-02,  1.5202e-02],\n",
       "                        [-1.7770e-02,  2.4162e-02, -3.5678e-02]],\n",
       "              \n",
       "                       [[ 3.0592e-03,  3.4002e-02,  3.4799e-02],\n",
       "                        [-4.4372e-02, -1.8201e-03,  6.0714e-03],\n",
       "                        [ 1.2361e-02, -7.5202e-03,  2.8692e-02]],\n",
       "              \n",
       "                       [[-4.6974e-02, -1.0879e-02, -4.9560e-03],\n",
       "                        [-4.0794e-02, -3.5885e-02,  3.0491e-02],\n",
       "                        [-2.6868e-02,  9.2678e-03, -1.4490e-02]]],\n",
       "              \n",
       "              \n",
       "                      ...,\n",
       "              \n",
       "              \n",
       "                      [[[-6.9916e-03,  1.8252e-02, -2.5449e-02],\n",
       "                        [-2.0542e-02,  4.9275e-02,  4.0564e-02],\n",
       "                        [-4.3281e-03,  3.9472e-02, -2.4402e-02]],\n",
       "              \n",
       "                       [[-1.2588e-02,  1.0244e-02,  2.9825e-02],\n",
       "                        [-4.7526e-02, -5.7236e-03,  8.8270e-03],\n",
       "                        [-1.7899e-02, -2.7689e-02, -4.0819e-02]],\n",
       "              \n",
       "                       [[ 3.1398e-02,  2.6839e-02,  3.4917e-02],\n",
       "                        [-1.7502e-02,  1.7580e-02,  8.0680e-03],\n",
       "                        [ 3.6548e-02, -5.0807e-03, -3.5366e-02]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-2.4835e-02, -2.4317e-02, -6.5434e-04],\n",
       "                        [ 1.2415e-02,  1.1836e-02,  2.8612e-02],\n",
       "                        [-2.7877e-02,  1.8029e-02, -8.0368e-03]],\n",
       "              \n",
       "                       [[-2.6217e-02,  8.8551e-03,  5.1346e-02],\n",
       "                        [ 1.9311e-02,  6.7770e-03,  9.8852e-03],\n",
       "                        [-5.7957e-03,  1.3919e-02,  4.1960e-02]],\n",
       "              \n",
       "                       [[-2.1293e-03,  1.3231e-02,  2.9981e-02],\n",
       "                        [ 3.4173e-02,  2.3971e-02,  2.1246e-02],\n",
       "                        [-4.2923e-02,  6.1742e-03, -2.5334e-02]]],\n",
       "              \n",
       "              \n",
       "                      [[[-2.5141e-02, -8.5986e-03,  2.6005e-02],\n",
       "                        [ 2.5317e-02,  1.2642e-02, -2.8985e-02],\n",
       "                        [-8.1050e-03,  1.6298e-03,  1.6467e-02]],\n",
       "              \n",
       "                       [[-2.8957e-02, -5.6861e-03, -1.2490e-02],\n",
       "                        [ 1.1941e-02, -2.9318e-02, -3.1382e-02],\n",
       "                        [-9.9331e-05, -2.8824e-02, -6.5226e-02]],\n",
       "              \n",
       "                       [[ 3.6990e-02, -2.4199e-02, -1.0001e-03],\n",
       "                        [ 1.3555e-02,  1.6203e-02,  3.3566e-03],\n",
       "                        [-2.0636e-02, -2.8967e-02, -1.4747e-02]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-4.5314e-02,  2.2598e-02, -3.2030e-02],\n",
       "                        [-4.6495e-02, -5.7751e-03,  1.6617e-02],\n",
       "                        [ 2.2405e-02, -2.5019e-02, -9.5019e-03]],\n",
       "              \n",
       "                       [[-2.7977e-02,  1.7690e-02, -1.4988e-02],\n",
       "                        [ 3.3382e-02,  1.0949e-02,  6.0637e-02],\n",
       "                        [-3.1572e-02,  5.6475e-02, -9.0269e-03]],\n",
       "              \n",
       "                       [[-2.0418e-02, -1.3797e-03,  3.5282e-02],\n",
       "                        [ 1.8171e-02, -2.9451e-02, -1.7769e-02],\n",
       "                        [-3.5664e-02, -1.1049e-02, -1.8497e-03]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 1.7677e-02, -2.3095e-02,  1.6355e-02],\n",
       "                        [-3.7812e-02, -3.2005e-02, -3.0291e-02],\n",
       "                        [-4.8211e-02,  2.4435e-03, -9.4052e-03]],\n",
       "              \n",
       "                       [[-4.5667e-03,  7.3812e-03, -5.3448e-03],\n",
       "                        [-9.4756e-03,  1.7876e-02,  1.1061e-03],\n",
       "                        [ 4.5304e-02,  3.3333e-02,  1.9611e-02]],\n",
       "              \n",
       "                       [[-4.6693e-02, -2.2780e-02,  2.3643e-02],\n",
       "                        [-8.6032e-03,  1.6179e-02, -1.2689e-02],\n",
       "                        [-7.4975e-03, -2.3664e-02, -4.7072e-02]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 1.6350e-02, -2.0746e-02, -2.6277e-02],\n",
       "                        [-2.5249e-02, -4.3957e-02,  1.3248e-02],\n",
       "                        [-4.4161e-02, -4.6273e-02, -2.5690e-02]],\n",
       "              \n",
       "                       [[ 3.1706e-02,  1.3431e-02,  2.0984e-02],\n",
       "                        [ 1.8843e-02,  2.0580e-02,  8.4872e-03],\n",
       "                        [ 1.9925e-02, -1.8994e-02, -1.5190e-02]],\n",
       "              \n",
       "                       [[-3.0474e-02, -5.8226e-02, -5.3902e-02],\n",
       "                        [-5.3814e-02, -2.0460e-02,  2.2955e-02],\n",
       "                        [-4.4204e-02, -1.9621e-02, -1.2426e-02]]]])),\n",
       "             ('module.decoder.net.9.bias',\n",
       "              tensor([ 0.0325, -0.0359,  0.0178, -0.0090,  0.0192,  0.0349,  0.0241, -0.0286,\n",
       "                       0.0249,  0.0297, -0.0420,  0.0225,  0.0228,  0.0282,  0.0079,  0.0193,\n",
       "                      -0.0315,  0.0266, -0.0224, -0.0343, -0.0007,  0.0345,  0.0073,  0.0062,\n",
       "                       0.0146,  0.0372,  0.0066, -0.0098, -0.0045, -0.0281,  0.0113, -0.0265,\n",
       "                      -0.0147, -0.0180,  0.0254, -0.0036,  0.0007,  0.0165,  0.0367, -0.0178,\n",
       "                      -0.0191,  0.0224, -0.0300, -0.0390,  0.0054, -0.0354, -0.0312,  0.0398,\n",
       "                       0.0473, -0.0111,  0.0022, -0.0057,  0.0224, -0.0005,  0.0006, -0.0001,\n",
       "                      -0.0035, -0.0142,  0.0270, -0.0025, -0.0332,  0.0310, -0.0309, -0.0338])),\n",
       "             ('module.decoder.net.10.weight',\n",
       "              tensor([0.9828, 0.9697, 1.0024, 1.0420, 0.9894, 0.9759, 0.9944, 1.0261, 1.0084,\n",
       "                      1.0113, 1.0246, 0.9964, 1.0118, 1.0079, 1.0068, 0.9848, 1.0247, 1.0068,\n",
       "                      0.9459, 0.9893, 0.9972, 1.0106, 0.9901, 1.0665, 0.9948, 0.9694, 0.9892,\n",
       "                      1.0104, 0.9829, 1.0220, 0.9912, 0.9878, 0.9759, 0.9848, 0.9941, 0.9952,\n",
       "                      0.9849, 0.9796, 0.9673, 0.9832, 0.9925, 0.9969, 0.9769, 1.0542, 1.0016,\n",
       "                      0.9944, 0.9913, 1.0035, 0.9896, 1.0042, 1.0251, 0.9172, 0.9931, 1.0056,\n",
       "                      0.9985, 0.9987, 0.9861, 0.9746, 1.0265, 1.0165, 1.0221, 1.0159, 1.0071,\n",
       "                      0.9815])),\n",
       "             ('module.decoder.net.10.bias',\n",
       "              tensor([-0.0242, -0.0336, -0.0096, -0.1441, -0.0095, -0.0645,  0.0094,  0.0232,\n",
       "                      -0.0152,  0.0163, -0.0643, -0.0198,  0.0224, -0.1856, -0.0972,  0.0046,\n",
       "                      -0.0500,  0.0269, -0.0288,  0.0090, -0.0009,  0.0378, -0.0288, -0.1685,\n",
       "                      -0.0107, -0.0214, -0.1037, -0.1213,  0.0225, -0.0425,  0.0080, -0.0157,\n",
       "                      -0.0115, -0.0649,  0.0225,  0.0293,  0.0211, -0.0191, -0.0402, -0.0101,\n",
       "                      -0.0017, -0.0870, -0.0104, -0.1806, -0.0040, -0.0925, -0.1464, -0.0099,\n",
       "                       0.0046,  0.0086, -0.1379, -0.0827, -0.0068, -0.0848, -0.0038, -0.0071,\n",
       "                      -0.0020, -0.0321, -0.1298, -0.0861,  0.0302,  0.0252,  0.0004,  0.0160])),\n",
       "             ('module.decoder.net.10.running_mean',\n",
       "              tensor([-0.2891, -0.6504, -0.1602,  0.4414, -1.0207, -0.0330, -0.7843, -1.0729,\n",
       "                      -0.0515, -1.0180, -0.6873, -0.2267, -0.9785,  0.1492, -0.3487, -0.7028,\n",
       "                      -0.3658,  0.1213,  0.4052, -0.4544, -0.6827, -0.8337, -0.5952,  0.4667,\n",
       "                      -0.1001, -0.0064,  0.0934, -0.3848, -1.0284, -0.0336, -0.6191, -0.4665,\n",
       "                       0.2981, -0.3325, -0.8321, -0.0151, -0.3291, -0.6575, -0.8917, -0.0661,\n",
       "                       0.5471, -0.1673, -0.7599,  0.2084,  0.4064, -0.2210,  0.2943,  0.6059,\n",
       "                      -0.3936, -0.3276,  0.0895,  0.1083, -0.9177,  0.5635, -0.7553, -0.5581,\n",
       "                      -1.1263, -0.8745, -0.2640, -0.4986, -0.1939, -0.3288, -0.9253, -1.3565])),\n",
       "             ('module.decoder.net.10.running_var',\n",
       "              tensor([1.1180, 0.6190, 1.9285, 0.3678, 0.6221, 0.6058, 0.8125, 1.0050, 0.5607,\n",
       "                      1.1964, 0.4901, 1.0484, 1.0857, 0.5080, 0.6779, 1.0033, 0.6742, 2.2723,\n",
       "                      1.6562, 2.3496, 1.1107, 1.1246, 1.4345, 0.4076, 1.9826, 0.8487, 0.7285,\n",
       "                      0.6172, 1.1290, 0.7601, 0.3605, 0.9195, 1.9949, 0.3222, 0.8874, 1.4182,\n",
       "                      0.6163, 0.4875, 2.5166, 0.6372, 2.5416, 0.5258, 1.0218, 0.6327, 2.9463,\n",
       "                      0.5569, 1.3975, 2.0226, 1.3269, 1.2006, 0.5390, 0.9987, 0.8245, 0.6229,\n",
       "                      1.1132, 1.5736, 1.5142, 0.7291, 0.7161, 0.6448, 1.0335, 1.1445, 0.9392,\n",
       "                      1.3138])),\n",
       "             ('module.decoder.net.10.num_batches_tracked', tensor(0)),\n",
       "             ('module.decoder.net.13.weight',\n",
       "              tensor([[[[-1.7916e-02, -1.6408e-03, -3.2174e-02],\n",
       "                        [-1.4707e-02,  1.0993e-02,  1.3973e-02],\n",
       "                        [ 3.0672e-02, -1.0105e-02,  1.3758e-02]],\n",
       "              \n",
       "                       [[ 3.7966e-02,  4.0144e-02,  1.5235e-03],\n",
       "                        [-1.5627e-02,  1.5096e-03, -5.4654e-03],\n",
       "                        [-2.4882e-02, -3.0121e-02,  2.1971e-02]],\n",
       "              \n",
       "                       [[-3.6955e-02, -2.3216e-02, -8.3340e-03],\n",
       "                        [-5.2959e-02,  1.8840e-02, -4.0187e-03],\n",
       "                        [-5.5378e-02, -4.7907e-02,  1.6850e-02]],\n",
       "              \n",
       "                       [[-5.0212e-02, -1.0428e-02,  3.3116e-02],\n",
       "                        [-5.6327e-02,  3.4490e-02,  4.1596e-02],\n",
       "                        [-3.6433e-02, -2.8786e-03,  3.2279e-02]],\n",
       "              \n",
       "                       [[ 3.3685e-02, -3.9250e-02,  3.7656e-02],\n",
       "                        [ 1.5998e-02,  3.5659e-02,  3.5082e-02],\n",
       "                        [-2.6590e-02, -3.8764e-02, -3.9542e-02]],\n",
       "              \n",
       "                       [[-4.9508e-03, -1.0402e-02,  1.0114e-03],\n",
       "                        [ 1.4940e-02,  2.5405e-02,  4.0197e-02],\n",
       "                        [-3.0039e-02, -2.9277e-02,  8.5364e-03]],\n",
       "              \n",
       "                       [[-1.7281e-02,  3.3933e-02,  2.3714e-02],\n",
       "                        [ 5.4624e-03,  2.2992e-02, -1.4494e-02],\n",
       "                        [ 1.2318e-02,  4.2469e-02,  7.9734e-03]],\n",
       "              \n",
       "                       [[ 3.3991e-02, -7.6166e-03,  8.9982e-03],\n",
       "                        [-2.8817e-02, -6.8045e-03,  3.1878e-02],\n",
       "                        [ 2.2245e-02, -3.2093e-04, -1.9013e-02]],\n",
       "              \n",
       "                       [[ 9.1617e-03,  1.6060e-02, -7.6787e-03],\n",
       "                        [-2.7408e-05, -2.2905e-02, -2.1066e-03],\n",
       "                        [ 2.8936e-02,  3.9148e-02,  1.3210e-02]],\n",
       "              \n",
       "                       [[-4.6667e-02,  2.7163e-02, -2.0228e-02],\n",
       "                        [-1.2208e-02,  1.2044e-02, -4.3873e-03],\n",
       "                        [-5.0882e-02, -2.5903e-02,  2.5428e-02]],\n",
       "              \n",
       "                       [[ 2.4518e-03, -4.6816e-02, -3.6411e-02],\n",
       "                        [ 1.3068e-02,  2.6002e-02,  1.7545e-02],\n",
       "                        [-3.5958e-02,  1.9700e-02,  1.1671e-02]],\n",
       "              \n",
       "                       [[ 6.2647e-03, -3.3527e-02, -4.0790e-04],\n",
       "                        [-6.3438e-03, -4.0270e-02,  3.5766e-02],\n",
       "                        [-9.5703e-03,  8.0579e-03, -1.5909e-02]],\n",
       "              \n",
       "                       [[ 4.3153e-02, -1.6875e-02,  1.5924e-02],\n",
       "                        [-6.9902e-04,  3.5242e-02,  3.1701e-02],\n",
       "                        [ 1.0114e-02,  5.4993e-02, -1.6957e-02]],\n",
       "              \n",
       "                       [[-1.2903e-02, -2.5000e-02, -1.2554e-02],\n",
       "                        [-1.2890e-02,  3.7718e-02,  3.1062e-02],\n",
       "                        [ 2.3578e-02,  1.4671e-02,  1.6079e-02]],\n",
       "              \n",
       "                       [[-2.2985e-02, -4.2380e-02,  3.3641e-02],\n",
       "                        [-4.0657e-02, -8.3643e-03,  1.5716e-02],\n",
       "                        [ 5.2323e-03,  3.5761e-02,  1.3457e-03]],\n",
       "              \n",
       "                       [[-2.2839e-02,  3.3552e-02, -3.0227e-02],\n",
       "                        [ 7.3465e-03,  6.1285e-03, -1.8157e-02],\n",
       "                        [ 2.4991e-02,  2.3156e-02,  3.8989e-02]],\n",
       "              \n",
       "                       [[-7.0884e-03, -3.8215e-02,  4.3265e-02],\n",
       "                        [-1.4856e-02,  9.2022e-03, -2.7039e-02],\n",
       "                        [-2.4093e-02, -5.3395e-03,  4.4911e-02]],\n",
       "              \n",
       "                       [[-3.1859e-02, -3.9369e-02, -2.0106e-02],\n",
       "                        [ 1.3754e-02, -1.2201e-02,  9.8246e-03],\n",
       "                        [-5.1934e-02, -3.7893e-02, -1.1612e-02]],\n",
       "              \n",
       "                       [[-5.3376e-02, -3.0299e-02, -3.7488e-02],\n",
       "                        [-2.1550e-02,  2.9598e-02,  5.0038e-03],\n",
       "                        [-2.0635e-02, -2.1950e-02,  1.2644e-02]],\n",
       "              \n",
       "                       [[-4.3061e-02,  2.3824e-02, -2.5175e-02],\n",
       "                        [-2.7410e-02, -2.2440e-02,  8.6829e-03],\n",
       "                        [-1.5720e-02, -1.0218e-02, -3.9418e-02]],\n",
       "              \n",
       "                       [[ 7.4889e-03, -4.6049e-02, -5.7202e-02],\n",
       "                        [-6.4537e-03,  2.2850e-02, -3.4683e-02],\n",
       "                        [-1.7413e-02, -7.9653e-03, -2.2441e-02]],\n",
       "              \n",
       "                       [[-1.5323e-02, -1.1275e-02, -3.4455e-02],\n",
       "                        [ 2.5223e-02, -5.8421e-03, -2.1252e-02],\n",
       "                        [-2.4618e-02, -4.9079e-02,  1.7546e-02]],\n",
       "              \n",
       "                       [[-3.0012e-02,  3.7550e-02,  3.9738e-02],\n",
       "                        [-6.0158e-03,  3.4293e-02, -4.3950e-02],\n",
       "                        [ 3.7776e-02, -3.8361e-02, -1.2638e-02]],\n",
       "              \n",
       "                       [[ 3.2988e-02,  2.2542e-02,  1.1473e-03],\n",
       "                        [ 9.8843e-03,  2.8243e-02, -5.1178e-02],\n",
       "                        [ 5.5202e-02,  4.4858e-03, -8.4620e-02]],\n",
       "              \n",
       "                       [[-3.0490e-02,  5.0549e-03, -4.1018e-02],\n",
       "                        [ 7.7333e-03, -3.7561e-02, -3.7237e-02],\n",
       "                        [-4.3342e-02, -3.8267e-02,  4.5243e-03]],\n",
       "              \n",
       "                       [[-4.3345e-02, -2.8440e-02, -2.9485e-02],\n",
       "                        [-2.7743e-02, -3.6249e-02,  2.7034e-02],\n",
       "                        [-1.7674e-02,  1.2772e-03,  2.1574e-02]],\n",
       "              \n",
       "                       [[-1.0491e-02, -2.9647e-02,  2.7099e-02],\n",
       "                        [-3.3213e-02,  3.9842e-02,  2.1082e-02],\n",
       "                        [ 1.7953e-02,  2.7855e-02, -2.9884e-02]],\n",
       "              \n",
       "                       [[ 4.6522e-02, -2.3322e-02, -1.6866e-02],\n",
       "                        [-1.1164e-02,  3.6387e-02,  4.3379e-02],\n",
       "                        [-3.3213e-02,  3.5156e-02, -1.9312e-02]],\n",
       "              \n",
       "                       [[-2.3346e-02,  2.6180e-02,  2.1488e-02],\n",
       "                        [ 2.3132e-02, -3.8277e-02,  7.9143e-03],\n",
       "                        [ 1.6293e-02, -2.3051e-03,  1.9815e-03]],\n",
       "              \n",
       "                       [[ 1.1778e-02,  1.4107e-02, -3.5508e-02],\n",
       "                        [-1.6085e-02,  1.4811e-02,  1.3423e-02],\n",
       "                        [ 2.5774e-02, -6.4724e-05,  1.1833e-02]],\n",
       "              \n",
       "                       [[-3.3618e-02,  1.1790e-02, -9.9679e-03],\n",
       "                        [-1.6835e-02,  1.3345e-02,  2.8244e-02],\n",
       "                        [-4.0599e-02,  1.9468e-02,  4.1839e-02]],\n",
       "              \n",
       "                       [[-1.4271e-02,  1.3911e-02,  1.1030e-02],\n",
       "                        [-3.4836e-02, -2.8052e-02,  2.9171e-02],\n",
       "                        [ 4.0353e-03, -3.5939e-02,  3.1649e-02]],\n",
       "              \n",
       "                       [[ 2.5365e-02, -3.9977e-02, -2.1761e-02],\n",
       "                        [-4.1687e-02, -4.6089e-02,  2.2882e-02],\n",
       "                        [-3.3822e-03, -3.6442e-02, -2.8426e-02]],\n",
       "              \n",
       "                       [[ 1.0235e-02,  1.4712e-02, -2.4990e-02],\n",
       "                        [-8.1456e-03,  2.9726e-02, -2.9425e-02],\n",
       "                        [ 2.4773e-02, -2.9620e-02,  1.2099e-02]],\n",
       "              \n",
       "                       [[ 9.6283e-03,  7.8084e-03,  5.9479e-03],\n",
       "                        [-3.1053e-02, -7.0533e-03, -1.3722e-02],\n",
       "                        [ 2.9344e-02, -3.8247e-02, -4.9254e-02]],\n",
       "              \n",
       "                       [[ 8.1974e-03, -3.3592e-03, -1.0284e-02],\n",
       "                        [ 9.8545e-03, -1.5733e-02, -4.7624e-02],\n",
       "                        [-3.1755e-02, -2.2926e-02, -3.9713e-02]],\n",
       "              \n",
       "                       [[ 2.8171e-02,  3.9544e-02, -3.7671e-02],\n",
       "                        [-3.6329e-02, -1.0238e-02, -3.5852e-02],\n",
       "                        [ 1.0456e-02, -3.6542e-02,  2.4991e-02]],\n",
       "              \n",
       "                       [[-1.6425e-02, -1.3940e-02, -1.1435e-02],\n",
       "                        [ 1.1926e-04,  2.2810e-02,  2.3832e-02],\n",
       "                        [-5.2410e-02,  1.9377e-02,  2.7828e-02]],\n",
       "              \n",
       "                       [[ 2.2938e-02,  3.3419e-02, -1.0102e-02],\n",
       "                        [ 2.0688e-02,  8.6106e-03, -2.1034e-02],\n",
       "                        [ 4.0096e-02, -2.5477e-02,  1.0003e-03]],\n",
       "              \n",
       "                       [[ 1.0100e-02, -1.8278e-02,  2.7808e-03],\n",
       "                        [ 2.2032e-02,  1.7597e-02, -4.0257e-02],\n",
       "                        [-3.2220e-03,  8.6902e-03,  2.4665e-02]],\n",
       "              \n",
       "                       [[ 2.4593e-02, -4.6764e-02, -3.5188e-02],\n",
       "                        [-5.5422e-02, -1.7215e-02, -5.6794e-02],\n",
       "                        [-8.0020e-03, -4.8115e-02,  8.0812e-03]],\n",
       "              \n",
       "                       [[-2.5454e-02,  3.2368e-02,  2.7091e-02],\n",
       "                        [-4.2619e-04, -9.7416e-03,  3.3045e-02],\n",
       "                        [-4.1527e-02,  3.0588e-02,  2.9501e-02]],\n",
       "              \n",
       "                       [[-8.3168e-03,  2.8135e-02, -2.6199e-02],\n",
       "                        [ 2.4928e-02,  1.7843e-02, -8.0917e-03],\n",
       "                        [ 1.2979e-02,  1.5417e-02, -2.8703e-02]],\n",
       "              \n",
       "                       [[ 4.6138e-03, -3.3654e-02,  2.0688e-02],\n",
       "                        [ 1.4083e-02, -2.6378e-02,  9.6478e-03],\n",
       "                        [-2.2673e-02,  4.8705e-02,  2.0441e-02]],\n",
       "              \n",
       "                       [[-2.6819e-02,  6.4305e-03,  8.7181e-03],\n",
       "                        [-5.3402e-02, -5.2115e-02, -4.9953e-02],\n",
       "                        [-4.8702e-02,  1.0423e-02, -4.0749e-02]],\n",
       "              \n",
       "                       [[-2.2046e-02,  3.4358e-02, -1.9696e-02],\n",
       "                        [-1.7046e-02,  3.8616e-02,  4.1839e-02],\n",
       "                        [ 1.0466e-02,  2.2403e-04, -3.1383e-02]],\n",
       "              \n",
       "                       [[ 2.2655e-02,  3.0648e-02,  2.4131e-03],\n",
       "                        [-1.9560e-02,  2.4793e-02,  9.7957e-03],\n",
       "                        [ 1.7132e-02, -6.0631e-04,  3.1681e-02]],\n",
       "              \n",
       "                       [[-4.2378e-02, -5.4674e-02, -4.4389e-02],\n",
       "                        [-4.4010e-02, -4.9126e-02, -3.0295e-02],\n",
       "                        [ 1.4017e-02, -5.5926e-02, -4.1826e-02]],\n",
       "              \n",
       "                       [[-3.1449e-02, -4.1084e-02, -5.3721e-02],\n",
       "                        [-3.4255e-02, -7.6238e-03,  1.4040e-02],\n",
       "                        [ 1.6153e-02, -1.3518e-02, -2.2194e-02]],\n",
       "              \n",
       "                       [[-2.8431e-02, -2.0858e-02, -1.4485e-02],\n",
       "                        [-1.4500e-02, -1.3852e-02,  4.7358e-02],\n",
       "                        [ 5.9301e-02,  5.2398e-02, -9.5283e-03]],\n",
       "              \n",
       "                       [[ 1.6198e-02,  2.5225e-02,  2.1717e-02],\n",
       "                        [-3.5275e-03, -1.1761e-02,  3.7683e-02],\n",
       "                        [ 5.6554e-03,  2.0849e-02,  4.2432e-02]],\n",
       "              \n",
       "                       [[-3.3612e-03,  3.8133e-02, -1.8853e-02],\n",
       "                        [-2.9125e-02,  1.0028e-02,  2.3214e-02],\n",
       "                        [-1.8781e-02, -4.8652e-03, -1.0564e-02]],\n",
       "              \n",
       "                       [[ 2.2441e-02, -2.2948e-02, -1.2434e-02],\n",
       "                        [ 1.2694e-02, -3.4327e-03, -1.3709e-02],\n",
       "                        [ 1.9064e-03,  1.6405e-02,  1.8310e-04]],\n",
       "              \n",
       "                       [[-3.5508e-02,  3.8386e-02,  9.1544e-03],\n",
       "                        [ 2.0707e-02, -2.1372e-02, -9.4117e-03],\n",
       "                        [ 3.3386e-02,  4.2884e-02,  1.4236e-02]],\n",
       "              \n",
       "                       [[ 2.8702e-02, -2.8040e-02, -3.8101e-02],\n",
       "                        [-3.8048e-02, -2.4408e-02, -3.1507e-02],\n",
       "                        [ 3.0143e-02, -2.1989e-02, -1.8052e-02]],\n",
       "              \n",
       "                       [[-3.2684e-02, -1.2849e-02,  6.8674e-03],\n",
       "                        [-1.8977e-02, -1.4097e-02, -7.2094e-03],\n",
       "                        [-2.1427e-02, -3.4694e-02, -3.1338e-02]],\n",
       "              \n",
       "                       [[ 2.8533e-02, -1.0311e-02, -1.3711e-02],\n",
       "                        [ 3.4484e-02,  1.1987e-02, -8.9300e-03],\n",
       "                        [-2.4930e-02, -6.8541e-03,  3.5464e-02]],\n",
       "              \n",
       "                       [[ 1.9246e-02,  1.3042e-02, -2.3368e-02],\n",
       "                        [ 2.6234e-02, -2.8226e-02, -2.4157e-02],\n",
       "                        [ 1.8267e-02, -8.1121e-03,  2.8822e-02]],\n",
       "              \n",
       "                       [[ 1.8751e-02, -1.8536e-02, -3.6392e-02],\n",
       "                        [ 3.5903e-02, -1.2636e-02, -2.8422e-02],\n",
       "                        [ 3.2790e-02,  2.3144e-02, -1.7843e-02]],\n",
       "              \n",
       "                       [[-2.0704e-02,  1.3152e-03, -1.4456e-02],\n",
       "                        [-4.4512e-02, -2.3566e-02,  3.1183e-02],\n",
       "                        [ 2.7198e-02, -5.1114e-03,  4.5853e-02]],\n",
       "              \n",
       "                       [[-4.6367e-03, -4.4128e-02, -4.7730e-02],\n",
       "                        [ 1.9392e-02,  4.7676e-03,  2.9022e-03],\n",
       "                        [ 5.7715e-03, -3.0965e-02, -2.7132e-02]],\n",
       "              \n",
       "                       [[-2.2734e-02, -4.1337e-04, -4.5936e-02],\n",
       "                        [-1.3387e-02,  1.1342e-02, -4.1184e-02],\n",
       "                        [-2.1876e-02,  3.1547e-02, -2.4002e-03]],\n",
       "              \n",
       "                       [[ 1.5651e-02, -1.8021e-02, -3.8692e-02],\n",
       "                        [-2.1916e-02,  9.9581e-03,  7.0142e-03],\n",
       "                        [ 2.9404e-02, -4.2963e-02, -3.1834e-02]],\n",
       "              \n",
       "                       [[ 1.4291e-02, -6.3723e-03,  6.6180e-03],\n",
       "                        [ 2.7549e-02,  8.2385e-03, -2.5878e-02],\n",
       "                        [ 2.3192e-02,  2.4906e-02,  3.6733e-02]]]])),\n",
       "             ('module.decoder.net.13.bias', tensor([0.2110])),\n",
       "             ('module.decoder.net.14.weight', tensor([1.1504])),\n",
       "             ('module.decoder.net.14.bias', tensor([-0.4416])),\n",
       "             ('module.decoder.net.14.running_mean', tensor([0.0240])),\n",
       "             ('module.decoder.net.14.running_var', tensor([4.4406])),\n",
       "             ('module.decoder.net.14.num_batches_tracked', tensor(0)),\n",
       "             ('module.classifier_vae.encoder.net.0.weight',\n",
       "              tensor([[[[ 0.1450,  0.0988, -0.2400],\n",
       "                        [-0.1850, -0.1016, -0.1413],\n",
       "                        [ 0.1296, -0.2669,  0.2499]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.2049,  0.3520,  0.2988],\n",
       "                        [ 0.2565, -0.1013,  0.1333],\n",
       "                        [ 0.1499,  0.2071, -0.2072]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.0668,  0.1844,  0.2020],\n",
       "                        [ 0.0138, -0.0754,  0.0701],\n",
       "                        [-0.0401,  0.3499,  0.2554]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.0390,  0.2529, -0.2273],\n",
       "                        [-0.2155,  0.0239, -0.2810],\n",
       "                        [ 0.1419, -0.0556,  0.2131]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.1264, -0.0946,  0.2482],\n",
       "                        [ 0.2914,  0.2594, -0.0604],\n",
       "                        [ 0.0205,  0.1349, -0.1073]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.0164, -0.3211, -0.0313],\n",
       "                        [-0.1618,  0.0753,  0.1636],\n",
       "                        [-0.0097, -0.1392, -0.0044]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.1322, -0.1921,  0.2684],\n",
       "                        [ 0.1963, -0.0368, -0.1208],\n",
       "                        [-0.1278, -0.1506,  0.0592]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.0846,  0.3308, -0.3219],\n",
       "                        [ 0.1977,  0.2404,  0.0970],\n",
       "                        [ 0.1459, -0.2712, -0.1775]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.2980, -0.2288, -0.0688],\n",
       "                        [ 0.2959, -0.0502, -0.3329],\n",
       "                        [ 0.1864,  0.2229, -0.2958]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.0527,  0.2194,  0.2344],\n",
       "                        [-0.1691,  0.0897,  0.0062],\n",
       "                        [-0.1601,  0.2999, -0.0962]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.3094,  0.1451, -0.1005],\n",
       "                        [ 0.1399,  0.2088,  0.0289],\n",
       "                        [-0.2842,  0.3220,  0.1195]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.3509,  0.3545, -0.0767],\n",
       "                        [ 0.1725,  0.1362, -0.3054],\n",
       "                        [-0.1854,  0.3204, -0.2355]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.0656,  0.2789, -0.1019],\n",
       "                        [-0.0854,  0.0668, -0.0571],\n",
       "                        [ 0.0301, -0.2516,  0.1778]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.0627, -0.0349,  0.3013],\n",
       "                        [-0.3363,  0.0794, -0.1825],\n",
       "                        [-0.2594, -0.3203,  0.2221]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.0171,  0.3664,  0.0282],\n",
       "                        [ 0.2283, -0.1924,  0.1299],\n",
       "                        [ 0.2150,  0.3183,  0.0255]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.2476, -0.1032,  0.0905],\n",
       "                        [ 0.3220,  0.0076,  0.1699],\n",
       "                        [-0.1305, -0.0074, -0.1937]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.0633,  0.3619, -0.2487],\n",
       "                        [ 0.0591, -0.2029,  0.0557],\n",
       "                        [ 0.0093, -0.1940,  0.2359]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.0310, -0.0998, -0.3364],\n",
       "                        [-0.1377, -0.1292,  0.0903],\n",
       "                        [ 0.1820,  0.0700,  0.2617]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.0283, -0.1263,  0.0470],\n",
       "                        [-0.3689, -0.1651,  0.0096],\n",
       "                        [ 0.1516, -0.0248,  0.2869]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.0562, -0.3396, -0.1319],\n",
       "                        [-0.1661,  0.0450, -0.3033],\n",
       "                        [ 0.2238,  0.2561,  0.1748]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.3510,  0.0683,  0.2995],\n",
       "                        [-0.2619, -0.1685, -0.1145],\n",
       "                        [ 0.0096, -0.2397,  0.2362]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.0703, -0.0702,  0.0376],\n",
       "                        [ 0.0015,  0.0531,  0.2012],\n",
       "                        [ 0.2240,  0.1544,  0.0646]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.0178,  0.2202, -0.3559],\n",
       "                        [ 0.0440, -0.1101,  0.0033],\n",
       "                        [ 0.0383, -0.2195, -0.0723]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.2809, -0.1656, -0.2397],\n",
       "                        [-0.2059,  0.0514,  0.1118],\n",
       "                        [-0.1914, -0.1029, -0.1635]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.0206, -0.1521, -0.0011],\n",
       "                        [-0.1068,  0.1512, -0.0469],\n",
       "                        [-0.1938, -0.2271,  0.2199]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.0642, -0.0322, -0.1932],\n",
       "                        [-0.0351, -0.1305,  0.1566],\n",
       "                        [-0.0398, -0.3146,  0.2284]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.1607, -0.2062,  0.1897],\n",
       "                        [-0.3180,  0.3190, -0.2484],\n",
       "                        [-0.2349, -0.1061,  0.2690]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.0437,  0.1866, -0.2750],\n",
       "                        [ 0.0759, -0.0474, -0.0076],\n",
       "                        [-0.0915, -0.1731,  0.0533]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.2789, -0.0746, -0.1814],\n",
       "                        [-0.2469,  0.2226, -0.2389],\n",
       "                        [-0.1003, -0.2495, -0.0130]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.1868,  0.0825, -0.0290],\n",
       "                        [-0.1203, -0.1800, -0.1552],\n",
       "                        [ 0.2399, -0.3524, -0.1920]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.0124,  0.2907, -0.2146],\n",
       "                        [ 0.3154, -0.0759,  0.3584],\n",
       "                        [ 0.2133,  0.0128, -0.0752]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.0095,  0.2485,  0.0736],\n",
       "                        [ 0.2414, -0.2328, -0.1501],\n",
       "                        [ 0.2616, -0.0518,  0.1993]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.2846,  0.0540, -0.0104],\n",
       "                        [ 0.2560, -0.1641,  0.2757],\n",
       "                        [ 0.0680, -0.0985, -0.2351]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.1198, -0.3398, -0.1747],\n",
       "                        [-0.2282,  0.1613,  0.2722],\n",
       "                        [-0.1523, -0.2183,  0.1603]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.1521,  0.2460,  0.3285],\n",
       "                        [ 0.1020, -0.0691, -0.0417],\n",
       "                        [ 0.3148,  0.2013, -0.1806]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.1833, -0.3457, -0.0506],\n",
       "                        [ 0.2171,  0.1429,  0.1637],\n",
       "                        [-0.3328, -0.0796,  0.0159]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.3539, -0.1780,  0.0489],\n",
       "                        [ 0.2908,  0.2534, -0.1014],\n",
       "                        [-0.0335,  0.2350,  0.3095]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.0277, -0.1421,  0.0112],\n",
       "                        [ 0.0869,  0.2105,  0.2582],\n",
       "                        [-0.0501, -0.3028, -0.1131]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.0218, -0.1745,  0.1232],\n",
       "                        [ 0.2735,  0.0558,  0.0597],\n",
       "                        [ 0.1731,  0.2144,  0.2990]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.3080, -0.0112, -0.2501],\n",
       "                        [ 0.2580,  0.0119, -0.2987],\n",
       "                        [ 0.2906, -0.3168,  0.0785]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.0136, -0.2390,  0.3095],\n",
       "                        [-0.2867,  0.1822,  0.0317],\n",
       "                        [-0.3734, -0.3058,  0.0795]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.2354, -0.2449,  0.0468],\n",
       "                        [-0.1798,  0.2818,  0.0174],\n",
       "                        [ 0.1441,  0.1490,  0.2393]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.2415,  0.0256, -0.1323],\n",
       "                        [ 0.0283,  0.0869,  0.1824],\n",
       "                        [ 0.3124,  0.3485, -0.1046]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.2126,  0.0211, -0.0293],\n",
       "                        [ 0.1480,  0.0835,  0.2675],\n",
       "                        [ 0.0875,  0.1250,  0.2798]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.0049, -0.2322,  0.0023],\n",
       "                        [ 0.1535, -0.0876,  0.2325],\n",
       "                        [ 0.1411, -0.2084,  0.0346]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.0369,  0.2878, -0.1666],\n",
       "                        [-0.0205,  0.0092, -0.2517],\n",
       "                        [ 0.1512, -0.3108, -0.2117]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.2401, -0.3483,  0.1020],\n",
       "                        [-0.0233, -0.0831, -0.0602],\n",
       "                        [-0.0499,  0.2134,  0.1964]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.1093,  0.0620,  0.1654],\n",
       "                        [ 0.1920,  0.1586,  0.2924],\n",
       "                        [-0.2335,  0.1922, -0.1595]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.0761, -0.1096,  0.0820],\n",
       "                        [ 0.1074,  0.1964,  0.1290],\n",
       "                        [ 0.2382,  0.2152, -0.0788]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.2656,  0.1971,  0.1298],\n",
       "                        [-0.1487,  0.0674,  0.1298],\n",
       "                        [-0.3075,  0.0564,  0.2325]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.2599,  0.2378, -0.1313],\n",
       "                        [ 0.2726,  0.2889,  0.3173],\n",
       "                        [-0.0973, -0.1684,  0.0371]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.0966,  0.2378, -0.2418],\n",
       "                        [-0.1569,  0.3094,  0.3028],\n",
       "                        [ 0.2496, -0.0253,  0.0205]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.1963, -0.0403, -0.2298],\n",
       "                        [ 0.0025,  0.0655, -0.0072],\n",
       "                        [-0.1601,  0.3296, -0.2428]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.0111,  0.2926,  0.2344],\n",
       "                        [-0.2986, -0.2082,  0.0610],\n",
       "                        [ 0.1452,  0.1559, -0.1530]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.0300, -0.0115, -0.0856],\n",
       "                        [ 0.0277, -0.2809, -0.1755],\n",
       "                        [ 0.0198, -0.2076, -0.2797]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.0367,  0.2519, -0.0660],\n",
       "                        [ 0.1607,  0.1405,  0.2058],\n",
       "                        [ 0.2809, -0.0639,  0.2309]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.1307,  0.1936,  0.2008],\n",
       "                        [-0.1438, -0.1811, -0.1860],\n",
       "                        [-0.2109, -0.3069,  0.2882]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.0296, -0.2446, -0.2331],\n",
       "                        [ 0.2931, -0.1216, -0.0108],\n",
       "                        [ 0.0326,  0.0689,  0.1036]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.0611, -0.0748,  0.2149],\n",
       "                        [ 0.0989,  0.2824,  0.1495],\n",
       "                        [ 0.0617,  0.0751,  0.2512]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.0844, -0.1969,  0.0155],\n",
       "                        [ 0.1333, -0.1067, -0.1167],\n",
       "                        [-0.1403, -0.0523,  0.0465]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.1732,  0.2891, -0.2922],\n",
       "                        [ 0.0523, -0.1641, -0.0565],\n",
       "                        [ 0.0926, -0.2075,  0.0989]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.0470,  0.0258,  0.2325],\n",
       "                        [ 0.0273, -0.1051,  0.1853],\n",
       "                        [ 0.0264,  0.3259,  0.1296]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.0803,  0.2052,  0.1966],\n",
       "                        [ 0.0920,  0.2305, -0.3182],\n",
       "                        [ 0.0606, -0.2009, -0.2900]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.2335, -0.2631, -0.0453],\n",
       "                        [-0.0893, -0.0464, -0.1453],\n",
       "                        [-0.2259,  0.2042, -0.3643]]]])),\n",
       "             ('module.classifier_vae.encoder.net.0.bias',\n",
       "              tensor([-0.2298, -0.1504, -0.1960, -0.3160, -0.3095,  0.2895, -0.2317, -0.0834,\n",
       "                      -0.0192,  0.2346,  0.0745, -0.0608, -0.2354, -0.0693,  0.0853,  0.2104,\n",
       "                      -0.3242,  0.0545,  0.1026, -0.2306,  0.2171, -0.0917, -0.3075,  0.2860,\n",
       "                      -0.2548, -0.0049, -0.1595,  0.2211,  0.0291, -0.0647,  0.2872,  0.1350,\n",
       "                      -0.1010, -0.3059, -0.2252, -0.2239, -0.2203,  0.0756,  0.0153,  0.2279,\n",
       "                      -0.0189, -0.1817, -0.1972, -0.3071, -0.1999,  0.1144,  0.2205, -0.1253,\n",
       "                      -0.1618,  0.0155,  0.1385, -0.3100,  0.0571,  0.3261, -0.2762, -0.0985,\n",
       "                      -0.0972,  0.3119,  0.1374,  0.2057,  0.0958, -0.1927,  0.2403, -0.2325])),\n",
       "             ('module.classifier_vae.encoder.net.1.weight',\n",
       "              tensor([1.0039, 1.0011, 0.9952, 0.9584, 1.0146, 0.9902, 0.9680, 0.9483, 1.0189,\n",
       "                      0.9949, 1.0522, 1.0397, 1.0066, 0.9891, 1.0332, 0.9912, 1.0142, 0.9814,\n",
       "                      0.9601, 0.9472, 0.9807, 0.9827, 1.1024, 1.1248, 1.0052, 1.0128, 1.1166,\n",
       "                      1.0020, 1.1377, 1.0377, 0.9825, 0.9800, 0.9830, 0.9991, 0.9755, 1.0074,\n",
       "                      1.0193, 1.0017, 0.9431, 1.0420, 0.9963, 0.9885, 0.9404, 1.0245, 1.0136,\n",
       "                      0.9837, 0.9660, 0.9730, 0.9934, 0.9458, 0.9939, 1.0469, 1.0025, 1.0251,\n",
       "                      1.1272, 1.0570, 0.9570, 0.9394, 1.0064, 1.0861, 0.9737, 0.9927, 0.9385,\n",
       "                      1.0851])),\n",
       "             ('module.classifier_vae.encoder.net.1.bias',\n",
       "              tensor([ 0.0016,  0.0772,  0.0884, -0.0467,  0.0517,  0.0217, -0.0163,  0.0009,\n",
       "                       0.0253,  0.0341,  0.0810, -0.0645, -0.0065,  0.0385,  0.0578, -0.0358,\n",
       "                      -0.0003, -0.0455, -0.0313, -0.0637, -0.0114,  0.0786,  0.1036,  0.0971,\n",
       "                       0.0076,  0.0456,  0.0832,  0.0402,  0.1220,  0.0445,  0.0542,  0.0269,\n",
       "                      -0.0575,  0.0061,  0.1045, -0.0530,  0.0907, -0.0741,  0.0390,  0.0991,\n",
       "                       0.0358,  0.0629,  0.0564,  0.0849, -0.0120,  0.0071, -0.0587,  0.0272,\n",
       "                       0.0462, -0.0107,  0.0757,  0.0710, -0.0198,  0.0461,  0.0897,  0.0851,\n",
       "                      -0.0150, -0.0368,  0.0908,  0.1096, -0.0174,  0.0612, -0.0112,  0.0529])),\n",
       "             ('module.classifier_vae.encoder.net.1.running_mean',\n",
       "              tensor([-0.2605, -0.0716, -0.1060, -0.3338, -0.2597,  0.2547, -0.2417, -0.0687,\n",
       "                      -0.0719,  0.2668,  0.1539, -0.0748, -0.2320, -0.1249,  0.1873,  0.2051,\n",
       "                      -0.3240,  0.0410,  0.0826, -0.2508,  0.1675, -0.0246, -0.3434,  0.2302,\n",
       "                      -0.2920, -0.0379, -0.2059,  0.1894, -0.0731, -0.1427,  0.3583,  0.1865,\n",
       "                      -0.0995, -0.3655, -0.1323, -0.2279, -0.1155,  0.0718,  0.1026,  0.1763,\n",
       "                      -0.0734, -0.1199, -0.1486, -0.2021, -0.1934,  0.0644,  0.1890, -0.0717,\n",
       "                      -0.0982,  0.0219,  0.2288, -0.2387,  0.0504,  0.3467, -0.3602,  0.0053,\n",
       "                      -0.1416,  0.2953,  0.2266,  0.1751,  0.0875, -0.1155,  0.2428, -0.3386])),\n",
       "             ('module.classifier_vae.encoder.net.1.running_var',\n",
       "              tensor([0.0103, 0.0333, 0.0349, 0.0084, 0.0169, 0.0093, 0.0060, 0.0167, 0.0322,\n",
       "                      0.0136, 0.0355, 0.0139, 0.0030, 0.0310, 0.0421, 0.0055, 0.0039, 0.0145,\n",
       "                      0.0170, 0.0239, 0.0300, 0.0193, 0.0104, 0.0160, 0.0104, 0.0107, 0.0207,\n",
       "                      0.0058, 0.0404, 0.0319, 0.0273, 0.0146, 0.0058, 0.0283, 0.0418, 0.0093,\n",
       "                      0.0501, 0.0079, 0.0376, 0.0251, 0.0344, 0.0196, 0.0248, 0.0453, 0.0056,\n",
       "                      0.0229, 0.0209, 0.0222, 0.0228, 0.0199, 0.0505, 0.0264, 0.0074, 0.0111,\n",
       "                      0.0374, 0.0448, 0.0243, 0.0124, 0.0371, 0.0064, 0.0091, 0.0263, 0.0214,\n",
       "                      0.0443])),\n",
       "             ('module.classifier_vae.encoder.net.1.num_batches_tracked',\n",
       "              tensor(0)),\n",
       "             ('module.classifier_vae.encoder.net.3.weight',\n",
       "              tensor([[[[ 1.8177e-03,  1.5807e-02, -2.9654e-03],\n",
       "                        [-2.1436e-02, -3.3003e-02,  3.0290e-02],\n",
       "                        [-3.2440e-03, -2.6039e-02,  2.5011e-02]],\n",
       "              \n",
       "                       [[-3.2724e-02, -4.2849e-02, -5.1278e-04],\n",
       "                        [-3.4772e-02,  2.1060e-02, -5.5106e-02],\n",
       "                        [ 2.3273e-02, -1.0349e-02, -3.1009e-03]],\n",
       "              \n",
       "                       [[-5.7055e-02,  5.0578e-03, -1.1663e-02],\n",
       "                        [-8.9136e-03,  3.1825e-02,  2.1786e-02],\n",
       "                        [ 2.4652e-02, -2.0773e-02,  2.5429e-02]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-4.2507e-03, -2.5953e-02,  1.4606e-02],\n",
       "                        [ 1.9409e-02, -8.0893e-03, -1.1761e-02],\n",
       "                        [ 2.3553e-02,  3.4531e-02,  5.5434e-03]],\n",
       "              \n",
       "                       [[ 4.9535e-02, -3.3643e-02,  2.8811e-02],\n",
       "                        [ 1.0489e-02, -2.4659e-02,  2.0452e-02],\n",
       "                        [ 1.7026e-03, -2.5198e-02, -2.5228e-02]],\n",
       "              \n",
       "                       [[ 3.9992e-02,  1.5632e-02, -8.0971e-03],\n",
       "                        [ 2.7177e-02, -1.8670e-02,  4.8614e-02],\n",
       "                        [-3.1021e-02, -9.1506e-03,  3.5263e-02]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 4.8625e-03, -9.2236e-03, -4.2484e-02],\n",
       "                        [-3.8919e-02, -6.9247e-02, -1.3389e-02],\n",
       "                        [-2.2814e-02, -5.1747e-02, -1.4168e-02]],\n",
       "              \n",
       "                       [[ 4.6391e-02,  5.7646e-02,  5.0787e-02],\n",
       "                        [ 2.4597e-02, -4.8847e-03,  2.3210e-02],\n",
       "                        [ 3.1240e-03, -2.9990e-03,  2.7921e-02]],\n",
       "              \n",
       "                       [[ 2.8376e-02,  7.1674e-02, -4.1785e-04],\n",
       "                        [ 1.3004e-02,  2.3395e-02,  3.4758e-02],\n",
       "                        [ 4.1930e-02, -1.3604e-02, -2.6695e-02]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 4.0436e-03,  1.4178e-02,  8.7202e-03],\n",
       "                        [ 9.2425e-03,  5.3828e-02,  9.4415e-03],\n",
       "                        [-7.8250e-03, -8.8314e-03,  2.0769e-02]],\n",
       "              \n",
       "                       [[ 3.8572e-02, -3.1740e-02,  2.5983e-02],\n",
       "                        [-1.6995e-02,  3.6479e-02,  1.6953e-02],\n",
       "                        [ 3.2304e-02, -2.1156e-02, -1.2322e-02]],\n",
       "              \n",
       "                       [[-3.7219e-02, -3.9377e-02, -5.7920e-02],\n",
       "                        [-1.6266e-02, -5.8985e-03, -2.3277e-02],\n",
       "                        [ 1.6594e-02, -2.6136e-02, -1.0590e-01]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 1.5409e-02, -2.1090e-02, -5.7404e-03],\n",
       "                        [ 7.0012e-03,  3.3946e-02,  1.5845e-03],\n",
       "                        [ 1.1008e-02, -2.5290e-02, -3.2156e-02]],\n",
       "              \n",
       "                       [[-1.2860e-02, -4.5003e-02,  1.0001e-02],\n",
       "                        [-4.9914e-02, -1.4619e-02, -4.2936e-02],\n",
       "                        [-4.0630e-02,  8.4513e-03,  2.5836e-02]],\n",
       "              \n",
       "                       [[-1.1437e-02, -1.5014e-02, -3.4150e-03],\n",
       "                        [-4.7235e-05,  3.6168e-02, -2.8791e-02],\n",
       "                        [-4.8039e-02, -1.0653e-02, -6.8314e-03]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-1.4401e-02, -2.7559e-02, -3.2273e-03],\n",
       "                        [ 1.5381e-02, -2.6111e-02, -2.5711e-03],\n",
       "                        [-1.1961e-02, -1.0437e-03, -2.4136e-02]],\n",
       "              \n",
       "                       [[ 2.3150e-02,  3.4188e-02,  1.0508e-02],\n",
       "                        [-2.3304e-02, -4.5515e-02,  3.7321e-02],\n",
       "                        [-2.5139e-02, -1.3258e-02, -4.5175e-02]],\n",
       "              \n",
       "                       [[ 7.3631e-03, -1.9547e-02,  3.2623e-02],\n",
       "                        [-1.5092e-02, -3.4524e-02, -4.9035e-03],\n",
       "                        [ 4.0040e-02,  1.2490e-02,  2.0403e-02]]],\n",
       "              \n",
       "              \n",
       "                      ...,\n",
       "              \n",
       "              \n",
       "                      [[[ 3.6984e-02, -2.0591e-02, -4.6975e-02],\n",
       "                        [-2.6542e-02, -3.2333e-02, -2.5406e-02],\n",
       "                        [ 1.5322e-02,  1.3343e-02,  1.6902e-02]],\n",
       "              \n",
       "                       [[-3.1818e-02,  1.1935e-02, -5.0502e-03],\n",
       "                        [-3.9853e-02, -4.7882e-03, -3.7820e-03],\n",
       "                        [ 3.0168e-02, -2.5282e-02, -8.6032e-03]],\n",
       "              \n",
       "                       [[-3.3534e-02,  3.7965e-02, -2.2916e-02],\n",
       "                        [ 1.2038e-03,  1.8582e-02,  2.4256e-03],\n",
       "                        [-3.3708e-02,  2.8672e-02,  1.4317e-02]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 3.0922e-02, -3.2151e-02, -2.9747e-02],\n",
       "                        [-2.9906e-02, -4.8167e-02,  7.9964e-04],\n",
       "                        [ 8.3467e-03,  3.2081e-02, -2.4233e-02]],\n",
       "              \n",
       "                       [[ 7.3037e-03, -5.0963e-02,  9.6717e-03],\n",
       "                        [-4.0682e-02, -3.9269e-02, -3.1437e-02],\n",
       "                        [-2.8273e-02, -3.0570e-02, -2.0627e-02]],\n",
       "              \n",
       "                       [[-2.5096e-02,  5.7596e-02,  4.9907e-02],\n",
       "                        [ 1.9215e-03, -2.2374e-02,  1.8379e-02],\n",
       "                        [ 4.9511e-03, -1.1992e-03, -2.7076e-02]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 3.9947e-02,  1.5141e-02,  4.9483e-02],\n",
       "                        [ 3.3038e-04, -1.5589e-02, -4.0922e-02],\n",
       "                        [ 4.3919e-02,  2.3770e-02,  2.4948e-02]],\n",
       "              \n",
       "                       [[ 1.7121e-02,  2.5598e-03,  8.3033e-03],\n",
       "                        [ 1.1580e-02,  3.3504e-03, -4.2924e-02],\n",
       "                        [ 4.4929e-02,  2.6810e-02, -5.0287e-02]],\n",
       "              \n",
       "                       [[-3.0774e-02, -5.1941e-03, -2.6809e-02],\n",
       "                        [ 7.9440e-03,  4.2803e-03,  7.4273e-03],\n",
       "                        [-3.5722e-02,  3.2876e-02, -1.6934e-03]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-3.4522e-02, -2.6568e-02, -3.7610e-02],\n",
       "                        [ 1.3146e-02, -5.2393e-02,  2.5398e-02],\n",
       "                        [-8.9279e-03, -1.1769e-02, -4.5239e-03]],\n",
       "              \n",
       "                       [[-1.7614e-02, -5.6336e-03, -3.9640e-02],\n",
       "                        [ 2.1318e-02,  5.3845e-03,  2.7070e-02],\n",
       "                        [ 2.6947e-02, -4.9018e-02,  1.1368e-02]],\n",
       "              \n",
       "                       [[ 3.9494e-02,  4.5772e-02,  5.5968e-02],\n",
       "                        [-4.2530e-02, -3.9442e-02,  3.6610e-02],\n",
       "                        [-1.4311e-02,  2.7202e-02, -3.4003e-03]]],\n",
       "              \n",
       "              \n",
       "                      [[[-4.2218e-02,  1.3295e-02,  2.3946e-02],\n",
       "                        [-3.9808e-02, -3.7076e-02, -4.2027e-02],\n",
       "                        [ 4.0949e-02,  4.8737e-03, -4.7076e-02]],\n",
       "              \n",
       "                       [[-9.1843e-04,  2.6792e-03, -1.0673e-02],\n",
       "                        [ 1.1668e-02,  1.2922e-02,  1.9739e-03],\n",
       "                        [-2.8360e-02,  5.9003e-02,  8.9652e-03]],\n",
       "              \n",
       "                       [[-7.0946e-03,  3.4245e-03,  3.6592e-02],\n",
       "                        [ 5.0957e-02,  1.3877e-02, -3.7040e-02],\n",
       "                        [-2.7622e-02, -3.1315e-03, -2.8418e-02]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 1.8040e-04,  6.3471e-02,  3.3515e-02],\n",
       "                        [ 2.4226e-02,  6.7356e-02,  2.9909e-02],\n",
       "                        [ 3.0561e-02, -6.2313e-03, -2.8385e-03]],\n",
       "              \n",
       "                       [[ 1.2638e-02, -1.2752e-02, -1.9124e-02],\n",
       "                        [ 4.8601e-02,  3.4877e-02, -4.0493e-02],\n",
       "                        [-3.3395e-02, -3.8178e-03,  1.7086e-02]],\n",
       "              \n",
       "                       [[-2.0332e-03, -1.3524e-03, -2.4806e-03],\n",
       "                        [-5.9530e-02, -2.9261e-02,  1.5181e-02],\n",
       "                        [-2.4968e-03, -6.0510e-02, -2.6952e-02]]]])),\n",
       "             ('module.classifier_vae.encoder.net.3.bias',\n",
       "              tensor([ 0.0268, -0.0336,  0.0109,  0.0365, -0.0347,  0.0362, -0.0195,  0.0181,\n",
       "                       0.0297, -0.0120,  0.0399, -0.0042,  0.0223, -0.0331, -0.0264,  0.0376,\n",
       "                      -0.0076, -0.0212,  0.0186,  0.0376,  0.0033,  0.0399, -0.0243,  0.0258,\n",
       "                      -0.0418,  0.0413, -0.0268,  0.0381,  0.0284,  0.0262, -0.0063, -0.0410,\n",
       "                       0.0314,  0.0006, -0.0334,  0.0158,  0.0006,  0.0103,  0.0126, -0.0077,\n",
       "                      -0.0051,  0.0301,  0.0037, -0.0226, -0.0317,  0.0341, -0.0237,  0.0227,\n",
       "                      -0.0205,  0.0318,  0.0003, -0.0403, -0.0343, -0.0394, -0.0315, -0.0242,\n",
       "                       0.0038, -0.0081,  0.0425,  0.0092, -0.0038,  0.0135,  0.0123,  0.0353])),\n",
       "             ('module.classifier_vae.encoder.net.4.weight',\n",
       "              tensor([1.0000, 0.9610, 1.0294, 1.0410, 0.9821, 0.9954, 0.9826, 1.0187, 0.9317,\n",
       "                      1.1103, 0.9710, 0.9927, 1.0832, 1.0461, 1.0045, 0.9668, 1.0275, 0.9630,\n",
       "                      0.9734, 0.9801, 1.0106, 1.0685, 1.1103, 0.9833, 0.9613, 0.9827, 1.0422,\n",
       "                      0.9859, 1.0457, 1.0159, 1.0381, 0.9610, 1.0037, 1.0042, 1.0156, 0.9841,\n",
       "                      0.9872, 1.0599, 0.9938, 0.9592, 0.9725, 1.0020, 1.0292, 1.0367, 1.0441,\n",
       "                      1.0414, 1.0472, 1.0568, 0.9763, 1.0023, 0.9730, 0.9607, 0.9657, 1.0841,\n",
       "                      1.0418, 0.9957, 0.9653, 0.9793, 0.9758, 0.9725, 1.0562, 1.0263, 1.0275,\n",
       "                      0.9624])),\n",
       "             ('module.classifier_vae.encoder.net.4.bias',\n",
       "              tensor([ 0.0388, -0.0194,  0.0577,  0.0983, -0.0221, -0.0108, -0.0216,  0.0664,\n",
       "                      -0.0425,  0.1211, -0.0148,  0.0079,  0.1027,  0.1388,  0.0180, -0.0510,\n",
       "                       0.0583, -0.0110, -0.0311, -0.0455,  0.0416,  0.1605,  0.1214,  0.0109,\n",
       "                      -0.0010,  0.0100,  0.0726, -0.0401,  0.0927,  0.0879,  0.1000, -0.0381,\n",
       "                      -0.0254, -0.0065,  0.0057, -0.0385, -0.0127,  0.0942,  0.0004,  0.0134,\n",
       "                      -0.0412,  0.0185,  0.0514,  0.0799,  0.0799,  0.0820,  0.0965,  0.1198,\n",
       "                      -0.0288,  0.0413,  0.0207, -0.0067, -0.0244,  0.0984,  0.0725, -0.0437,\n",
       "                      -0.0250, -0.0334, -0.0260,  0.0095,  0.0896,  0.0205,  0.0906, -0.0457])),\n",
       "             ('module.classifier_vae.encoder.net.4.running_mean',\n",
       "              tensor([-0.5863,  0.6010, -0.6253,  0.4268,  1.0373,  0.7837,  0.5875, -0.4412,\n",
       "                       0.6306, -0.5084,  0.7962,  0.9004, -0.1652, -0.8066,  0.7524,  0.5030,\n",
       "                      -0.4147,  0.4852,  0.8894,  0.5188,  1.0848,  0.1578,  0.0280,  0.8291,\n",
       "                       0.9078, -0.6129,  0.4818,  0.3150, -0.6172, -0.5148, -0.6298,  0.4798,\n",
       "                       0.9820,  0.1515, -0.9294,  0.6743,  0.7268, -0.6206,  0.7613,  0.8835,\n",
       "                       0.4173, -0.6160, -0.5879,  0.8556, -0.4449,  0.0160, -0.1409, -0.0810,\n",
       "                       0.1776, -0.5240, -0.6103,  0.4855,  0.4254,  0.4513, -0.3330,  0.7281,\n",
       "                       0.6420,  0.4868,  0.9044,  0.5412, -0.4379,  0.3924,  0.6850,  0.6429])),\n",
       "             ('module.classifier_vae.encoder.net.4.running_var',\n",
       "              tensor([2.6257, 6.8779, 1.1419, 0.7472, 8.8378, 6.2005, 9.2008, 1.0274, 5.5161,\n",
       "                      1.4972, 3.9786, 3.2711, 1.0776, 2.3159, 7.8683, 6.6355, 1.0769, 6.5919,\n",
       "                      7.0127, 5.3401, 4.4037, 2.4344, 1.1346, 8.1882, 6.1073, 1.2496, 1.6763,\n",
       "                      4.1786, 3.7972, 1.6842, 1.9771, 4.7901, 7.2982, 1.8106, 3.9437, 5.8496,\n",
       "                      8.3941, 1.9083, 8.4847, 4.1710, 6.2449, 2.0961, 2.4850, 0.9201, 1.6264,\n",
       "                      1.0363, 1.7443, 1.4802, 5.2939, 1.2418, 1.3196, 3.4646, 5.8154, 1.2857,\n",
       "                      1.2491, 9.0343, 6.5077, 4.6646, 5.2784, 1.2696, 2.6091, 2.2378, 0.9950,\n",
       "                      5.3726])),\n",
       "             ('module.classifier_vae.encoder.net.4.num_batches_tracked',\n",
       "              tensor(0)),\n",
       "             ('module.classifier_vae.encoder.net.6.weight',\n",
       "              tensor([[[[-0.0324,  0.0258,  0.0045],\n",
       "                        [ 0.0426, -0.0065, -0.0443],\n",
       "                        [ 0.0274,  0.0016, -0.0395]],\n",
       "              \n",
       "                       [[ 0.0555, -0.0105, -0.0335],\n",
       "                        [ 0.0541,  0.0415,  0.0060],\n",
       "                        [-0.0246, -0.0175, -0.0232]],\n",
       "              \n",
       "                       [[ 0.0304, -0.0062,  0.0219],\n",
       "                        [-0.0197, -0.0075, -0.0426],\n",
       "                        [ 0.0028,  0.0090,  0.0119]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-0.0234,  0.0277,  0.0045],\n",
       "                        [ 0.0124,  0.0099, -0.0151],\n",
       "                        [ 0.0485, -0.0352, -0.0439]],\n",
       "              \n",
       "                       [[ 0.0335,  0.0341, -0.0567],\n",
       "                        [ 0.0123, -0.0184,  0.0394],\n",
       "                        [ 0.0541, -0.0236, -0.0027]],\n",
       "              \n",
       "                       [[ 0.0074,  0.0562,  0.0171],\n",
       "                        [-0.0095,  0.0098,  0.0334],\n",
       "                        [ 0.0701,  0.0511,  0.0255]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.0396, -0.0084, -0.0280],\n",
       "                        [-0.0134,  0.0115,  0.0101],\n",
       "                        [-0.0406,  0.0050, -0.0208]],\n",
       "              \n",
       "                       [[-0.0187, -0.0228, -0.0343],\n",
       "                        [ 0.0313,  0.0568, -0.0399],\n",
       "                        [-0.0031,  0.0410,  0.0126]],\n",
       "              \n",
       "                       [[ 0.0302,  0.0242, -0.0391],\n",
       "                        [-0.0468,  0.0518, -0.0015],\n",
       "                        [ 0.0195,  0.0082,  0.0149]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-0.0248, -0.0422,  0.0081],\n",
       "                        [ 0.0084, -0.0005, -0.0491],\n",
       "                        [-0.0161,  0.0250, -0.0031]],\n",
       "              \n",
       "                       [[-0.0298,  0.0218, -0.0282],\n",
       "                        [ 0.0221, -0.0356,  0.0172],\n",
       "                        [ 0.0226, -0.0012,  0.0235]],\n",
       "              \n",
       "                       [[-0.0327, -0.0152, -0.0120],\n",
       "                        [ 0.0443,  0.0335, -0.0053],\n",
       "                        [ 0.0261,  0.0043,  0.0238]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.0358, -0.0385, -0.0532],\n",
       "                        [ 0.0144,  0.0147, -0.0062],\n",
       "                        [ 0.0124, -0.0219, -0.0110]],\n",
       "              \n",
       "                       [[-0.0075, -0.0154, -0.0235],\n",
       "                        [ 0.0085,  0.0208, -0.0064],\n",
       "                        [-0.0162,  0.0048, -0.0216]],\n",
       "              \n",
       "                       [[-0.0328, -0.0020,  0.0220],\n",
       "                        [-0.0153, -0.0438, -0.0227],\n",
       "                        [ 0.0017, -0.0111, -0.0449]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 0.0441, -0.0157, -0.0158],\n",
       "                        [-0.0159, -0.0296,  0.0173],\n",
       "                        [ 0.0291,  0.0192, -0.0227]],\n",
       "              \n",
       "                       [[-0.0237,  0.0260,  0.0042],\n",
       "                        [ 0.0435,  0.0370, -0.0247],\n",
       "                        [-0.0082, -0.0032,  0.0078]],\n",
       "              \n",
       "                       [[-0.0116,  0.0067, -0.0310],\n",
       "                        [-0.0045, -0.0148, -0.0564],\n",
       "                        [ 0.0081, -0.0469, -0.0568]]],\n",
       "              \n",
       "              \n",
       "                      ...,\n",
       "              \n",
       "              \n",
       "                      [[[ 0.0173,  0.0081,  0.0624],\n",
       "                        [ 0.0131, -0.0140,  0.0550],\n",
       "                        [ 0.0398,  0.0052, -0.0218]],\n",
       "              \n",
       "                       [[-0.0561,  0.0207, -0.0358],\n",
       "                        [-0.0159,  0.0161,  0.0371],\n",
       "                        [ 0.0041,  0.0010,  0.0172]],\n",
       "              \n",
       "                       [[ 0.0186,  0.0246,  0.0727],\n",
       "                        [-0.0214, -0.0048, -0.0084],\n",
       "                        [ 0.0558,  0.0175,  0.0091]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 0.0053, -0.0487,  0.0514],\n",
       "                        [ 0.0440,  0.0051,  0.0524],\n",
       "                        [ 0.0074, -0.0058,  0.0109]],\n",
       "              \n",
       "                       [[ 0.0367, -0.0433,  0.0448],\n",
       "                        [-0.0161, -0.0240,  0.0078],\n",
       "                        [-0.0135, -0.0396, -0.0135]],\n",
       "              \n",
       "                       [[-0.0118, -0.0279, -0.0113],\n",
       "                        [ 0.0213,  0.0366,  0.0300],\n",
       "                        [-0.0029,  0.0229,  0.0067]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.0228,  0.0145, -0.0217],\n",
       "                        [ 0.0169,  0.0248, -0.0204],\n",
       "                        [ 0.0300, -0.0200, -0.0122]],\n",
       "              \n",
       "                       [[-0.0404, -0.0506,  0.0208],\n",
       "                        [-0.0500, -0.0181,  0.0418],\n",
       "                        [ 0.0274, -0.0078,  0.0112]],\n",
       "              \n",
       "                       [[ 0.0174,  0.0141, -0.0235],\n",
       "                        [-0.0301,  0.0118,  0.0098],\n",
       "                        [ 0.0067,  0.0167, -0.0147]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 0.0144,  0.0264,  0.0561],\n",
       "                        [-0.0023, -0.0472,  0.0065],\n",
       "                        [-0.0210,  0.0285,  0.0471]],\n",
       "              \n",
       "                       [[ 0.0141,  0.0664,  0.0129],\n",
       "                        [ 0.0049, -0.0319, -0.0481],\n",
       "                        [ 0.0042,  0.0337,  0.0263]],\n",
       "              \n",
       "                       [[ 0.0234, -0.0457,  0.0027],\n",
       "                        [-0.0035,  0.0212,  0.0237],\n",
       "                        [-0.0088,  0.0287,  0.0487]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.0342, -0.0294, -0.0224],\n",
       "                        [-0.0258, -0.0074, -0.0500],\n",
       "                        [-0.0168, -0.0598,  0.0163]],\n",
       "              \n",
       "                       [[-0.0291, -0.0173, -0.0532],\n",
       "                        [ 0.0062, -0.0103, -0.0384],\n",
       "                        [ 0.0114, -0.0093, -0.0235]],\n",
       "              \n",
       "                       [[ 0.0575,  0.0143, -0.0053],\n",
       "                        [ 0.0276,  0.0250,  0.0103],\n",
       "                        [ 0.0363, -0.0112,  0.0108]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 0.0317, -0.0151,  0.0071],\n",
       "                        [-0.0156,  0.0194, -0.0275],\n",
       "                        [-0.0242,  0.0050,  0.0308]],\n",
       "              \n",
       "                       [[ 0.0063,  0.0028,  0.0463],\n",
       "                        [ 0.0329,  0.0267, -0.0006],\n",
       "                        [-0.0573, -0.0150, -0.0120]],\n",
       "              \n",
       "                       [[ 0.0336, -0.0240, -0.0092],\n",
       "                        [-0.0501, -0.0361,  0.0094],\n",
       "                        [ 0.0186,  0.0174, -0.0529]]]])),\n",
       "             ('module.classifier_vae.encoder.net.6.bias',\n",
       "              tensor([-0.0063, -0.0103, -0.0093, -0.0308, -0.0424, -0.0389,  0.0181, -0.0043,\n",
       "                      -0.0202, -0.0195, -0.0306, -0.0105, -0.0041, -0.0353, -0.0128, -0.0349,\n",
       "                      -0.0025, -0.0219, -0.0325,  0.0205, -0.0190,  0.0165, -0.0178,  0.0344,\n",
       "                       0.0009,  0.0124, -0.0295,  0.0308, -0.0166, -0.0397, -0.0112,  0.0363,\n",
       "                      -0.0219,  0.0438, -0.0113,  0.0191,  0.0283, -0.0353, -0.0024, -0.0324,\n",
       "                      -0.0166, -0.0350,  0.0252,  0.0466,  0.0329, -0.0261, -0.0221,  0.0211,\n",
       "                       0.0190,  0.0083, -0.0044, -0.0366, -0.0239,  0.0083,  0.0286,  0.0169,\n",
       "                       0.0026,  0.0016, -0.0225, -0.0048,  0.0345,  0.0045,  0.0417, -0.0340])),\n",
       "             ('module.classifier_vae.encoder.net.7.weight',\n",
       "              tensor([0.9499, 1.0419, 1.0387, 0.9943, 1.0262, 0.9737, 0.9796, 0.9579, 1.0648,\n",
       "                      1.0050, 1.0360, 1.0455, 0.9907, 1.0206, 0.9894, 1.0156, 1.0098, 0.9564,\n",
       "                      1.0012, 0.9803, 0.9296, 1.0121, 1.0109, 1.0116, 1.0045, 1.0205, 1.0651,\n",
       "                      0.9230, 0.9772, 1.0373, 0.9607, 0.9542, 1.0245, 1.0558, 1.0060, 1.0582,\n",
       "                      1.0053, 1.0077, 1.0299, 1.0000, 0.9831, 1.0566, 1.0148, 1.0231, 0.9884,\n",
       "                      0.9840, 0.9452, 1.0136, 0.9281, 1.0809, 0.9824, 0.9286, 1.0265, 1.0007,\n",
       "                      0.9654, 1.0421, 0.9762, 1.0099, 1.0132, 0.9990, 1.0236, 0.9698, 0.9960,\n",
       "                      1.0356])),\n",
       "             ('module.classifier_vae.encoder.net.7.bias',\n",
       "              tensor([-0.0532,  0.0075,  0.0888, -0.0247,  0.0800, -0.0589, -0.0670, -0.0603,\n",
       "                       0.1213, -0.0044,  0.0174,  0.0501,  0.0005,  0.0493, -0.0304,  0.0077,\n",
       "                       0.0164, -0.0295,  0.0089, -0.0678, -0.1122,  0.0505, -0.0668,  0.0152,\n",
       "                      -0.0076,  0.0305,  0.0267, -0.0531, -0.0448,  0.0004, -0.0650, -0.1229,\n",
       "                       0.0455, -0.0106,  0.0519,  0.0703, -0.0029,  0.0925,  0.0628,  0.0278,\n",
       "                      -0.0416,  0.1093,  0.0298,  0.0426, -0.0654, -0.0379, -0.0498,  0.0589,\n",
       "                      -0.0512,  0.0028, -0.0726, -0.0532,  0.0242, -0.0065, -0.0400,  0.0728,\n",
       "                      -0.1230,  0.0267,  0.0146, -0.0678,  0.0171,  0.0148,  0.0043,  0.0656])),\n",
       "             ('module.classifier_vae.encoder.net.7.running_mean',\n",
       "              tensor([ 1.0270, -1.7850, -0.9128,  2.4480, -0.9321,  1.8941, -0.1612,  1.4864,\n",
       "                       1.8730, -0.6621,  1.2055, -1.9276, -0.2846, -0.9581, -0.1514,  0.0081,\n",
       "                      -1.2305,  2.0258, -0.3364,  1.4749,  0.9271, -1.1947,  0.4415,  0.4734,\n",
       "                      -2.4277,  2.5869, -0.8779,  0.1925,  0.1830, -0.5972,  1.7134,  0.6843,\n",
       "                      -2.2437, -0.6269, -0.1757,  2.7869, -1.0050, -0.9724, -1.2598,  1.3765,\n",
       "                       0.7283, -0.8357, -1.0317,  0.8124,  1.2485, -0.0935,  1.7265, -0.1175,\n",
       "                       1.8017, -1.0585,  0.7850,  0.8046, -1.1605, -0.6317, -0.2130,  0.3102,\n",
       "                       1.1969, -1.3388, -0.6094,  0.8987, -1.3020,  1.8184,  1.0788, -0.4926])),\n",
       "             ('module.classifier_vae.encoder.net.7.running_var',\n",
       "              tensor([ 3.3555, 13.4198,  1.4319,  3.2735,  2.4771,  4.8738,  2.1200,  2.9964,\n",
       "                      17.5149,  6.1499,  3.0951,  4.4963,  2.6367,  1.3412,  3.5065,  2.0475,\n",
       "                       2.3142,  3.9774,  1.4176,  5.2176,  2.8306,  1.3745,  3.7232,  1.4617,\n",
       "                       2.9625,  2.8527,  1.2801,  1.5137,  2.5080,  2.3078,  3.9315,  3.6997,\n",
       "                      15.6799,  3.6245,  1.9230, 12.6915,  5.5172,  2.7783, 26.0979,  3.2776,\n",
       "                       3.6605,  8.5525,  1.7271,  7.7182,  3.7999,  3.7153,  2.7899,  3.2570,\n",
       "                       3.2084,  2.3757,  2.6057,  2.0723,  3.4505,  1.3213,  2.3170,  3.2244,\n",
       "                       3.1959,  2.8449,  1.7634,  2.6120,  7.9089,  3.1438,  2.1330,  1.6073])),\n",
       "             ('module.classifier_vae.encoder.net.7.num_batches_tracked',\n",
       "              tensor(0)),\n",
       "             ('module.classifier_vae.encoder.net.9.weight',\n",
       "              tensor([[[[-0.0198,  0.0010,  0.0342],\n",
       "                        [-0.0262, -0.0157, -0.0307],\n",
       "                        [ 0.0080, -0.0403,  0.0274]],\n",
       "              \n",
       "                       [[-0.0358, -0.0028,  0.0257],\n",
       "                        [ 0.0094,  0.0286,  0.0402],\n",
       "                        [ 0.0241, -0.0244, -0.0286]],\n",
       "              \n",
       "                       [[-0.0695, -0.0227,  0.0310],\n",
       "                        [-0.0415, -0.0344, -0.0342],\n",
       "                        [-0.0144, -0.0332,  0.0443]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-0.0370, -0.0062,  0.0064],\n",
       "                        [-0.0319,  0.0029, -0.0143],\n",
       "                        [ 0.0338, -0.0141,  0.0180]],\n",
       "              \n",
       "                       [[-0.0494,  0.0086,  0.0284],\n",
       "                        [-0.0420,  0.0137,  0.0371],\n",
       "                        [ 0.0196, -0.0591,  0.0173]],\n",
       "              \n",
       "                       [[-0.0306,  0.0441,  0.0014],\n",
       "                        [ 0.0222, -0.0115, -0.0422],\n",
       "                        [ 0.0071, -0.0615, -0.0055]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.0004, -0.0180,  0.0132],\n",
       "                        [-0.0071, -0.0294,  0.0271],\n",
       "                        [ 0.0050,  0.0369, -0.0271]],\n",
       "              \n",
       "                       [[ 0.0214, -0.0017,  0.0367],\n",
       "                        [ 0.0188,  0.0109, -0.0042],\n",
       "                        [-0.0101, -0.0532, -0.0320]],\n",
       "              \n",
       "                       [[ 0.0094, -0.0206, -0.0671],\n",
       "                        [-0.0077,  0.0089, -0.0327],\n",
       "                        [-0.0500,  0.0341,  0.0273]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 0.0011, -0.0184,  0.0173],\n",
       "                        [-0.0259,  0.0229,  0.0145],\n",
       "                        [-0.0127, -0.0173,  0.0199]],\n",
       "              \n",
       "                       [[ 0.0215, -0.0522, -0.0148],\n",
       "                        [-0.0214, -0.0346, -0.0040],\n",
       "                        [ 0.0276, -0.0403,  0.0105]],\n",
       "              \n",
       "                       [[-0.0413, -0.0289, -0.0676],\n",
       "                        [-0.0551, -0.0090, -0.0525],\n",
       "                        [-0.0300,  0.0173, -0.0256]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.0145,  0.0186,  0.0370],\n",
       "                        [-0.0119, -0.0242,  0.0150],\n",
       "                        [ 0.0180, -0.0129, -0.0296]],\n",
       "              \n",
       "                       [[ 0.0146, -0.0218, -0.0428],\n",
       "                        [-0.0015,  0.0077,  0.0059],\n",
       "                        [-0.0408,  0.0077, -0.0192]],\n",
       "              \n",
       "                       [[-0.0082, -0.0232,  0.0108],\n",
       "                        [-0.0583, -0.0006, -0.0443],\n",
       "                        [-0.0060, -0.0393, -0.0066]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-0.0411,  0.0098,  0.0236],\n",
       "                        [-0.0263,  0.0215, -0.0397],\n",
       "                        [ 0.0175,  0.0107, -0.0048]],\n",
       "              \n",
       "                       [[ 0.0022, -0.0103, -0.0209],\n",
       "                        [-0.0374, -0.0334,  0.0112],\n",
       "                        [ 0.0127, -0.0393,  0.0565]],\n",
       "              \n",
       "                       [[ 0.0413,  0.0112,  0.0096],\n",
       "                        [-0.0252, -0.0483, -0.0326],\n",
       "                        [-0.0345,  0.0083, -0.0413]]],\n",
       "              \n",
       "              \n",
       "                      ...,\n",
       "              \n",
       "              \n",
       "                      [[[-0.0094,  0.0097,  0.0146],\n",
       "                        [-0.0131, -0.0428, -0.0212],\n",
       "                        [-0.0301,  0.0313,  0.0435]],\n",
       "              \n",
       "                       [[ 0.0180,  0.0006,  0.0242],\n",
       "                        [-0.0049, -0.0380,  0.0156],\n",
       "                        [ 0.0627,  0.0088,  0.0324]],\n",
       "              \n",
       "                       [[-0.0099, -0.0291, -0.0540],\n",
       "                        [ 0.0053,  0.0137, -0.0070],\n",
       "                        [-0.0544, -0.0585, -0.0421]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-0.0529, -0.0013,  0.0149],\n",
       "                        [ 0.0224, -0.0402,  0.0260],\n",
       "                        [-0.0486,  0.0501,  0.0074]],\n",
       "              \n",
       "                       [[-0.0627, -0.0303,  0.0296],\n",
       "                        [-0.0044, -0.0205, -0.0332],\n",
       "                        [ 0.0421, -0.0027,  0.0387]],\n",
       "              \n",
       "                       [[-0.0089, -0.0082, -0.0195],\n",
       "                        [-0.0051,  0.0066, -0.0030],\n",
       "                        [-0.0057, -0.0114,  0.0118]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.0376, -0.0134,  0.0138],\n",
       "                        [-0.0437,  0.0011, -0.0047],\n",
       "                        [ 0.0359, -0.0130,  0.0240]],\n",
       "              \n",
       "                       [[-0.0163, -0.0152,  0.0443],\n",
       "                        [-0.0042,  0.0490, -0.0053],\n",
       "                        [-0.0316, -0.0375, -0.0458]],\n",
       "              \n",
       "                       [[ 0.0167, -0.0171,  0.0107],\n",
       "                        [-0.0363,  0.0325, -0.0179],\n",
       "                        [-0.0167, -0.0808, -0.0201]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-0.0432, -0.0537, -0.0401],\n",
       "                        [ 0.0613,  0.0080, -0.0401],\n",
       "                        [-0.0241, -0.0227,  0.0219]],\n",
       "              \n",
       "                       [[-0.0146,  0.0153, -0.0280],\n",
       "                        [-0.0497, -0.0108,  0.0015],\n",
       "                        [-0.0185, -0.0451, -0.0288]],\n",
       "              \n",
       "                       [[-0.0323, -0.0192,  0.0063],\n",
       "                        [-0.0093,  0.0023, -0.0025],\n",
       "                        [-0.0427, -0.0217, -0.0453]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.0509, -0.0237, -0.0209],\n",
       "                        [-0.0142, -0.0141, -0.0391],\n",
       "                        [-0.0274, -0.0265, -0.0094]],\n",
       "              \n",
       "                       [[-0.0370, -0.0089, -0.0159],\n",
       "                        [ 0.0139, -0.0026,  0.0042],\n",
       "                        [-0.0708,  0.0069,  0.0315]],\n",
       "              \n",
       "                       [[ 0.0114,  0.0114, -0.0302],\n",
       "                        [ 0.0582, -0.0197,  0.0268],\n",
       "                        [ 0.0010, -0.0002,  0.0303]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-0.0096, -0.0672, -0.0122],\n",
       "                        [ 0.0210, -0.0214,  0.0285],\n",
       "                        [-0.0016,  0.0102, -0.0278]],\n",
       "              \n",
       "                       [[-0.0565,  0.0044,  0.0195],\n",
       "                        [-0.0333,  0.0018,  0.0030],\n",
       "                        [ 0.0094, -0.0513,  0.0266]],\n",
       "              \n",
       "                       [[-0.0140,  0.0185, -0.0473],\n",
       "                        [ 0.0487,  0.0059,  0.0261],\n",
       "                        [ 0.0296, -0.0135, -0.0169]]]])),\n",
       "             ('module.classifier_vae.encoder.net.9.bias',\n",
       "              tensor([ 0.0496, -0.0151, -0.0151,  0.0331,  0.0426,  0.0403,  0.0153,  0.0218,\n",
       "                       0.0003,  0.0013,  0.0089,  0.0063,  0.0393,  0.0298, -0.0055, -0.0333,\n",
       "                       0.0085, -0.0218,  0.0068,  0.0215,  0.0381,  0.0101, -0.0323,  0.0157,\n",
       "                      -0.0383, -0.0168,  0.0100,  0.0253,  0.0037,  0.0067,  0.0164, -0.0070,\n",
       "                       0.0070,  0.0312, -0.0168,  0.0389,  0.0098,  0.0144, -0.0261,  0.0039,\n",
       "                      -0.0271,  0.0033, -0.0272, -0.0327, -0.0241,  0.0312,  0.0121,  0.0091,\n",
       "                       0.0167, -0.0218,  0.0270,  0.0017, -0.0284,  0.0349, -0.0093, -0.0176,\n",
       "                      -0.0029,  0.0144,  0.0016,  0.0389, -0.0052,  0.0404, -0.0395,  0.0295])),\n",
       "             ('module.classifier_vae.encoder.net.10.weight',\n",
       "              tensor([0.9406, 0.9260, 0.9365, 0.9663, 0.9499, 0.8850, 0.8832, 0.9242, 0.9294,\n",
       "                      0.8787, 0.9049, 0.8394, 0.9070, 0.8977, 0.9437, 0.8460, 0.9222, 0.9682,\n",
       "                      0.9662, 0.8494, 0.9361, 0.9397, 0.9384, 0.9601, 0.9586, 0.9676, 0.9167,\n",
       "                      0.9153, 0.9629, 0.9385, 0.9383, 0.8903, 0.9862, 0.9054, 0.9149, 0.9214,\n",
       "                      0.9345, 0.8849, 0.9025, 0.9243, 0.9358, 0.9189, 0.9752, 0.9164, 0.9695,\n",
       "                      0.9323, 0.9070, 0.9004, 0.9178, 0.8971, 0.8677, 0.9225, 0.9490, 0.9645,\n",
       "                      0.9144, 0.9560, 0.8851, 0.9753, 0.9210, 0.9282, 0.8698, 0.9324, 0.9380,\n",
       "                      0.9419])),\n",
       "             ('module.classifier_vae.encoder.net.10.bias',\n",
       "              tensor([-0.1211, -0.1215, -0.1271, -0.1036, -0.0881, -0.1351, -0.1657, -0.1272,\n",
       "                      -0.1348, -0.1149, -0.1360, -0.1539, -0.1468, -0.1977, -0.0840, -0.1696,\n",
       "                      -0.1559, -0.0761, -0.1299, -0.1412, -0.1241, -0.1036, -0.0889, -0.1062,\n",
       "                      -0.0897, -0.1312, -0.1173, -0.0637, -0.0485, -0.1441, -0.0989, -0.1370,\n",
       "                       0.0343, -0.1287, -0.0935, -0.0959, -0.1296, -0.1422, -0.1163, -0.0800,\n",
       "                      -0.0382, -0.1061, -0.1127, -0.1127, -0.0748, -0.0607, -0.0872, -0.1168,\n",
       "                      -0.1683, -0.1503, -0.1404, -0.1119, -0.0788, -0.0772, -0.0946, -0.1390,\n",
       "                      -0.0660, -0.0861, -0.1103, -0.1145, -0.1314, -0.1313, -0.1441, -0.0696])),\n",
       "             ('module.classifier_vae.encoder.net.10.running_mean',\n",
       "              tensor([-0.9709, -1.2705, -1.5923, -1.5038, -1.2405, -0.1593, -1.2530, -1.0092,\n",
       "                      -1.4260,  1.8047, -1.3789,  0.8524, -1.1706,  1.7571, -1.6197, -1.0391,\n",
       "                      -1.5169, -1.6855, -1.5285,  1.3478, -1.0405,  0.1156, -1.7942, -2.1362,\n",
       "                      -1.6945, -0.8686, -1.0981, -1.5449, -2.1787, -1.8690, -1.3808, -1.2254,\n",
       "                      -0.0597, -0.8251, -1.1324, -1.0131, -1.0150, -1.1197, -1.0986,  2.0170,\n",
       "                       2.0313,  1.9376, -3.9178, -1.1655, -1.1572,  2.4706,  2.4899,  0.5093,\n",
       "                      -1.1226, -0.6200,  1.5280, -1.0906,  0.2810, -1.4050, -1.1378, -1.3597,\n",
       "                       0.7162, -2.0462, -1.0633, -0.8796, -1.1497, -0.8675, -1.8210, -1.6641])),\n",
       "             ('module.classifier_vae.encoder.net.10.running_var',\n",
       "              tensor([  8.9688,   7.0547,   7.5193,  11.7601,   8.1702,   3.6961,   4.2037,\n",
       "                        5.3026,  10.1226,  20.1045,   6.3147,   1.9227,   9.6011,   3.7531,\n",
       "                        5.8982,   6.4742,   7.1384,   8.9820,   7.6041,   2.5522,   8.1121,\n",
       "                       10.4693,  10.5124,   8.0104,   7.1667,   8.4233,   4.1798,   4.1419,\n",
       "                        7.6375,   6.2617,  10.3243,   6.0817,  11.5333,   7.6620,   5.3963,\n",
       "                        5.7178,   9.1931,   8.6440,  10.1563,  59.4668,  27.6794,   4.1744,\n",
       "                      109.2267,   6.2814,   8.0222,   6.9055,  14.1214,   5.8602,  12.7033,\n",
       "                        4.5421,   3.1452,   7.4209,  22.0698,  11.8598,   9.1758,   6.3141,\n",
       "                       23.7897,   4.1763,   7.9652,   6.0092,   2.6904,   8.1948,   7.3220,\n",
       "                        8.6956])),\n",
       "             ('module.classifier_vae.encoder.net.10.num_batches_tracked',\n",
       "              tensor(0)),\n",
       "             ('module.classifier_vae.encoder.h1.weight',\n",
       "              tensor([[-0.0559, -0.0799,  0.0417,  ..., -0.0518,  0.0375, -0.0403],\n",
       "                      [ 0.0085,  0.0484,  0.0143,  ...,  0.1222, -0.0080,  0.0423],\n",
       "                      [-0.0147, -0.0701, -0.0145,  ..., -0.0991,  0.0420,  0.0283],\n",
       "                      ...,\n",
       "                      [-0.0043, -0.0128,  0.0322,  ..., -0.0253, -0.0306,  0.0211],\n",
       "                      [-0.0410, -0.0300,  0.0997,  ..., -0.0195,  0.0694,  0.0383],\n",
       "                      [ 0.0161, -0.0667,  0.0406,  ..., -0.0175, -0.0200, -0.0108]])),\n",
       "             ('module.classifier_vae.encoder.h1.bias',\n",
       "              tensor([-0.0286,  0.0311, -0.0147,  0.0690,  0.0293, -0.0308,  0.0166,  0.0846,\n",
       "                      -0.0308, -0.0474,  0.0201,  0.0176, -0.0546, -0.0570,  0.0264,  0.0441,\n",
       "                       0.0108, -0.0327,  0.0048, -0.0849, -0.0219, -0.0786,  0.0208,  0.0084,\n",
       "                      -0.0807, -0.0107,  0.0868, -0.0176,  0.0586,  0.0520,  0.0914,  0.0668,\n",
       "                       0.0704,  0.0039, -0.0141, -0.0466, -0.0165,  0.0028, -0.0368,  0.0622,\n",
       "                      -0.0678,  0.0182,  0.0477, -0.0095, -0.0182, -0.0337, -0.0484, -0.0673,\n",
       "                      -0.0879, -0.0028, -0.0074, -0.0368, -0.0085,  0.0428,  0.0145,  0.0770,\n",
       "                       0.1130, -0.0202, -0.0312,  0.0665, -0.0086, -0.0417,  0.0163, -0.0533])),\n",
       "             ('module.classifier_vae.encoder.h2.weight',\n",
       "              tensor([[-0.0255,  0.0414,  0.0397,  ...,  0.0807, -0.0071, -0.0601],\n",
       "                      [ 0.0682, -0.0425, -0.0423,  ..., -0.0545, -0.0180, -0.0193],\n",
       "                      [ 0.0272, -0.0088, -0.0435,  ..., -0.0253,  0.0260, -0.0633],\n",
       "                      ...,\n",
       "                      [ 0.0265,  0.0099, -0.0304,  ...,  0.0165, -0.0032, -0.0550],\n",
       "                      [ 0.0482,  0.0346,  0.0530,  ...,  0.0183, -0.0183,  0.0488],\n",
       "                      [ 0.0064,  0.0363, -0.0184,  ..., -0.0395,  0.0147,  0.0381]])),\n",
       "             ('module.classifier_vae.encoder.h2.bias',\n",
       "              tensor([-0.0003,  0.0074,  0.0312, -0.0218, -0.0425, -0.0277,  0.0025,  0.0232,\n",
       "                      -0.0319,  0.0059, -0.0052, -0.0302,  0.0331,  0.0137,  0.0301,  0.0128,\n",
       "                      -0.0083,  0.0255,  0.0122, -0.0233, -0.0416, -0.0208, -0.0237,  0.0207,\n",
       "                       0.0242, -0.0158, -0.0627,  0.0027,  0.0176, -0.0476, -0.0792, -0.0051,\n",
       "                      -0.0488,  0.0152, -0.0128, -0.0016, -0.0439, -0.0264, -0.0209, -0.0370,\n",
       "                      -0.0277, -0.0109,  0.0238, -0.0502, -0.0132, -0.0265, -0.0124, -0.0426,\n",
       "                       0.0028, -0.0094, -0.0077, -0.0471,  0.0269, -0.0254,  0.0409, -0.0362,\n",
       "                      -0.0550,  0.0360,  0.0125, -0.0499, -0.0286, -0.0251, -0.0209, -0.0316])),\n",
       "             ('module.classifier_vae.classifier.0.weight',\n",
       "              tensor([[ 0.1481, -0.1034,  0.2199,  ...,  0.1460,  0.1145,  0.0980],\n",
       "                      [ 0.0531,  0.1875, -0.1960,  ..., -0.1304, -0.0101, -0.1187],\n",
       "                      [-0.0125, -0.0228, -0.0950,  ..., -0.0728,  0.0272, -0.0314],\n",
       "                      ...,\n",
       "                      [ 0.0696, -0.0083,  0.0289,  ...,  0.1064,  0.0277, -0.0757],\n",
       "                      [-0.0559, -0.0307,  0.0359,  ..., -0.0531, -0.1327,  0.0690],\n",
       "                      [ 0.0981, -0.0259, -0.0375,  ...,  0.0096, -0.0518,  0.0534]])),\n",
       "             ('module.classifier_vae.classifier.0.bias',\n",
       "              tensor([ 0.4160,  0.5034,  0.1216, -0.0992,  0.2928,  0.1139,  0.0349,  0.5032,\n",
       "                       0.0237,  0.0635,  0.1119,  0.1817, -0.0858,  0.1437,  0.1180,  0.1373,\n",
       "                      -0.0251, -0.0772,  0.0113,  0.0799,  0.2099,  0.1319,  0.4262, -0.1027,\n",
       "                       0.2638,  0.2372,  0.4965,  0.3632,  0.1761, -0.0283,  0.4650, -0.0200])),\n",
       "             ('module.classifier_vae.classifier.2.weight',\n",
       "              tensor([[ 4.1989e-02,  2.1426e-03, -7.9198e-03,  6.6192e-02, -8.6003e-02,\n",
       "                       -3.8665e-02, -2.9936e-02,  2.3778e-02, -1.4696e-02,  2.5076e-03,\n",
       "                        1.2046e-01,  2.1181e-02, -4.3225e-02, -3.2561e-02, -3.4581e-02,\n",
       "                       -3.8555e-02, -2.9432e-02, -1.9723e-02, -3.8767e-05, -2.1103e-02,\n",
       "                       -3.1692e-03,  4.9076e-02, -2.6703e-03,  7.1650e-02,  4.0365e-02,\n",
       "                        7.6883e-02, -1.9526e-02, -2.2983e-02,  3.8891e-02,  7.1743e-02,\n",
       "                       -1.7695e-02, -3.4836e-02],\n",
       "                      [ 3.7834e-02,  1.7231e-02,  2.7625e-02,  2.5003e-02, -6.1510e-02,\n",
       "                       -5.1137e-02, -3.5266e-02,  1.5047e-02, -2.2592e-02, -7.3782e-03,\n",
       "                        1.0817e-01,  4.4241e-02, -2.1250e-02, -1.1683e-02,  2.6089e-02,\n",
       "                       -4.0966e-02,  1.2601e-02, -4.0154e-02,  4.5808e-03, -2.0622e-02,\n",
       "                       -5.2280e-03,  5.5784e-02,  3.3815e-02,  8.5943e-02,  1.7088e-02,\n",
       "                        5.5248e-02, -2.8142e-02, -3.4241e-02,  1.1802e-02,  8.8216e-02,\n",
       "                       -3.4567e-02, -3.2263e-02],\n",
       "                      [ 3.4306e-02,  2.0291e-02, -2.4333e-02,  7.9260e-02, -7.6601e-02,\n",
       "                       -5.9871e-02, -6.3828e-02,  2.7272e-02, -3.5122e-02,  1.3118e-02,\n",
       "                        1.0581e-01,  3.1422e-02, -3.4793e-02, -2.6677e-02, -1.0355e-02,\n",
       "                       -6.5345e-02,  2.5394e-02,  1.0956e-02, -2.1928e-02, -1.8397e-02,\n",
       "                       -2.7663e-02,  3.1912e-02,  3.0360e-02,  6.3528e-02,  3.5597e-02,\n",
       "                        5.7826e-02, -2.3371e-02, -2.5251e-02,  4.0801e-02,  1.0694e-01,\n",
       "                       -2.5084e-02, -3.1064e-02],\n",
       "                      [ 3.4797e-02,  2.3299e-02, -1.0562e-02,  5.1642e-02, -5.6601e-02,\n",
       "                       -1.8166e-02, -4.0254e-02,  2.9870e-02, -1.3695e-02, -2.6286e-02,\n",
       "                        8.9801e-02,  4.0451e-02, -2.4949e-02, -7.4184e-03, -4.6830e-03,\n",
       "                       -4.5416e-02, -4.6892e-02, -2.9287e-02, -3.1535e-02, -2.6495e-02,\n",
       "                       -7.9775e-03,  3.8136e-02, -1.8677e-02,  8.9336e-02,  3.2237e-02,\n",
       "                        7.7375e-02, -2.2564e-02, -5.1359e-02,  3.1682e-02,  6.2874e-02,\n",
       "                       -4.9458e-02, -3.5751e-02],\n",
       "                      [ 4.7945e-02,  1.3306e-02, -4.2698e-02,  5.6664e-02, -7.4679e-02,\n",
       "                       -1.7444e-02, -4.8386e-02,  3.0383e-02,  4.2101e-04, -2.9670e-02,\n",
       "                        1.3296e-01,  4.4355e-02,  1.5987e-02, -6.1073e-03, -3.3549e-02,\n",
       "                       -4.3584e-02, -4.6292e-02, -2.8373e-02, -2.4093e-02, -2.3722e-02,\n",
       "                       -7.8553e-03,  6.7260e-02, -7.8417e-03,  7.0020e-02,  2.9809e-02,\n",
       "                        6.4749e-02, -7.5147e-03, -4.5481e-02,  2.3243e-02,  8.4878e-02,\n",
       "                       -2.2470e-02, -3.2629e-02]])),\n",
       "             ('module.classifier_vae.classifier.2.bias',\n",
       "              tensor([ 0.0407,  0.0153, -0.0109,  0.0599,  0.0635]))])"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learner.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "ttask = train_tasks.sample()\n",
    "model = learner.clone()\n",
    "evaluation_loss, evaluation_accuracy, reconst_img, query_imgs, mu_l, log_var_l, mu_s, log_var_s = inner_adapt_delpo(\n",
    "                ttask, reconst_loss, model, n_ways, k_shots, q_shots, inner_adapt_steps_train, device, True, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'elbo': tensor(34.0709, grad_fn=<AddBackward0>),\n",
       " 'label_kl': tensor(3.2133, grad_fn=<MeanBackward0>),\n",
       " 'style_kl': tensor(2.9706, grad_fn=<MeanBackward0>),\n",
       " 'reconstruction_loss': tensor(115.3331, grad_fn=<MeanBackward0>),\n",
       " 'classification_loss': tensor(1.6354, grad_fn=<NllLossBackward>)}"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluation_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_loss['elbo'].backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Meta backpropagation of gradients\n",
    "for p in learner.parameters():\n",
    "    p.grad.data.mul_(1.0 / 5)\n",
    "opt.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = [1, evaluation_accuracy.item()]\n",
    "tmp = tmp + [a.item() for a in evaluation_loss.values()]\n",
    "batch_losses.append(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1,\n",
       "  0.20000000298023224,\n",
       "  213.8948516845703,\n",
       "  15.829955101013184,\n",
       "  14.429597854614258,\n",
       "  2242.850830078125,\n",
       "  1.6120678186416626]]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner = torch.load('./model.pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MAML(\n",
       "  (module): CCVAE(\n",
       "    (encoder): CEncoder(\n",
       "      (net): Sequential(\n",
       "        (0): Conv2d(1, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU()\n",
       "        (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "        (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): ReLU()\n",
       "        (6): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "        (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (8): ReLU()\n",
       "        (9): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "        (10): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (11): ReLU()\n",
       "        (12): Flatten(start_dim=1, end_dim=-1)\n",
       "      )\n",
       "      (h1): Linear(in_features=256, out_features=64, bias=True)\n",
       "      (h2): Linear(in_features=256, out_features=64, bias=True)\n",
       "    )\n",
       "    (decoder): CDecoder(\n",
       "      (linear): Sequential(\n",
       "        (0): Linear(in_features=128, out_features=256, bias=True)\n",
       "        (1): ReLU()\n",
       "      )\n",
       "      (net): Sequential(\n",
       "        (0): UpsamplingNearest2d(size=(4, 4), mode=nearest)\n",
       "        (1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
       "        (2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (3): ReLU()\n",
       "        (4): UpsamplingNearest2d(size=(7, 7), mode=nearest)\n",
       "        (5): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
       "        (6): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (7): ReLU()\n",
       "        (8): UpsamplingNearest2d(size=(14, 14), mode=nearest)\n",
       "        (9): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
       "        (10): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (11): ReLU()\n",
       "        (12): UpsamplingNearest2d(size=(28, 28), mode=nearest)\n",
       "        (13): Conv2d(64, 1, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
       "        (14): BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (15): Sigmoid()\n",
       "      )\n",
       "    )\n",
       "    (classifier_vae): Classifier_VAE(\n",
       "      (encoder): CEncoder(\n",
       "        (net): Sequential(\n",
       "          (0): Conv2d(1, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU()\n",
       "          (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "          (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (5): ReLU()\n",
       "          (6): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "          (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (8): ReLU()\n",
       "          (9): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "          (10): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (11): ReLU()\n",
       "          (12): Flatten(start_dim=1, end_dim=-1)\n",
       "        )\n",
       "        (h1): Linear(in_features=256, out_features=64, bias=True)\n",
       "        (h2): Linear(in_features=256, out_features=64, bias=True)\n",
       "      )\n",
       "      (classifier): Sequential(\n",
       "        (0): Linear(in_features=64, out_features=32, bias=True)\n",
       "        (1): ReLU()\n",
       "        (2): Linear(in_features=32, out_features=5, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learner.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt.load_state_dict(torch.load('./opt.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Training ##\n",
    "for iter in range(2):\n",
    "    opt.zero_grad()\n",
    "    batch_losses = []\n",
    "\n",
    "    for batch in range(meta_batch_size):\n",
    "        ttask = train_tasks.sample()\n",
    "        model = learner.clone()\n",
    "        if (batch == 0):\n",
    "            evaluation_loss, evaluation_accuracy, reconst_img, query_imgs, mu_l, log_var_l, mu_s, log_var_s = inner_adapt_delpo(\n",
    "                ttask, reconst_loss, model, n_ways, k_shots, q_shots, inner_adapt_steps_train, device, True, args)\n",
    "            \n",
    "            # Logging train-task images and latents\n",
    "            # di = {\"reconst_examples\": reconst_img.detach().to('cpu'), \"gt_examples\": query_imgs.detach().to('cpu')}\n",
    "            # dl = {\"label_latents\": [mu_l.detach().to('cpu'), log_var_l.detach().to('cpu')],\n",
    "            #       \"style_latents\": [mu_s.detach().to('cpu'), log_var_s.detach().to('cpu')]}\n",
    "            # profiler.log_data(di, iter, 'images', 'train')\n",
    "            # profiler.log_data(dl, iter, 'latents', 'train')\n",
    "\n",
    "        else:\n",
    "            evaluation_loss, evaluation_accuracy = inner_adapt_delpo(\n",
    "                ttask, reconst_loss, model, n_ways, k_shots, q_shots, inner_adapt_steps_train, device, False, args)\n",
    "\n",
    "        evaluation_loss['elbo'].backward()\n",
    "\n",
    "        # Logging per train-task losses and accuracies\n",
    "        # tmp = [(iter*meta_batch_size)+batch, evaluation_accuracy.item()]\n",
    "        # tmp = tmp + [a.item() for a in evaluation_loss.values()]\n",
    "        # batch_losses.append(tmp)\n",
    "\n",
    "    #     wandb.log(dict({f\"train/{key}\": loss.item() for _, (key, loss) in enumerate(evaluation_loss.items())},\n",
    "    #               **{'train/accuracies': evaluation_accuracy.item(), 'train/task': (iter*args.meta_batch_size)+batch}))\n",
    "\n",
    "    # rimages = wandb.Image(reconst_img, caption=\"Reconstructed Query Images\")\n",
    "    # qimages = wandb.Image(query_imgs, caption=\"Query Images\")\n",
    "    # wandb.log({\"reconst_examples\": rimages, \"gt_examples\": qimages})\n",
    "\n",
    "    vtask = valid_tasks.sample()\n",
    "    model = learner.clone()\n",
    "    if iter % 2 == 0:\n",
    "        validation_loss, validation_accuracy, reconst_img, query_imgs, mu_l, log_var_l, mu_s, log_var_s = inner_adapt_delpo(\n",
    "            vtask, reconst_loss, model, n_ways, k_shots, q_shots, inner_adapt_steps_train, device, True, args)\n",
    "        # # Logging valid-task images and latents\n",
    "        # di = {\"reconst_examples\": reconst_img.to('cpu'), \"gt_examples\": query_imgs.to('cpu')}\n",
    "        # dl = {\"label_latents\": [mu_l.to('cpu'), log_var_l.to('cpu')],\n",
    "        #     \"style_latents\": [mu_s.to('cpu'), log_var_s.to('cpu')]}\n",
    "        # profiler.log_data(di, iter, 'images', 'valid')\n",
    "        # profiler.log_data(dl, iter, 'latents', 'valid')\n",
    "\n",
    "    else:\n",
    "        validation_loss, validation_accuracy = inner_adapt_delpo(\n",
    "            vtask, reconst_loss, model, n_ways, k_shots, q_shots, inner_adapt_steps_train, device, False, args)\n",
    "\n",
    "    # Logging per validation-task losses and accuracies\n",
    "    # tmp = [iter, validation_accuracy.item()]\n",
    "    # tmp = tmp + [a.item() for a in validation_loss.values()]\n",
    "\n",
    "    # wandb.log(dict({f\"valid/{key}\": loss.item() for _, (key, loss) in enumerate(validation_loss.items())},\n",
    "    #           **{'valid/accuracies': validation_accuracy.item(), 'valid/task': iter}))\n",
    "# Meta backpropagation of gradients\n",
    "    for p in learner.parameters():\n",
    "        p.grad.data.mul_(1.0 / 5)\n",
    "    opt.step()\n",
    "\n",
    "    # Saving the Logs\n",
    "    # profiler.log_csv(batch_losses, 'train')\n",
    "    # profiler.log_csv(tmp, 'valid')\n",
    "\n",
    "#torch.save(learner, f='../repro')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(learner.to('cpu').state_dict(), './model.pt')\n",
    "torch.save(opt.state_dict(), './opt.pt')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'state': {0: {'step': 2,\n",
       "   'exp_avg': tensor([[[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]]], device='cuda:0'),\n",
       "   'exp_avg_sq': tensor([[[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]]], device='cuda:0')},\n",
       "  1: {'step': 2,\n",
       "   'exp_avg': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "           nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "           nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
       "          device='cuda:0'),\n",
       "   'exp_avg_sq': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "           nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "           nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
       "          device='cuda:0')},\n",
       "  2: {'step': 2,\n",
       "   'exp_avg': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "           nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "           nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
       "          device='cuda:0'),\n",
       "   'exp_avg_sq': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "           nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "           nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
       "          device='cuda:0')},\n",
       "  3: {'step': 2,\n",
       "   'exp_avg': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "           nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "           nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
       "          device='cuda:0'),\n",
       "   'exp_avg_sq': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "           nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "           nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
       "          device='cuda:0')},\n",
       "  4: {'step': 2,\n",
       "   'exp_avg': tensor([[[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            ...,\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            ...,\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            ...,\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           ...,\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            ...,\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            ...,\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            ...,\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]]], device='cuda:0'),\n",
       "   'exp_avg_sq': tensor([[[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            ...,\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            ...,\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            ...,\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           ...,\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            ...,\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            ...,\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            ...,\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]]], device='cuda:0')},\n",
       "  5: {'step': 2,\n",
       "   'exp_avg': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "           nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "           nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
       "          device='cuda:0'),\n",
       "   'exp_avg_sq': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "           nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "           nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
       "          device='cuda:0')},\n",
       "  6: {'step': 2,\n",
       "   'exp_avg': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "           nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "           nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
       "          device='cuda:0'),\n",
       "   'exp_avg_sq': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "           nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "           nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
       "          device='cuda:0')},\n",
       "  7: {'step': 2,\n",
       "   'exp_avg': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "           nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "           nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
       "          device='cuda:0'),\n",
       "   'exp_avg_sq': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "           nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "           nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
       "          device='cuda:0')},\n",
       "  8: {'step': 2,\n",
       "   'exp_avg': tensor([[[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            ...,\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            ...,\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            ...,\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           ...,\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            ...,\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            ...,\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            ...,\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]]], device='cuda:0'),\n",
       "   'exp_avg_sq': tensor([[[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            ...,\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            ...,\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            ...,\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           ...,\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            ...,\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            ...,\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            ...,\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]]], device='cuda:0')},\n",
       "  9: {'step': 2,\n",
       "   'exp_avg': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "           nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "           nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
       "          device='cuda:0'),\n",
       "   'exp_avg_sq': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "           nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "           nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
       "          device='cuda:0')},\n",
       "  10: {'step': 2,\n",
       "   'exp_avg': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "           nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "           nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
       "          device='cuda:0'),\n",
       "   'exp_avg_sq': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "           nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "           nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
       "          device='cuda:0')},\n",
       "  11: {'step': 2,\n",
       "   'exp_avg': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "           nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "           nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
       "          device='cuda:0'),\n",
       "   'exp_avg_sq': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "           nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "           nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
       "          device='cuda:0')},\n",
       "  12: {'step': 2,\n",
       "   'exp_avg': tensor([[[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            ...,\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            ...,\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            ...,\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           ...,\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            ...,\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            ...,\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            ...,\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]]], device='cuda:0'),\n",
       "   'exp_avg_sq': tensor([[[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            ...,\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            ...,\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            ...,\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           ...,\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            ...,\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            ...,\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            ...,\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]]], device='cuda:0')},\n",
       "  13: {'step': 2,\n",
       "   'exp_avg': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "           nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "           nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
       "          device='cuda:0'),\n",
       "   'exp_avg_sq': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "           nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "           nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
       "          device='cuda:0')},\n",
       "  14: {'step': 2,\n",
       "   'exp_avg': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "           nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "           nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
       "          device='cuda:0'),\n",
       "   'exp_avg_sq': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "           nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "           nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
       "          device='cuda:0')},\n",
       "  15: {'step': 2,\n",
       "   'exp_avg': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "           nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "           nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
       "          device='cuda:0'),\n",
       "   'exp_avg_sq': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "           nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "           nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
       "          device='cuda:0')},\n",
       "  16: {'step': 2,\n",
       "   'exp_avg': tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
       "           [nan, nan, nan,  ..., nan, nan, nan],\n",
       "           [nan, nan, nan,  ..., nan, nan, nan],\n",
       "           ...,\n",
       "           [nan, nan, nan,  ..., nan, nan, nan],\n",
       "           [nan, nan, nan,  ..., nan, nan, nan],\n",
       "           [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0'),\n",
       "   'exp_avg_sq': tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
       "           [nan, nan, nan,  ..., nan, nan, nan],\n",
       "           [nan, nan, nan,  ..., nan, nan, nan],\n",
       "           ...,\n",
       "           [nan, nan, nan,  ..., nan, nan, nan],\n",
       "           [nan, nan, nan,  ..., nan, nan, nan],\n",
       "           [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0')},\n",
       "  17: {'step': 2,\n",
       "   'exp_avg': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "           nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "           nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
       "          device='cuda:0'),\n",
       "   'exp_avg_sq': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "           nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "           nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
       "          device='cuda:0')},\n",
       "  18: {'step': 2,\n",
       "   'exp_avg': tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
       "           [nan, nan, nan,  ..., nan, nan, nan],\n",
       "           [nan, nan, nan,  ..., nan, nan, nan],\n",
       "           ...,\n",
       "           [nan, nan, nan,  ..., nan, nan, nan],\n",
       "           [nan, nan, nan,  ..., nan, nan, nan],\n",
       "           [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0'),\n",
       "   'exp_avg_sq': tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
       "           [nan, nan, nan,  ..., nan, nan, nan],\n",
       "           [nan, nan, nan,  ..., nan, nan, nan],\n",
       "           ...,\n",
       "           [nan, nan, nan,  ..., nan, nan, nan],\n",
       "           [nan, nan, nan,  ..., nan, nan, nan],\n",
       "           [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0')},\n",
       "  19: {'step': 2,\n",
       "   'exp_avg': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "           nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "           nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
       "          device='cuda:0'),\n",
       "   'exp_avg_sq': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "           nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "           nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
       "          device='cuda:0')},\n",
       "  20: {'step': 2,\n",
       "   'exp_avg': tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
       "           [nan, nan, nan,  ..., nan, nan, nan],\n",
       "           [nan, nan, nan,  ..., nan, nan, nan],\n",
       "           ...,\n",
       "           [nan, nan, nan,  ..., nan, nan, nan],\n",
       "           [nan, nan, nan,  ..., nan, nan, nan],\n",
       "           [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0'),\n",
       "   'exp_avg_sq': tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
       "           [nan, nan, nan,  ..., nan, nan, nan],\n",
       "           [nan, nan, nan,  ..., nan, nan, nan],\n",
       "           ...,\n",
       "           [nan, nan, nan,  ..., nan, nan, nan],\n",
       "           [nan, nan, nan,  ..., nan, nan, nan],\n",
       "           [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0')},\n",
       "  21: {'step': 2,\n",
       "   'exp_avg': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "           nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "           nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "           nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "           nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "           nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "           nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "           nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "           nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "           nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "           nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
       "          device='cuda:0'),\n",
       "   'exp_avg_sq': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "           nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "           nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "           nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "           nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "           nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "           nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "           nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "           nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "           nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "           nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
       "          device='cuda:0')},\n",
       "  22: {'step': 2,\n",
       "   'exp_avg': tensor([[[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            ...,\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            ...,\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            ...,\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           ...,\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            ...,\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            ...,\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            ...,\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]]], device='cuda:0'),\n",
       "   'exp_avg_sq': tensor([[[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            ...,\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            ...,\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            ...,\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           ...,\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            ...,\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            ...,\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            ...,\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]]], device='cuda:0')},\n",
       "  23: {'step': 2,\n",
       "   'exp_avg': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "           nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "           nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
       "          device='cuda:0'),\n",
       "   'exp_avg_sq': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "           nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "           nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
       "          device='cuda:0')},\n",
       "  24: {'step': 2,\n",
       "   'exp_avg': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "           nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "           nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
       "          device='cuda:0'),\n",
       "   'exp_avg_sq': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "           nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "           nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
       "          device='cuda:0')},\n",
       "  25: {'step': 2,\n",
       "   'exp_avg': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "           nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "           nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
       "          device='cuda:0'),\n",
       "   'exp_avg_sq': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "           nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "           nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
       "          device='cuda:0')},\n",
       "  26: {'step': 2,\n",
       "   'exp_avg': tensor([[[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            ...,\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            ...,\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            ...,\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           ...,\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            ...,\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            ...,\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            ...,\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]]], device='cuda:0'),\n",
       "   'exp_avg_sq': tensor([[[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            ...,\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            ...,\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            ...,\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           ...,\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            ...,\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            ...,\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            ...,\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]]], device='cuda:0')},\n",
       "  27: {'step': 2,\n",
       "   'exp_avg': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "           nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "           nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
       "          device='cuda:0'),\n",
       "   'exp_avg_sq': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "           nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "           nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
       "          device='cuda:0')},\n",
       "  28: {'step': 2,\n",
       "   'exp_avg': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "           nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "           nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
       "          device='cuda:0'),\n",
       "   'exp_avg_sq': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "           nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "           nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
       "          device='cuda:0')},\n",
       "  29: {'step': 2,\n",
       "   'exp_avg': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "           nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "           nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
       "          device='cuda:0'),\n",
       "   'exp_avg_sq': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "           nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "           nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
       "          device='cuda:0')},\n",
       "  30: {'step': 2,\n",
       "   'exp_avg': tensor([[[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            ...,\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            ...,\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            ...,\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           ...,\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            ...,\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            ...,\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            ...,\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]]], device='cuda:0'),\n",
       "   'exp_avg_sq': tensor([[[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            ...,\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            ...,\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            ...,\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           ...,\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            ...,\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            ...,\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            ...,\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]]], device='cuda:0')},\n",
       "  31: {'step': 2,\n",
       "   'exp_avg': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "           nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "           nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
       "          device='cuda:0'),\n",
       "   'exp_avg_sq': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "           nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "           nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
       "          device='cuda:0')},\n",
       "  32: {'step': 2,\n",
       "   'exp_avg': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "           nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "           nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
       "          device='cuda:0'),\n",
       "   'exp_avg_sq': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "           nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "           nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
       "          device='cuda:0')},\n",
       "  33: {'step': 2,\n",
       "   'exp_avg': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "           nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "           nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
       "          device='cuda:0'),\n",
       "   'exp_avg_sq': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "           nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "           nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
       "          device='cuda:0')},\n",
       "  34: {'step': 2,\n",
       "   'exp_avg': tensor([[[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]]], device='cuda:0'),\n",
       "   'exp_avg_sq': tensor([[[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]]], device='cuda:0')},\n",
       "  35: {'step': 2,\n",
       "   'exp_avg': tensor([nan], device='cuda:0'),\n",
       "   'exp_avg_sq': tensor([nan], device='cuda:0')},\n",
       "  36: {'step': 2,\n",
       "   'exp_avg': tensor([nan], device='cuda:0'),\n",
       "   'exp_avg_sq': tensor([nan], device='cuda:0')},\n",
       "  37: {'step': 2,\n",
       "   'exp_avg': tensor([nan], device='cuda:0'),\n",
       "   'exp_avg_sq': tensor([nan], device='cuda:0')},\n",
       "  38: {'step': 2,\n",
       "   'exp_avg': tensor([[[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]]], device='cuda:0'),\n",
       "   'exp_avg_sq': tensor([[[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]]], device='cuda:0')},\n",
       "  39: {'step': 2,\n",
       "   'exp_avg': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "           nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "           nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
       "          device='cuda:0'),\n",
       "   'exp_avg_sq': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "           nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "           nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
       "          device='cuda:0')},\n",
       "  40: {'step': 2,\n",
       "   'exp_avg': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "           nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "           nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
       "          device='cuda:0'),\n",
       "   'exp_avg_sq': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "           nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "           nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
       "          device='cuda:0')},\n",
       "  41: {'step': 2,\n",
       "   'exp_avg': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "           nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "           nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
       "          device='cuda:0'),\n",
       "   'exp_avg_sq': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "           nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "           nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
       "          device='cuda:0')},\n",
       "  42: {'step': 2,\n",
       "   'exp_avg': tensor([[[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            ...,\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            ...,\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            ...,\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           ...,\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            ...,\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            ...,\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            ...,\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]]], device='cuda:0'),\n",
       "   'exp_avg_sq': tensor([[[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            ...,\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            ...,\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            ...,\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           ...,\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            ...,\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            ...,\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            ...,\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]]], device='cuda:0')},\n",
       "  43: {'step': 2,\n",
       "   'exp_avg': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "           nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "           nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
       "          device='cuda:0'),\n",
       "   'exp_avg_sq': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "           nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "           nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
       "          device='cuda:0')},\n",
       "  44: {'step': 2,\n",
       "   'exp_avg': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "           nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "           nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
       "          device='cuda:0'),\n",
       "   'exp_avg_sq': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "           nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "           nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
       "          device='cuda:0')},\n",
       "  45: {'step': 2,\n",
       "   'exp_avg': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "           nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "           nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
       "          device='cuda:0'),\n",
       "   'exp_avg_sq': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "           nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "           nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
       "          device='cuda:0')},\n",
       "  46: {'step': 2,\n",
       "   'exp_avg': tensor([[[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            ...,\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            ...,\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            ...,\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           ...,\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            ...,\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            ...,\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            ...,\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]]], device='cuda:0'),\n",
       "   'exp_avg_sq': tensor([[[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            ...,\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            ...,\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            ...,\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           ...,\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            ...,\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            ...,\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            ...,\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]]], device='cuda:0')},\n",
       "  47: {'step': 2,\n",
       "   'exp_avg': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "           nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "           nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
       "          device='cuda:0'),\n",
       "   'exp_avg_sq': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "           nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "           nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
       "          device='cuda:0')},\n",
       "  48: {'step': 2,\n",
       "   'exp_avg': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "           nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "           nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
       "          device='cuda:0'),\n",
       "   'exp_avg_sq': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "           nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "           nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
       "          device='cuda:0')},\n",
       "  49: {'step': 2,\n",
       "   'exp_avg': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "           nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "           nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
       "          device='cuda:0'),\n",
       "   'exp_avg_sq': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "           nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "           nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
       "          device='cuda:0')},\n",
       "  50: {'step': 2,\n",
       "   'exp_avg': tensor([[[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            ...,\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            ...,\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            ...,\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           ...,\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            ...,\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            ...,\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            ...,\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]]], device='cuda:0'),\n",
       "   'exp_avg_sq': tensor([[[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            ...,\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            ...,\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            ...,\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           ...,\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            ...,\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            ...,\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            ...,\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]]], device='cuda:0')},\n",
       "  51: {'step': 2,\n",
       "   'exp_avg': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "           nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "           nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
       "          device='cuda:0'),\n",
       "   'exp_avg_sq': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "           nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "           nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
       "          device='cuda:0')},\n",
       "  52: {'step': 2,\n",
       "   'exp_avg': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "           nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "           nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
       "          device='cuda:0'),\n",
       "   'exp_avg_sq': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "           nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "           nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
       "          device='cuda:0')},\n",
       "  53: {'step': 2,\n",
       "   'exp_avg': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "           nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "           nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
       "          device='cuda:0'),\n",
       "   'exp_avg_sq': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "           nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "           nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
       "          device='cuda:0')},\n",
       "  54: {'step': 2,\n",
       "   'exp_avg': tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
       "           [nan, nan, nan,  ..., nan, nan, nan],\n",
       "           [nan, nan, nan,  ..., nan, nan, nan],\n",
       "           ...,\n",
       "           [nan, nan, nan,  ..., nan, nan, nan],\n",
       "           [nan, nan, nan,  ..., nan, nan, nan],\n",
       "           [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0'),\n",
       "   'exp_avg_sq': tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
       "           [nan, nan, nan,  ..., nan, nan, nan],\n",
       "           [nan, nan, nan,  ..., nan, nan, nan],\n",
       "           ...,\n",
       "           [nan, nan, nan,  ..., nan, nan, nan],\n",
       "           [nan, nan, nan,  ..., nan, nan, nan],\n",
       "           [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0')},\n",
       "  55: {'step': 2,\n",
       "   'exp_avg': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "           nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "           nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
       "          device='cuda:0'),\n",
       "   'exp_avg_sq': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "           nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "           nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
       "          device='cuda:0')},\n",
       "  56: {'step': 2,\n",
       "   'exp_avg': tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
       "           [nan, nan, nan,  ..., nan, nan, nan],\n",
       "           [nan, nan, nan,  ..., nan, nan, nan],\n",
       "           ...,\n",
       "           [nan, nan, nan,  ..., nan, nan, nan],\n",
       "           [nan, nan, nan,  ..., nan, nan, nan],\n",
       "           [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0'),\n",
       "   'exp_avg_sq': tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
       "           [nan, nan, nan,  ..., nan, nan, nan],\n",
       "           [nan, nan, nan,  ..., nan, nan, nan],\n",
       "           ...,\n",
       "           [nan, nan, nan,  ..., nan, nan, nan],\n",
       "           [nan, nan, nan,  ..., nan, nan, nan],\n",
       "           [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0')},\n",
       "  57: {'step': 2,\n",
       "   'exp_avg': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "           nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "           nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
       "          device='cuda:0'),\n",
       "   'exp_avg_sq': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "           nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "           nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
       "          device='cuda:0')},\n",
       "  58: {'step': 2,\n",
       "   'exp_avg': tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
       "           [nan, nan, nan,  ..., nan, nan, nan],\n",
       "           [nan, nan, nan,  ..., nan, nan, nan],\n",
       "           ...,\n",
       "           [nan, nan, nan,  ..., nan, nan, nan],\n",
       "           [nan, nan, nan,  ..., nan, nan, nan],\n",
       "           [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0'),\n",
       "   'exp_avg_sq': tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
       "           [nan, nan, nan,  ..., nan, nan, nan],\n",
       "           [nan, nan, nan,  ..., nan, nan, nan],\n",
       "           ...,\n",
       "           [nan, nan, nan,  ..., nan, nan, nan],\n",
       "           [nan, nan, nan,  ..., nan, nan, nan],\n",
       "           [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0')},\n",
       "  59: {'step': 2,\n",
       "   'exp_avg': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "           nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0'),\n",
       "   'exp_avg_sq': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "           nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0')},\n",
       "  60: {'step': 2,\n",
       "   'exp_avg': tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "            nan, nan, nan, nan, nan, nan, nan, nan],\n",
       "           [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "            nan, nan, nan, nan, nan, nan, nan, nan],\n",
       "           [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "            nan, nan, nan, nan, nan, nan, nan, nan],\n",
       "           [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "            nan, nan, nan, nan, nan, nan, nan, nan],\n",
       "           [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "            nan, nan, nan, nan, nan, nan, nan, nan]], device='cuda:0'),\n",
       "   'exp_avg_sq': tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "            nan, nan, nan, nan, nan, nan, nan, nan],\n",
       "           [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "            nan, nan, nan, nan, nan, nan, nan, nan],\n",
       "           [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "            nan, nan, nan, nan, nan, nan, nan, nan],\n",
       "           [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "            nan, nan, nan, nan, nan, nan, nan, nan],\n",
       "           [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "            nan, nan, nan, nan, nan, nan, nan, nan]], device='cuda:0')},\n",
       "  61: {'step': 2,\n",
       "   'exp_avg': tensor([nan, nan, nan, nan, nan], device='cuda:0'),\n",
       "   'exp_avg_sq': tensor([nan, nan, nan, nan, nan], device='cuda:0')}},\n",
       " 'param_groups': [{'lr': 0.003,\n",
       "   'betas': (0.9, 0.999),\n",
       "   'eps': 1e-08,\n",
       "   'weight_decay': 0,\n",
       "   'amsgrad': False,\n",
       "   'params': [0,\n",
       "    1,\n",
       "    2,\n",
       "    3,\n",
       "    4,\n",
       "    5,\n",
       "    6,\n",
       "    7,\n",
       "    8,\n",
       "    9,\n",
       "    10,\n",
       "    11,\n",
       "    12,\n",
       "    13,\n",
       "    14,\n",
       "    15,\n",
       "    16,\n",
       "    17,\n",
       "    18,\n",
       "    19,\n",
       "    20,\n",
       "    21,\n",
       "    22,\n",
       "    23,\n",
       "    24,\n",
       "    25,\n",
       "    26,\n",
       "    27,\n",
       "    28,\n",
       "    29,\n",
       "    30,\n",
       "    31,\n",
       "    32,\n",
       "    33,\n",
       "    34,\n",
       "    35,\n",
       "    36,\n",
       "    37,\n",
       "    38,\n",
       "    39,\n",
       "    40,\n",
       "    41,\n",
       "    42,\n",
       "    43,\n",
       "    44,\n",
       "    45,\n",
       "    46,\n",
       "    47,\n",
       "    48,\n",
       "    49,\n",
       "    50,\n",
       "    51,\n",
       "    52,\n",
       "    53,\n",
       "    54,\n",
       "    55,\n",
       "    56,\n",
       "    57,\n",
       "    58,\n",
       "    59,\n",
       "    60,\n",
       "    61]}]}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opt.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'state': {0: {'step': 2,\n",
       "   'exp_avg': tensor([[[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]]], device='cuda:0'),\n",
       "   'exp_avg_sq': tensor([[[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]]], device='cuda:0')},\n",
       "  1: {'step': 2,\n",
       "   'exp_avg': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "           nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "           nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
       "          device='cuda:0'),\n",
       "   'exp_avg_sq': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "           nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "           nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
       "          device='cuda:0')},\n",
       "  2: {'step': 2,\n",
       "   'exp_avg': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "           nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "           nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
       "          device='cuda:0'),\n",
       "   'exp_avg_sq': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "           nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "           nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
       "          device='cuda:0')},\n",
       "  3: {'step': 2,\n",
       "   'exp_avg': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "           nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "           nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
       "          device='cuda:0'),\n",
       "   'exp_avg_sq': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "           nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "           nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
       "          device='cuda:0')},\n",
       "  4: {'step': 2,\n",
       "   'exp_avg': tensor([[[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            ...,\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            ...,\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            ...,\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           ...,\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            ...,\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            ...,\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            ...,\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]]], device='cuda:0'),\n",
       "   'exp_avg_sq': tensor([[[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            ...,\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            ...,\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            ...,\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           ...,\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            ...,\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            ...,\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            ...,\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]]], device='cuda:0')},\n",
       "  5: {'step': 2,\n",
       "   'exp_avg': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "           nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "           nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
       "          device='cuda:0'),\n",
       "   'exp_avg_sq': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "           nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "           nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
       "          device='cuda:0')},\n",
       "  6: {'step': 2,\n",
       "   'exp_avg': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "           nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "           nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
       "          device='cuda:0'),\n",
       "   'exp_avg_sq': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "           nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "           nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
       "          device='cuda:0')},\n",
       "  7: {'step': 2,\n",
       "   'exp_avg': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "           nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "           nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
       "          device='cuda:0'),\n",
       "   'exp_avg_sq': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "           nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "           nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
       "          device='cuda:0')},\n",
       "  8: {'step': 2,\n",
       "   'exp_avg': tensor([[[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            ...,\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            ...,\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            ...,\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           ...,\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            ...,\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            ...,\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            ...,\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]]], device='cuda:0'),\n",
       "   'exp_avg_sq': tensor([[[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            ...,\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            ...,\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            ...,\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           ...,\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            ...,\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            ...,\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            ...,\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]]], device='cuda:0')},\n",
       "  9: {'step': 2,\n",
       "   'exp_avg': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "           nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "           nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
       "          device='cuda:0'),\n",
       "   'exp_avg_sq': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "           nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "           nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
       "          device='cuda:0')},\n",
       "  10: {'step': 2,\n",
       "   'exp_avg': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "           nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "           nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
       "          device='cuda:0'),\n",
       "   'exp_avg_sq': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "           nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "           nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
       "          device='cuda:0')},\n",
       "  11: {'step': 2,\n",
       "   'exp_avg': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "           nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "           nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
       "          device='cuda:0'),\n",
       "   'exp_avg_sq': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "           nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "           nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
       "          device='cuda:0')},\n",
       "  12: {'step': 2,\n",
       "   'exp_avg': tensor([[[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            ...,\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            ...,\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            ...,\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           ...,\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            ...,\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            ...,\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            ...,\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]]], device='cuda:0'),\n",
       "   'exp_avg_sq': tensor([[[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            ...,\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            ...,\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            ...,\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           ...,\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            ...,\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            ...,\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            ...,\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]]], device='cuda:0')},\n",
       "  13: {'step': 2,\n",
       "   'exp_avg': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "           nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "           nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
       "          device='cuda:0'),\n",
       "   'exp_avg_sq': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "           nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "           nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
       "          device='cuda:0')},\n",
       "  14: {'step': 2,\n",
       "   'exp_avg': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "           nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "           nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
       "          device='cuda:0'),\n",
       "   'exp_avg_sq': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "           nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "           nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
       "          device='cuda:0')},\n",
       "  15: {'step': 2,\n",
       "   'exp_avg': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "           nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "           nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
       "          device='cuda:0'),\n",
       "   'exp_avg_sq': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "           nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "           nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
       "          device='cuda:0')},\n",
       "  16: {'step': 2,\n",
       "   'exp_avg': tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
       "           [nan, nan, nan,  ..., nan, nan, nan],\n",
       "           [nan, nan, nan,  ..., nan, nan, nan],\n",
       "           ...,\n",
       "           [nan, nan, nan,  ..., nan, nan, nan],\n",
       "           [nan, nan, nan,  ..., nan, nan, nan],\n",
       "           [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0'),\n",
       "   'exp_avg_sq': tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
       "           [nan, nan, nan,  ..., nan, nan, nan],\n",
       "           [nan, nan, nan,  ..., nan, nan, nan],\n",
       "           ...,\n",
       "           [nan, nan, nan,  ..., nan, nan, nan],\n",
       "           [nan, nan, nan,  ..., nan, nan, nan],\n",
       "           [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0')},\n",
       "  17: {'step': 2,\n",
       "   'exp_avg': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "           nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "           nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
       "          device='cuda:0'),\n",
       "   'exp_avg_sq': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "           nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "           nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
       "          device='cuda:0')},\n",
       "  18: {'step': 2,\n",
       "   'exp_avg': tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
       "           [nan, nan, nan,  ..., nan, nan, nan],\n",
       "           [nan, nan, nan,  ..., nan, nan, nan],\n",
       "           ...,\n",
       "           [nan, nan, nan,  ..., nan, nan, nan],\n",
       "           [nan, nan, nan,  ..., nan, nan, nan],\n",
       "           [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0'),\n",
       "   'exp_avg_sq': tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
       "           [nan, nan, nan,  ..., nan, nan, nan],\n",
       "           [nan, nan, nan,  ..., nan, nan, nan],\n",
       "           ...,\n",
       "           [nan, nan, nan,  ..., nan, nan, nan],\n",
       "           [nan, nan, nan,  ..., nan, nan, nan],\n",
       "           [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0')},\n",
       "  19: {'step': 2,\n",
       "   'exp_avg': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "           nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "           nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
       "          device='cuda:0'),\n",
       "   'exp_avg_sq': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "           nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "           nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
       "          device='cuda:0')},\n",
       "  20: {'step': 2,\n",
       "   'exp_avg': tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
       "           [nan, nan, nan,  ..., nan, nan, nan],\n",
       "           [nan, nan, nan,  ..., nan, nan, nan],\n",
       "           ...,\n",
       "           [nan, nan, nan,  ..., nan, nan, nan],\n",
       "           [nan, nan, nan,  ..., nan, nan, nan],\n",
       "           [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0'),\n",
       "   'exp_avg_sq': tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
       "           [nan, nan, nan,  ..., nan, nan, nan],\n",
       "           [nan, nan, nan,  ..., nan, nan, nan],\n",
       "           ...,\n",
       "           [nan, nan, nan,  ..., nan, nan, nan],\n",
       "           [nan, nan, nan,  ..., nan, nan, nan],\n",
       "           [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0')},\n",
       "  21: {'step': 2,\n",
       "   'exp_avg': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "           nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "           nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "           nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "           nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "           nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "           nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "           nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "           nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "           nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "           nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
       "          device='cuda:0'),\n",
       "   'exp_avg_sq': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "           nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "           nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "           nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "           nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "           nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "           nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "           nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "           nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "           nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "           nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
       "          device='cuda:0')},\n",
       "  22: {'step': 2,\n",
       "   'exp_avg': tensor([[[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            ...,\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            ...,\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            ...,\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           ...,\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            ...,\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            ...,\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            ...,\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]]], device='cuda:0'),\n",
       "   'exp_avg_sq': tensor([[[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            ...,\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            ...,\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            ...,\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           ...,\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            ...,\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            ...,\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            ...,\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]]], device='cuda:0')},\n",
       "  23: {'step': 2,\n",
       "   'exp_avg': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "           nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "           nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
       "          device='cuda:0'),\n",
       "   'exp_avg_sq': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "           nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "           nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
       "          device='cuda:0')},\n",
       "  24: {'step': 2,\n",
       "   'exp_avg': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "           nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "           nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
       "          device='cuda:0'),\n",
       "   'exp_avg_sq': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "           nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "           nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
       "          device='cuda:0')},\n",
       "  25: {'step': 2,\n",
       "   'exp_avg': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "           nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "           nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
       "          device='cuda:0'),\n",
       "   'exp_avg_sq': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "           nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "           nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
       "          device='cuda:0')},\n",
       "  26: {'step': 2,\n",
       "   'exp_avg': tensor([[[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            ...,\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            ...,\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            ...,\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           ...,\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            ...,\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            ...,\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            ...,\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]]], device='cuda:0'),\n",
       "   'exp_avg_sq': tensor([[[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            ...,\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            ...,\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            ...,\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           ...,\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            ...,\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            ...,\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            ...,\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]]], device='cuda:0')},\n",
       "  27: {'step': 2,\n",
       "   'exp_avg': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "           nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "           nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
       "          device='cuda:0'),\n",
       "   'exp_avg_sq': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "           nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "           nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
       "          device='cuda:0')},\n",
       "  28: {'step': 2,\n",
       "   'exp_avg': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "           nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "           nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
       "          device='cuda:0'),\n",
       "   'exp_avg_sq': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "           nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "           nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
       "          device='cuda:0')},\n",
       "  29: {'step': 2,\n",
       "   'exp_avg': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "           nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "           nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
       "          device='cuda:0'),\n",
       "   'exp_avg_sq': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "           nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "           nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
       "          device='cuda:0')},\n",
       "  30: {'step': 2,\n",
       "   'exp_avg': tensor([[[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            ...,\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            ...,\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            ...,\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           ...,\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            ...,\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            ...,\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            ...,\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]]], device='cuda:0'),\n",
       "   'exp_avg_sq': tensor([[[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            ...,\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            ...,\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            ...,\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           ...,\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            ...,\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            ...,\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            ...,\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]]], device='cuda:0')},\n",
       "  31: {'step': 2,\n",
       "   'exp_avg': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "           nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "           nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
       "          device='cuda:0'),\n",
       "   'exp_avg_sq': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "           nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "           nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
       "          device='cuda:0')},\n",
       "  32: {'step': 2,\n",
       "   'exp_avg': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "           nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "           nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
       "          device='cuda:0'),\n",
       "   'exp_avg_sq': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "           nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "           nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
       "          device='cuda:0')},\n",
       "  33: {'step': 2,\n",
       "   'exp_avg': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "           nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "           nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
       "          device='cuda:0'),\n",
       "   'exp_avg_sq': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "           nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "           nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
       "          device='cuda:0')},\n",
       "  34: {'step': 2,\n",
       "   'exp_avg': tensor([[[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]]], device='cuda:0'),\n",
       "   'exp_avg_sq': tensor([[[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]]], device='cuda:0')},\n",
       "  35: {'step': 2,\n",
       "   'exp_avg': tensor([nan], device='cuda:0'),\n",
       "   'exp_avg_sq': tensor([nan], device='cuda:0')},\n",
       "  36: {'step': 2,\n",
       "   'exp_avg': tensor([nan], device='cuda:0'),\n",
       "   'exp_avg_sq': tensor([nan], device='cuda:0')},\n",
       "  37: {'step': 2,\n",
       "   'exp_avg': tensor([nan], device='cuda:0'),\n",
       "   'exp_avg_sq': tensor([nan], device='cuda:0')},\n",
       "  38: {'step': 2,\n",
       "   'exp_avg': tensor([[[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]]], device='cuda:0'),\n",
       "   'exp_avg_sq': tensor([[[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]]], device='cuda:0')},\n",
       "  39: {'step': 2,\n",
       "   'exp_avg': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "           nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "           nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
       "          device='cuda:0'),\n",
       "   'exp_avg_sq': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "           nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "           nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
       "          device='cuda:0')},\n",
       "  40: {'step': 2,\n",
       "   'exp_avg': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "           nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "           nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
       "          device='cuda:0'),\n",
       "   'exp_avg_sq': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "           nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "           nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
       "          device='cuda:0')},\n",
       "  41: {'step': 2,\n",
       "   'exp_avg': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "           nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "           nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
       "          device='cuda:0'),\n",
       "   'exp_avg_sq': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "           nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "           nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
       "          device='cuda:0')},\n",
       "  42: {'step': 2,\n",
       "   'exp_avg': tensor([[[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            ...,\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            ...,\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            ...,\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           ...,\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            ...,\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            ...,\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            ...,\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]]], device='cuda:0'),\n",
       "   'exp_avg_sq': tensor([[[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            ...,\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            ...,\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            ...,\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           ...,\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            ...,\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            ...,\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            ...,\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]]], device='cuda:0')},\n",
       "  43: {'step': 2,\n",
       "   'exp_avg': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "           nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "           nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
       "          device='cuda:0'),\n",
       "   'exp_avg_sq': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "           nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "           nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
       "          device='cuda:0')},\n",
       "  44: {'step': 2,\n",
       "   'exp_avg': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "           nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "           nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
       "          device='cuda:0'),\n",
       "   'exp_avg_sq': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "           nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "           nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
       "          device='cuda:0')},\n",
       "  45: {'step': 2,\n",
       "   'exp_avg': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "           nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "           nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
       "          device='cuda:0'),\n",
       "   'exp_avg_sq': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "           nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "           nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
       "          device='cuda:0')},\n",
       "  46: {'step': 2,\n",
       "   'exp_avg': tensor([[[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            ...,\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            ...,\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            ...,\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           ...,\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            ...,\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            ...,\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            ...,\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]]], device='cuda:0'),\n",
       "   'exp_avg_sq': tensor([[[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            ...,\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            ...,\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            ...,\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           ...,\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            ...,\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            ...,\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            ...,\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]]], device='cuda:0')},\n",
       "  47: {'step': 2,\n",
       "   'exp_avg': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "           nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "           nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
       "          device='cuda:0'),\n",
       "   'exp_avg_sq': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "           nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "           nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
       "          device='cuda:0')},\n",
       "  48: {'step': 2,\n",
       "   'exp_avg': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "           nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "           nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
       "          device='cuda:0'),\n",
       "   'exp_avg_sq': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "           nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "           nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
       "          device='cuda:0')},\n",
       "  49: {'step': 2,\n",
       "   'exp_avg': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "           nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "           nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
       "          device='cuda:0'),\n",
       "   'exp_avg_sq': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "           nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "           nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
       "          device='cuda:0')},\n",
       "  50: {'step': 2,\n",
       "   'exp_avg': tensor([[[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            ...,\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            ...,\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            ...,\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           ...,\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            ...,\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            ...,\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            ...,\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]]], device='cuda:0'),\n",
       "   'exp_avg_sq': tensor([[[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            ...,\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            ...,\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            ...,\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           ...,\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            ...,\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            ...,\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]],\n",
       "   \n",
       "   \n",
       "           [[[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            ...,\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]],\n",
       "   \n",
       "            [[nan, nan, nan],\n",
       "             [nan, nan, nan],\n",
       "             [nan, nan, nan]]]], device='cuda:0')},\n",
       "  51: {'step': 2,\n",
       "   'exp_avg': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "           nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "           nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
       "          device='cuda:0'),\n",
       "   'exp_avg_sq': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "           nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "           nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
       "          device='cuda:0')},\n",
       "  52: {'step': 2,\n",
       "   'exp_avg': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "           nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "           nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
       "          device='cuda:0'),\n",
       "   'exp_avg_sq': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "           nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "           nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
       "          device='cuda:0')},\n",
       "  53: {'step': 2,\n",
       "   'exp_avg': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "           nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "           nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
       "          device='cuda:0'),\n",
       "   'exp_avg_sq': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "           nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "           nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
       "          device='cuda:0')},\n",
       "  54: {'step': 2,\n",
       "   'exp_avg': tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
       "           [nan, nan, nan,  ..., nan, nan, nan],\n",
       "           [nan, nan, nan,  ..., nan, nan, nan],\n",
       "           ...,\n",
       "           [nan, nan, nan,  ..., nan, nan, nan],\n",
       "           [nan, nan, nan,  ..., nan, nan, nan],\n",
       "           [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0'),\n",
       "   'exp_avg_sq': tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
       "           [nan, nan, nan,  ..., nan, nan, nan],\n",
       "           [nan, nan, nan,  ..., nan, nan, nan],\n",
       "           ...,\n",
       "           [nan, nan, nan,  ..., nan, nan, nan],\n",
       "           [nan, nan, nan,  ..., nan, nan, nan],\n",
       "           [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0')},\n",
       "  55: {'step': 2,\n",
       "   'exp_avg': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "           nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "           nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
       "          device='cuda:0'),\n",
       "   'exp_avg_sq': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "           nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "           nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
       "          device='cuda:0')},\n",
       "  56: {'step': 2,\n",
       "   'exp_avg': tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
       "           [nan, nan, nan,  ..., nan, nan, nan],\n",
       "           [nan, nan, nan,  ..., nan, nan, nan],\n",
       "           ...,\n",
       "           [nan, nan, nan,  ..., nan, nan, nan],\n",
       "           [nan, nan, nan,  ..., nan, nan, nan],\n",
       "           [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0'),\n",
       "   'exp_avg_sq': tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
       "           [nan, nan, nan,  ..., nan, nan, nan],\n",
       "           [nan, nan, nan,  ..., nan, nan, nan],\n",
       "           ...,\n",
       "           [nan, nan, nan,  ..., nan, nan, nan],\n",
       "           [nan, nan, nan,  ..., nan, nan, nan],\n",
       "           [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0')},\n",
       "  57: {'step': 2,\n",
       "   'exp_avg': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "           nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "           nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
       "          device='cuda:0'),\n",
       "   'exp_avg_sq': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "           nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "           nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
       "          device='cuda:0')},\n",
       "  58: {'step': 2,\n",
       "   'exp_avg': tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
       "           [nan, nan, nan,  ..., nan, nan, nan],\n",
       "           [nan, nan, nan,  ..., nan, nan, nan],\n",
       "           ...,\n",
       "           [nan, nan, nan,  ..., nan, nan, nan],\n",
       "           [nan, nan, nan,  ..., nan, nan, nan],\n",
       "           [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0'),\n",
       "   'exp_avg_sq': tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
       "           [nan, nan, nan,  ..., nan, nan, nan],\n",
       "           [nan, nan, nan,  ..., nan, nan, nan],\n",
       "           ...,\n",
       "           [nan, nan, nan,  ..., nan, nan, nan],\n",
       "           [nan, nan, nan,  ..., nan, nan, nan],\n",
       "           [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0')},\n",
       "  59: {'step': 2,\n",
       "   'exp_avg': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "           nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0'),\n",
       "   'exp_avg_sq': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "           nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0')},\n",
       "  60: {'step': 2,\n",
       "   'exp_avg': tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "            nan, nan, nan, nan, nan, nan, nan, nan],\n",
       "           [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "            nan, nan, nan, nan, nan, nan, nan, nan],\n",
       "           [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "            nan, nan, nan, nan, nan, nan, nan, nan],\n",
       "           [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "            nan, nan, nan, nan, nan, nan, nan, nan],\n",
       "           [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "            nan, nan, nan, nan, nan, nan, nan, nan]], device='cuda:0'),\n",
       "   'exp_avg_sq': tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "            nan, nan, nan, nan, nan, nan, nan, nan],\n",
       "           [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "            nan, nan, nan, nan, nan, nan, nan, nan],\n",
       "           [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "            nan, nan, nan, nan, nan, nan, nan, nan],\n",
       "           [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "            nan, nan, nan, nan, nan, nan, nan, nan],\n",
       "           [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "            nan, nan, nan, nan, nan, nan, nan, nan]], device='cuda:0')},\n",
       "  61: {'step': 2,\n",
       "   'exp_avg': tensor([nan, nan, nan, nan, nan], device='cuda:0'),\n",
       "   'exp_avg_sq': tensor([nan, nan, nan, nan, nan], device='cuda:0')}},\n",
       " 'param_groups': [{'lr': 0.003,\n",
       "   'betas': (0.9, 0.999),\n",
       "   'eps': 1e-08,\n",
       "   'weight_decay': 0,\n",
       "   'amsgrad': False,\n",
       "   'params': [0,\n",
       "    1,\n",
       "    2,\n",
       "    3,\n",
       "    4,\n",
       "    5,\n",
       "    6,\n",
       "    7,\n",
       "    8,\n",
       "    9,\n",
       "    10,\n",
       "    11,\n",
       "    12,\n",
       "    13,\n",
       "    14,\n",
       "    15,\n",
       "    16,\n",
       "    17,\n",
       "    18,\n",
       "    19,\n",
       "    20,\n",
       "    21,\n",
       "    22,\n",
       "    23,\n",
       "    24,\n",
       "    25,\n",
       "    26,\n",
       "    27,\n",
       "    28,\n",
       "    29,\n",
       "    30,\n",
       "    31,\n",
       "    32,\n",
       "    33,\n",
       "    34,\n",
       "    35,\n",
       "    36,\n",
       "    37,\n",
       "    38,\n",
       "    39,\n",
       "    40,\n",
       "    41,\n",
       "    42,\n",
       "    43,\n",
       "    44,\n",
       "    45,\n",
       "    46,\n",
       "    47,\n",
       "    48,\n",
       "    49,\n",
       "    50,\n",
       "    51,\n",
       "    52,\n",
       "    53,\n",
       "    54,\n",
       "    55,\n",
       "    56,\n",
       "    57,\n",
       "    58,\n",
       "    59,\n",
       "    60,\n",
       "    61]}]}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opt.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 64])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_var_l.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hi\n",
      "hi\n",
      "hi\n"
     ]
    }
   ],
   "source": [
    "for i in range(2,5):\n",
    "    print('hi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[[[179., 164., 166.,  ..., 219., 225., 229.],\n",
       "           [162., 171., 162.,  ..., 212., 220., 218.],\n",
       "           [154., 151., 161.,  ..., 216., 215., 215.],\n",
       "           ...,\n",
       "           [255., 255., 255.,  ..., 255., 255., 255.],\n",
       "           [255., 255., 255.,  ..., 255., 255., 255.],\n",
       "           [255., 255., 255.,  ..., 255., 255., 255.]],\n",
       " \n",
       "          [[171., 160., 163.,  ..., 223., 232., 233.],\n",
       "           [156., 166., 158.,  ..., 212., 230., 229.],\n",
       "           [149., 144., 154.,  ..., 216., 227., 227.],\n",
       "           ...,\n",
       "           [255., 255., 255.,  ..., 255., 255., 255.],\n",
       "           [255., 255., 255.,  ..., 255., 255., 255.],\n",
       "           [255., 255., 255.,  ..., 255., 255., 255.]],\n",
       " \n",
       "          [[157., 142., 155.,  ..., 235., 239., 243.],\n",
       "           [137., 147., 146.,  ..., 220., 235., 238.],\n",
       "           [128., 126., 135.,  ..., 225., 233., 238.],\n",
       "           ...,\n",
       "           [255., 255., 255.,  ..., 255., 255., 255.],\n",
       "           [255., 255., 255.,  ..., 255., 255., 255.],\n",
       "           [255., 255., 255.,  ..., 255., 255., 255.]]],\n",
       " \n",
       " \n",
       "         [[[ 59.,  70.,  53.,  ...,  44.,  14.,  38.],\n",
       "           [ 58.,  78.,  60.,  ...,   4.,  10.,  44.],\n",
       "           [ 48.,  56.,  73.,  ...,  41.,  11.,  24.],\n",
       "           ...,\n",
       "           [197., 153., 200.,  ..., 212., 162., 175.],\n",
       "           [217., 238., 228.,  ..., 165., 187., 202.],\n",
       "           [190., 183., 206.,  ..., 225., 185., 204.]],\n",
       " \n",
       "          [[ 69.,  70.,  52.,  ...,  71.,  18.,  30.],\n",
       "           [ 53.,  71.,  60.,  ...,  19.,   9.,  76.],\n",
       "           [ 47.,  57.,  78.,  ...,  58.,  16.,  40.],\n",
       "           ...,\n",
       "           [175., 132., 180.,  ..., 190., 137., 151.],\n",
       "           [190., 215., 212.,  ..., 138., 164., 185.],\n",
       "           [148., 151., 175.,  ..., 195., 164., 184.]],\n",
       " \n",
       "          [[ 55.,  66.,  60.,  ...,  30.,   4.,  44.],\n",
       "           [ 55.,  85.,  62.,  ...,  13.,  20.,  46.],\n",
       "           [ 49.,  46.,  84.,  ...,  42.,   8.,  32.],\n",
       "           ...,\n",
       "           [152., 101., 149.,  ..., 153., 100., 127.],\n",
       "           [164., 183., 192.,  ..., 109., 126., 149.],\n",
       "           [117., 116., 146.,  ..., 165., 126., 147.]]],\n",
       " \n",
       " \n",
       "         [[[187., 198., 208.,  ..., 182., 182., 183.],\n",
       "           [165., 174., 183.,  ..., 208., 209., 211.],\n",
       "           [144., 150., 157.,  ..., 247., 247., 248.],\n",
       "           ...,\n",
       "           [ 41.,  74.,  93.,  ...,  63., 221.,  64.],\n",
       "           [ 65., 249., 254.,  ..., 129., 149., 115.],\n",
       "           [ 78.,  66., 148.,  ..., 234., 117., 109.]],\n",
       " \n",
       "          [[182., 193., 202.,  ..., 177., 177., 178.],\n",
       "           [160., 169., 178.,  ..., 204., 205., 207.],\n",
       "           [141., 148., 154.,  ..., 241., 242., 243.],\n",
       "           ...,\n",
       "           [ 32.,  75.,  88.,  ...,  58., 208.,  52.],\n",
       "           [ 59., 249., 251.,  ..., 104., 135., 102.],\n",
       "           [ 77.,  45., 107.,  ..., 169.,  72.,  78.]],\n",
       " \n",
       "          [[160., 168., 178.,  ..., 157., 157., 157.],\n",
       "           [141., 147., 156.,  ..., 179., 178., 180.],\n",
       "           [124., 129., 135.,  ..., 212., 212., 213.],\n",
       "           ...,\n",
       "           [ 20.,  73.,  86.,  ...,  47., 166.,  39.],\n",
       "           [ 52., 246., 234.,  ...,  74., 109.,  78.],\n",
       "           [ 67.,  21.,  50.,  ...,  76.,  42.,  51.]]],\n",
       " \n",
       " \n",
       "         [[[225., 200., 208.,  ..., 181., 158., 143.],\n",
       "           [201., 187., 193.,  ..., 176., 156., 167.],\n",
       "           [242., 202., 193.,  ..., 162., 145., 139.],\n",
       "           ...,\n",
       "           [ 70.,  78.,  57.,  ...,  37.,  43.,  57.],\n",
       "           [ 61.,  70.,  52.,  ...,  37.,  43.,  49.],\n",
       "           [ 50.,  55.,  43.,  ...,  36.,  42.,  50.]],\n",
       " \n",
       "          [[169., 144., 150.,  ..., 120.,  99.,  89.],\n",
       "           [150., 133., 134.,  ..., 116.,  99., 116.],\n",
       "           [198., 151., 136.,  ..., 103.,  89.,  88.],\n",
       "           ...,\n",
       "           [ 62.,  56.,  49.,  ...,  49.,  49.,  55.],\n",
       "           [ 62.,  56.,  52.,  ...,  56.,  55.,  53.],\n",
       "           [ 58.,  49.,  51.,  ...,  59.,  57.,  59.]],\n",
       " \n",
       "          [[154., 113., 100.,  ...,  89.,  97., 108.],\n",
       "           [120.,  99.,  94.,  ...,  96., 103., 136.],\n",
       "           [148., 111., 106.,  ...,  96.,  99., 108.],\n",
       "           ...,\n",
       "           [ 97.,  97.,  88.,  ...,  93.,  88.,  90.],\n",
       "           [ 96.,  96.,  90.,  ...,  99.,  93.,  87.],\n",
       "           [ 91.,  89.,  88.,  ..., 103.,  97.,  94.]]]]),\n",
       " tensor([0, 0, 1, 1])]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_tasks.sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(learner, './model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner = torch.load('./model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner = learner.to('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[[0.5460, 0.7005, 0.7540,  ..., 0.6607, 0.5247, 0.6521],\n",
       "           [0.3869, 0.4778, 0.6000,  ..., 0.5082, 0.4487, 0.5776],\n",
       "           [0.5202, 0.7316, 0.7623,  ..., 0.5487, 0.4707, 0.5494],\n",
       "           ...,\n",
       "           [0.4386, 0.4401, 0.5506,  ..., 0.5411, 0.4927, 0.5433],\n",
       "           [0.4770, 0.3676, 0.4371,  ..., 0.3584, 0.3767, 0.4090],\n",
       "           [0.4787, 0.3858, 0.3679,  ..., 0.3389, 0.3394, 0.4149]],\n",
       " \n",
       "          [[0.4559, 0.4518, 0.4888,  ..., 0.7013, 0.6391, 0.3140],\n",
       "           [0.3996, 0.3678, 0.3866,  ..., 0.4096, 0.3197, 0.2149],\n",
       "           [0.4501, 0.2640, 0.2461,  ..., 0.2523, 0.1991, 0.1818],\n",
       "           ...,\n",
       "           [0.2929, 0.2676, 0.3173,  ..., 0.3396, 0.2282, 0.2554],\n",
       "           [0.3211, 0.2309, 0.2781,  ..., 0.4156, 0.2826, 0.2474],\n",
       "           [0.5293, 0.4962, 0.4777,  ..., 0.4851, 0.3834, 0.4208]],\n",
       " \n",
       "          [[0.7325, 0.5827, 0.5901,  ..., 0.7413, 0.6871, 0.7642],\n",
       "           [0.7580, 0.6519, 0.6883,  ..., 0.7370, 0.6829, 0.7204],\n",
       "           [0.7148, 0.4977, 0.5030,  ..., 0.7617, 0.6439, 0.5647],\n",
       "           ...,\n",
       "           [0.5718, 0.6577, 0.6703,  ..., 0.7768, 0.7952, 0.5863],\n",
       "           [0.5294, 0.6871, 0.6785,  ..., 0.6754, 0.7238, 0.6346],\n",
       "           [0.5640, 0.7525, 0.7336,  ..., 0.6574, 0.7076, 0.5791]]],\n",
       " \n",
       " \n",
       "         [[[0.4616, 0.4751, 0.6149,  ..., 0.5819, 0.4910, 0.5046],\n",
       "           [0.3783, 0.3729, 0.6283,  ..., 0.4075, 0.3890, 0.4244],\n",
       "           [0.2629, 0.3951, 0.6489,  ..., 0.3567, 0.3669, 0.3906],\n",
       "           ...,\n",
       "           [0.6776, 0.4958, 0.5216,  ..., 0.3829, 0.2843, 0.4355],\n",
       "           [0.6580, 0.3964, 0.3533,  ..., 0.3265, 0.2400, 0.3619],\n",
       "           [0.6106, 0.4477, 0.3370,  ..., 0.4344, 0.3043, 0.4539]],\n",
       " \n",
       "          [[0.4963, 0.6822, 0.6672,  ..., 0.5410, 0.4595, 0.3248],\n",
       "           [0.4093, 0.5696, 0.5441,  ..., 0.3824, 0.2312, 0.2577],\n",
       "           [0.4676, 0.5143, 0.4876,  ..., 0.2634, 0.1832, 0.2344],\n",
       "           ...,\n",
       "           [0.3796, 0.3746, 0.5096,  ..., 0.4192, 0.4114, 0.1637],\n",
       "           [0.2958, 0.3367, 0.4196,  ..., 0.3622, 0.3033, 0.1223],\n",
       "           [0.4633, 0.4559, 0.3886,  ..., 0.5714, 0.4509, 0.3555]],\n",
       " \n",
       "          [[0.6180, 0.5263, 0.6095,  ..., 0.7014, 0.6635, 0.6198],\n",
       "           [0.4234, 0.4492, 0.4586,  ..., 0.7301, 0.7368, 0.6853],\n",
       "           [0.4660, 0.4895, 0.4902,  ..., 0.7127, 0.6777, 0.5532],\n",
       "           ...,\n",
       "           [0.5009, 0.7045, 0.7995,  ..., 0.8717, 0.8806, 0.6905],\n",
       "           [0.4762, 0.6092, 0.6864,  ..., 0.7795, 0.7901, 0.6817],\n",
       "           [0.5316, 0.6010, 0.5450,  ..., 0.6916, 0.7390, 0.5898]]],\n",
       " \n",
       " \n",
       "         [[[0.4821, 0.5130, 0.5116,  ..., 0.6959, 0.5411, 0.5840],\n",
       "           [0.3825, 0.4224, 0.4856,  ..., 0.3638, 0.2365, 0.4427],\n",
       "           [0.3230, 0.5178, 0.5562,  ..., 0.2722, 0.1928, 0.3298],\n",
       "           ...,\n",
       "           [0.6793, 0.5820, 0.6569,  ..., 0.4021, 0.3972, 0.5089],\n",
       "           [0.6982, 0.4548, 0.4322,  ..., 0.2665, 0.2831, 0.4367],\n",
       "           [0.6291, 0.4972, 0.3638,  ..., 0.2979, 0.3020, 0.4428]],\n",
       " \n",
       "          [[0.5016, 0.5930, 0.5854,  ..., 0.6940, 0.5011, 0.2647],\n",
       "           [0.3702, 0.3991, 0.4451,  ..., 0.4448, 0.1458, 0.1108],\n",
       "           [0.3722, 0.3645, 0.4353,  ..., 0.3254, 0.1176, 0.0875],\n",
       "           ...,\n",
       "           [0.3851, 0.3307, 0.3950,  ..., 0.2707, 0.2286, 0.1893],\n",
       "           [0.3800, 0.3102, 0.3615,  ..., 0.2419, 0.1901, 0.1986],\n",
       "           [0.5376, 0.4944, 0.4409,  ..., 0.4452, 0.3750, 0.3521]],\n",
       " \n",
       "          [[0.5991, 0.4032, 0.3658,  ..., 0.8058, 0.7404, 0.7043],\n",
       "           [0.4025, 0.2679, 0.2317,  ..., 0.8030, 0.7794, 0.7700],\n",
       "           [0.4281, 0.2630, 0.2011,  ..., 0.8196, 0.6379, 0.5528],\n",
       "           ...,\n",
       "           [0.6831, 0.7812, 0.8230,  ..., 0.6421, 0.5840, 0.6470],\n",
       "           [0.6608, 0.7372, 0.7894,  ..., 0.6218, 0.6237, 0.6682],\n",
       "           [0.6913, 0.7762, 0.7650,  ..., 0.7149, 0.7117, 0.5803]]],\n",
       " \n",
       " \n",
       "         [[[0.5324, 0.5766, 0.6837,  ..., 0.6866, 0.6010, 0.7734],\n",
       "           [0.5194, 0.5345, 0.6862,  ..., 0.3670, 0.3845, 0.6499],\n",
       "           [0.4675, 0.6238, 0.7319,  ..., 0.3814, 0.3636, 0.5255],\n",
       "           ...,\n",
       "           [0.4486, 0.3721, 0.3139,  ..., 0.4006, 0.3422, 0.4420],\n",
       "           [0.4468, 0.2509, 0.2056,  ..., 0.3399, 0.2936, 0.4019],\n",
       "           [0.4766, 0.3563, 0.3380,  ..., 0.2876, 0.2746, 0.4041]],\n",
       " \n",
       "          [[0.5370, 0.6930, 0.6878,  ..., 0.6488, 0.5223, 0.2760],\n",
       "           [0.3615, 0.5174, 0.5597,  ..., 0.3380, 0.2640, 0.1145],\n",
       "           [0.3924, 0.5177, 0.5579,  ..., 0.1972, 0.1681, 0.0917],\n",
       "           ...,\n",
       "           [0.2539, 0.1568, 0.1638,  ..., 0.5084, 0.3242, 0.2768],\n",
       "           [0.2639, 0.1984, 0.2445,  ..., 0.5070, 0.3276, 0.2448],\n",
       "           [0.4651, 0.4393, 0.4071,  ..., 0.4805, 0.3468, 0.3638]],\n",
       " \n",
       "          [[0.6992, 0.5983, 0.5682,  ..., 0.8602, 0.8287, 0.7744],\n",
       "           [0.4479, 0.4213, 0.5190,  ..., 0.8024, 0.8361, 0.7427],\n",
       "           [0.4832, 0.4036, 0.4341,  ..., 0.8041, 0.7452, 0.5439],\n",
       "           ...,\n",
       "           [0.5775, 0.4673, 0.3731,  ..., 0.7253, 0.7617, 0.6082],\n",
       "           [0.5735, 0.5454, 0.4641,  ..., 0.6716, 0.6974, 0.6302],\n",
       "           [0.5665, 0.7579, 0.7315,  ..., 0.6474, 0.6677, 0.6242]]]],\n",
       "        grad_fn=<SigmoidBackward>),\n",
       " tensor([[ 0.2482,  0.6516],\n",
       "         [-0.1493,  0.3071],\n",
       "         [-0.2627,  0.7280],\n",
       "         [-0.1374,  0.1252]], grad_fn=<AddmmBackward>),\n",
       " tensor([[-1.0427e+00,  1.5486e+00,  3.7712e-01, -1.2363e+00, -6.6550e-01,\n",
       "          -6.2212e-01, -3.7823e-01, -2.4082e-01,  2.6278e-01, -8.9451e-01,\n",
       "          -1.2854e+00,  2.4860e-02, -5.5511e-01, -5.9627e-01, -6.5324e-03,\n",
       "           4.9552e-01, -4.2039e-01,  4.7758e-01,  4.5570e-01, -2.2097e-01,\n",
       "          -1.8639e-01,  6.1043e-01, -2.8873e-01,  4.6831e-01,  9.6868e-01,\n",
       "          -4.2490e-01, -4.6423e-01, -3.7781e-01,  5.8020e-02,  7.5179e-01,\n",
       "          -3.0532e-01,  4.8438e-01, -2.4328e-01,  1.0959e+00, -1.1996e+00,\n",
       "           6.3191e-01,  1.0069e+00,  2.4784e-01, -2.5084e-01,  1.2540e+00,\n",
       "          -3.9792e-02,  1.9532e-01,  1.1072e+00, -1.0880e-01,  2.4570e-01,\n",
       "          -6.3982e-01, -4.0435e-01,  8.6478e-01,  3.4675e-01, -4.6406e-01,\n",
       "           9.9928e-01,  4.3913e-01, -2.1397e-01,  5.5439e-01,  5.2898e-03,\n",
       "          -3.8093e-01,  1.5761e+00, -4.2980e-01, -8.3359e-02,  1.3419e-01,\n",
       "           1.2379e+00,  7.1658e-01,  1.5237e+00, -2.0429e-01],\n",
       "         [-9.8657e-01,  6.5907e-02,  4.4691e-01, -4.1792e-01,  2.3034e-01,\n",
       "          -6.2552e-01, -2.6754e-01, -4.5925e-01, -4.7429e-01, -5.7079e-01,\n",
       "          -1.2990e+00, -8.2958e-01, -8.6491e-02,  4.1192e-02,  5.1518e-01,\n",
       "           4.6955e-01, -3.6358e-01,  5.7182e-01,  1.4987e-01, -5.5069e-01,\n",
       "          -4.7041e-01,  3.4059e-01, -8.2564e-01,  2.4578e-01,  4.4072e-01,\n",
       "          -4.0165e-01, -1.0650e+00, -1.0647e+00,  6.7946e-01,  3.6562e-01,\n",
       "          -5.0909e-01,  5.3694e-01,  3.0152e-02,  6.3253e-02, -6.3895e-01,\n",
       "           7.6491e-01,  8.6761e-01,  4.5662e-01, -3.1709e-01,  5.0872e-01,\n",
       "           4.4868e-01,  9.8669e-02,  5.3296e-02, -6.0436e-01,  1.8684e-01,\n",
       "           2.2029e-01, -2.6696e-01,  1.9326e-01,  5.2352e-01,  6.3960e-01,\n",
       "           6.8786e-01,  1.8437e-01, -1.5243e-01,  5.9653e-01, -2.9414e-01,\n",
       "          -1.1424e+00,  9.7313e-01,  7.4257e-01, -8.0285e-01, -1.2369e-01,\n",
       "           1.5063e+00,  4.6705e-01,  8.5877e-01,  2.9591e-02],\n",
       "         [-6.2944e-01,  1.0690e+00,  1.1066e+00, -6.3273e-01,  1.8194e-01,\n",
       "          -3.9785e-01, -1.3444e+00, -8.3931e-01, -1.3893e+00, -1.1164e+00,\n",
       "          -1.6616e+00, -3.2131e-01, -6.4468e-01,  1.9330e-01,  9.0179e-02,\n",
       "           4.3819e-01, -6.1879e-01,  9.9477e-02,  3.2215e-02, -3.5475e-01,\n",
       "           1.8786e-01,  3.1329e-01, -1.6302e+00,  8.9239e-04,  7.8012e-01,\n",
       "          -9.8014e-01, -3.7182e-01, -3.4477e-01,  3.8969e-01,  4.6394e-01,\n",
       "          -3.4223e-01,  1.5741e-01,  8.1654e-01,  6.8169e-02, -1.9686e+00,\n",
       "           1.7471e+00,  1.1710e+00,  9.1011e-01,  3.8966e-01,  6.8218e-01,\n",
       "          -4.9394e-01, -7.1415e-01, -3.1851e-01, -1.2008e+00,  3.9927e-01,\n",
       "          -1.0891e+00, -7.3526e-01,  4.2310e-02,  9.4372e-01,  3.6476e-01,\n",
       "           1.4336e+00,  5.8398e-01, -2.4610e-02,  5.8569e-01,  9.3578e-02,\n",
       "          -6.1558e-01,  1.8143e+00, -2.9612e-01, -9.2693e-01,  1.1896e-01,\n",
       "           8.7674e-01, -2.7073e-01,  1.3244e+00,  9.5615e-01],\n",
       "         [-5.4136e-01,  3.2179e-01, -2.0281e-01, -2.0729e-01,  7.7492e-01,\n",
       "          -4.8828e-01, -3.5585e-01, -7.8893e-01, -1.6989e-01,  4.1251e-01,\n",
       "          -1.2901e+00, -4.9137e-01, -1.5146e-01,  1.8907e-01,  3.2065e-01,\n",
       "           1.9190e-01, -4.0879e-01,  3.1080e-01,  5.6968e-01, -4.8568e-01,\n",
       "          -1.7223e-01,  4.5407e-01, -3.2437e-01,  6.9592e-01,  4.2789e-01,\n",
       "          -4.2553e-01, -1.2073e+00, -1.0008e+00,  2.4737e-01,  4.8880e-02,\n",
       "           2.3815e-01,  7.4388e-01,  5.0519e-02, -5.6881e-02, -5.8152e-01,\n",
       "           3.6477e-01,  3.6029e-01,  3.5744e-01,  2.5598e-01,  3.9801e-01,\n",
       "           3.6650e-01,  2.2633e-01, -1.3265e-01, -1.1012e+00, -5.4078e-02,\n",
       "          -4.0193e-01, -5.2718e-01,  2.1821e-01,  7.3594e-01,  1.1589e+00,\n",
       "           3.5502e-01,  1.0436e+00,  3.9480e-01,  2.0411e-01,  8.9128e-02,\n",
       "          -6.1074e-01,  1.1933e+00,  7.5323e-01, -1.1211e+00,  6.3046e-02,\n",
       "           8.1822e-01,  5.9995e-01,  7.0758e-01, -4.2292e-01]],\n",
       "        grad_fn=<AddmmBackward>),\n",
       " tensor([[ 1.0647e-01,  1.5160e-01, -8.3628e-01, -2.1236e-01,  4.0951e-01,\n",
       "          -8.2976e-03, -3.4569e-01, -2.8417e-01, -4.7929e-01, -9.1334e-02,\n",
       "           4.1839e-01,  5.2088e-02, -1.7993e-01, -9.1891e-01,  3.5410e-02,\n",
       "           1.1948e-01,  9.7303e-01,  8.5831e-01, -1.0251e+00,  5.9573e-01,\n",
       "           3.6959e-01, -1.0120e+00,  7.4889e-01, -1.4848e+00, -4.0915e-01,\n",
       "           2.0774e-01, -2.4134e-02, -5.4017e-01, -2.9369e-01,  6.4744e-01,\n",
       "          -3.9573e-02,  3.2934e-01,  1.3344e-01, -1.7343e+00, -8.7552e-01,\n",
       "           8.1901e-01,  5.0429e-01,  4.7399e-01,  7.8999e-02,  4.8247e-01,\n",
       "           3.6501e-02,  3.7212e-01,  2.6029e-01, -8.3030e-02,  7.6731e-01,\n",
       "           1.5467e-01, -2.8189e-02,  1.8249e-01, -9.5448e-01, -2.4625e-01,\n",
       "          -4.0618e-01,  4.4419e-01,  9.9215e-01, -1.1120e+00, -2.0976e-02,\n",
       "          -1.5486e+00, -5.4511e-01,  1.5949e-01, -2.2132e-01,  3.7674e-02,\n",
       "           5.8980e-01, -5.0577e-01, -2.3003e-01, -9.8616e-01],\n",
       "         [ 2.2594e-01,  9.1214e-02, -4.4889e-02,  2.6438e-01,  1.8255e-01,\n",
       "           5.4208e-01, -1.3170e+00,  2.4087e-01,  1.0744e-01,  5.7052e-01,\n",
       "          -6.2096e-01,  1.8366e-01,  2.1383e-02, -8.0070e-01, -9.0154e-01,\n",
       "           1.1484e+00, -4.7308e-04,  1.2388e-01, -1.7311e-01, -8.8101e-03,\n",
       "           6.8999e-01, -4.2612e-01,  5.3558e-02, -1.6288e+00, -4.9598e-01,\n",
       "          -1.6475e-01, -4.8892e-02,  8.6772e-01, -5.6789e-01,  7.4009e-01,\n",
       "           1.2902e-01,  2.9499e-01, -3.2384e-01, -1.2955e+00, -6.6631e-01,\n",
       "           7.6000e-01, -8.2658e-02,  1.1554e+00,  6.8370e-01,  4.1997e-01,\n",
       "           1.2241e+00,  5.7718e-02,  5.8048e-01,  4.1887e-01,  4.5641e-01,\n",
       "          -1.7261e-01,  3.3833e-02, -1.3159e-01,  4.4846e-01, -5.3291e-01,\n",
       "          -3.8264e-01, -6.1419e-01,  9.6336e-02, -1.0548e+00,  5.3689e-01,\n",
       "          -1.4449e+00, -2.4938e-01,  6.7456e-01, -1.8049e-01,  2.0218e-01,\n",
       "           8.8946e-01, -1.6735e-01,  1.6849e-01,  6.1521e-01],\n",
       "         [ 2.6278e-01, -1.2206e-01,  1.8801e-01, -1.7964e-01,  4.0418e-01,\n",
       "          -4.9484e-01, -1.3293e+00,  8.3286e-02,  1.1726e+00,  2.0232e-01,\n",
       "          -2.7197e-01,  1.0621e+00,  1.5052e-01, -4.1656e-01,  9.0055e-02,\n",
       "           1.1208e+00,  7.9445e-01, -7.5440e-01, -1.5575e+00,  2.6832e-01,\n",
       "           1.1385e+00, -1.2621e+00,  4.4023e-01, -1.3598e+00, -1.2914e+00,\n",
       "           3.6100e-02, -7.5382e-01, -7.8916e-01,  2.9603e-01,  3.2361e-01,\n",
       "          -5.2163e-01,  2.6033e-01,  5.4026e-01, -1.4803e+00, -1.7555e+00,\n",
       "           8.3029e-01,  1.1177e+00,  4.9671e-01,  6.7320e-02,  9.8979e-01,\n",
       "          -6.7979e-02,  4.9277e-01,  3.0429e-03, -3.9393e-01,  6.2244e-01,\n",
       "           2.6136e-01, -4.2779e-01,  4.2325e-01,  1.1316e-01,  6.4757e-01,\n",
       "           3.9240e-01, -1.9224e-01,  4.8660e-01, -3.4058e-01, -1.0142e+00,\n",
       "          -5.8040e-01, -1.9005e-01, -5.4209e-01,  1.8410e-01,  1.2577e+00,\n",
       "           2.2352e+00, -5.4805e-01,  3.1239e-02, -8.6078e-01],\n",
       "         [-2.4549e-02, -3.3800e-01, -6.2523e-01,  1.1619e-01,  6.0409e-02,\n",
       "          -4.5932e-02, -5.3214e-01, -2.3429e-01,  2.0314e-01,  9.1364e-01,\n",
       "           1.4193e-01,  6.4621e-01, -7.3222e-01, -1.4316e+00, -1.5061e+00,\n",
       "           1.0817e+00,  3.0009e-01, -2.5054e-01,  1.2299e-01, -1.7100e-01,\n",
       "           3.4559e-01, -5.4650e-01, -8.6927e-02, -1.1887e+00, -1.9013e-01,\n",
       "           4.7733e-01,  1.7835e-01, -1.3032e-01, -4.4656e-01,  8.9288e-01,\n",
       "           6.8568e-01,  4.1050e-01, -6.4770e-02, -8.0258e-01, -1.2411e+00,\n",
       "           5.3010e-01,  1.9888e-01,  1.3725e+00,  4.2433e-01, -1.0141e-01,\n",
       "           5.4158e-01,  4.1288e-01,  5.9506e-01,  4.0014e-01,  3.0908e-01,\n",
       "           4.0854e-01, -6.8268e-02,  3.1139e-01,  6.3791e-01,  2.6370e-01,\n",
       "          -2.1406e-01, -1.9961e-01,  9.1837e-01, -1.2372e+00,  6.2316e-01,\n",
       "          -2.1850e-01, -3.5326e-01, -2.1721e-01, -5.9372e-01, -4.3825e-01,\n",
       "           9.5516e-01, -1.3608e-01,  6.5201e-01,  2.1145e-01]],\n",
       "        grad_fn=<AddmmBackward>),\n",
       " tensor([[ 3.7053e-01,  2.5323e-01, -7.3182e-01, -5.2788e-02, -1.1694e+00,\n",
       "          -7.6351e-01,  1.2535e+00, -2.8564e-01,  6.0491e-01, -2.3497e-01,\n",
       "           1.2060e+00,  4.7478e-01, -1.1254e+00,  8.0270e-01, -5.3399e-01,\n",
       "           2.4886e-01, -7.5631e-02, -8.7801e-01,  1.0650e+00, -9.5949e-01,\n",
       "          -1.4514e-01, -5.6843e-01,  6.5403e-01,  4.6875e-01, -1.5072e-01,\n",
       "          -2.6392e-01,  1.7090e-01,  6.6966e-01,  1.3442e-01, -5.3155e-01,\n",
       "          -1.2191e+00,  3.6736e-01,  3.9964e-01, -1.0296e+00, -1.6832e-02,\n",
       "          -5.6743e-01, -6.3973e-01, -8.0634e-02,  7.5467e-01,  1.4131e-01,\n",
       "           1.3297e+00, -1.8395e-01,  1.3485e-01,  5.8679e-01,  2.1811e-01,\n",
       "          -1.1531e-01,  1.1403e+00,  4.0352e-01, -4.8781e-01,  6.4588e-02,\n",
       "          -1.1730e+00,  6.8874e-01,  1.2159e+00, -5.4939e-01, -6.5562e-02,\n",
       "          -9.0735e-01, -3.3527e-01, -1.0406e+00, -7.0800e-01, -5.0842e-01,\n",
       "           4.1842e-01, -4.7375e-01,  7.1508e-02,  9.2959e-01],\n",
       "         [-1.5734e-01,  3.1082e-01, -5.4274e-01, -3.1558e-01,  6.3181e-02,\n",
       "          -3.0111e-02,  7.5616e-01, -3.3697e-01, -1.1732e-01, -2.6432e-02,\n",
       "           1.2178e-01,  5.7784e-03, -8.3572e-01,  4.4569e-01,  1.8992e-01,\n",
       "          -6.6022e-01, -3.2841e-01,  5.6726e-01,  7.1613e-01, -2.4800e-01,\n",
       "           4.4499e-01,  1.6994e-01,  2.4291e-01, -1.4463e-01, -6.0495e-01,\n",
       "           1.6416e-01,  3.8119e-01,  3.3503e-01, -5.7171e-03, -6.9619e-01,\n",
       "          -1.1760e+00,  4.1902e-01, -3.5177e-01,  1.2620e-01, -6.8741e-01,\n",
       "          -2.5846e-01,  1.2788e-01,  2.7059e-01, -4.6714e-02,  5.2342e-01,\n",
       "           8.3007e-01, -3.0649e-01,  1.0173e-01, -1.3733e-01,  4.0654e-01,\n",
       "           2.5516e-01,  2.7182e-01,  1.4526e+00, -3.8004e-01, -2.2514e-01,\n",
       "          -4.1715e-01,  1.0473e+00,  6.7590e-01,  2.9076e-01, -1.1863e+00,\n",
       "          -3.9941e-01, -1.0850e-01, -6.9598e-01, -5.3516e-01, -4.0775e-01,\n",
       "           6.9863e-02, -4.6485e-01,  8.3368e-01,  2.2382e-01],\n",
       "         [-5.1922e-01,  4.1087e-01, -9.6461e-01, -5.9268e-01, -1.0284e+00,\n",
       "           2.3808e-01,  2.0495e+00, -1.4990e+00,  9.7257e-01, -2.9252e-01,\n",
       "           1.8368e+00,  5.8737e-01, -5.3770e-01,  1.2123e+00, -9.3911e-03,\n",
       "          -6.9361e-01,  5.5304e-01, -2.7794e-01,  1.1953e+00, -2.1703e-01,\n",
       "           6.7457e-01, -5.6216e-02,  1.8309e-01,  3.6462e-01, -4.4976e-01,\n",
       "           5.4797e-01,  5.1741e-01,  2.5764e-01,  7.3841e-01, -2.9921e-01,\n",
       "          -1.7697e+00, -7.3590e-01, -6.8614e-01, -1.2330e+00, -4.9751e-01,\n",
       "          -2.9507e-01,  3.2606e-01,  3.1927e-01,  7.0089e-01, -5.7360e-03,\n",
       "           1.7714e+00, -6.0358e-01,  5.5823e-01,  4.7251e-01,  1.5523e+00,\n",
       "          -7.5727e-01,  1.4299e+00,  1.2045e+00,  7.2614e-01,  6.8322e-01,\n",
       "          -1.8802e+00,  1.0930e+00,  1.6481e+00, -4.7537e-01, -5.0388e-02,\n",
       "           1.8217e-01,  1.0421e+00, -8.1628e-01, -7.1592e-01, -9.2220e-01,\n",
       "           1.5931e+00, -1.0506e+00,  1.3581e-01,  1.8441e+00],\n",
       "         [-2.4381e-02,  7.0483e-01,  1.4827e-01,  2.4925e-02, -7.3998e-01,\n",
       "           5.9874e-01,  5.0960e-01, -2.3579e-01,  6.0291e-01,  1.2111e-01,\n",
       "           7.6643e-02, -6.9330e-02, -7.5927e-01,  5.1824e-01,  2.8755e-01,\n",
       "          -3.0019e-01, -3.9849e-02,  1.5034e-01,  4.7361e-01, -1.7478e-01,\n",
       "           2.3705e-01, -1.1238e-01,  3.4294e-01,  8.1528e-02, -6.8438e-01,\n",
       "          -6.8447e-02,  1.1703e-01,  6.1868e-02, -2.2731e-02, -4.5235e-01,\n",
       "          -1.0830e+00,  8.7829e-01, -2.1039e-01, -6.9001e-01,  2.1306e-01,\n",
       "           2.2783e-01,  2.0686e-01,  3.5820e-01,  1.4995e-01,  1.7772e-01,\n",
       "           1.4089e+00, -2.7253e-01,  3.8260e-01,  6.6075e-01,  5.6650e-01,\n",
       "          -3.9343e-01,  6.9858e-01,  6.0604e-01, -1.2405e-01, -4.9387e-01,\n",
       "          -1.2994e-01,  7.2545e-01,  2.2484e-01, -8.1440e-03, -4.9963e-01,\n",
       "           1.2066e-01, -1.7595e-01, -9.5018e-01, -2.0810e-01, -2.2851e-01,\n",
       "           4.5142e-01, -4.6140e-04,  6.3524e-01,  7.9341e-01]],\n",
       "        grad_fn=<AddmmBackward>),\n",
       " tensor([[ 8.7720e-01, -7.3727e-02, -5.0232e-01,  3.0900e-01,  4.0468e-01,\n",
       "           5.4463e-01,  7.2638e-02,  7.7138e-01, -1.1961e+00,  1.5552e+00,\n",
       "          -3.6341e-01,  8.3519e-01,  2.8246e-01,  6.4696e-01,  1.5454e+00,\n",
       "          -9.5145e-01,  4.6653e-01, -7.2505e-01,  8.0158e-02,  4.3186e-01,\n",
       "          -3.1520e-01, -1.8177e-01,  1.9857e-01,  6.2527e-01,  3.4276e-01,\n",
       "          -9.8300e-02, -1.2308e+00,  8.3307e-01,  2.8638e-01, -8.8110e-01,\n",
       "          -9.7370e-01,  2.3265e-01, -6.8499e-01, -2.8683e-01,  5.1744e-01,\n",
       "          -2.4636e-01, -4.0778e-01,  5.1478e-01,  1.3204e+00, -4.5983e-01,\n",
       "           1.2103e-01, -6.5724e-01, -6.3515e-01,  1.1760e-01, -1.2333e-01,\n",
       "          -5.5347e-01, -2.9888e-01, -5.8363e-02, -5.6267e-01,  1.0650e+00,\n",
       "           9.0831e-01,  5.6055e-02, -1.1609e-03,  2.6192e-01,  7.0959e-01,\n",
       "           1.7023e+00, -5.0380e-01,  4.6282e-01,  1.6097e+00, -9.8628e-01,\n",
       "          -5.3334e-01,  9.4544e-01, -1.8362e+00,  2.2186e-02],\n",
       "         [ 1.6855e-01, -3.0567e-01,  3.6838e-02,  8.1812e-01,  1.9319e-01,\n",
       "          -1.7373e-01, -9.1905e-03,  2.5303e-01, -4.7856e-01,  1.6104e+00,\n",
       "          -8.0445e-01, -5.5849e-01,  8.9761e-01, -1.0167e-01,  2.8165e-01,\n",
       "          -2.5356e-01,  4.7617e-01, -2.9719e-02,  1.1721e-01, -1.8875e-02,\n",
       "          -1.2856e-01, -6.4632e-01, -3.9122e-01,  1.7721e-01,  9.3885e-01,\n",
       "          -4.0102e-02, -4.2388e-01,  5.6234e-01, -1.6162e-03, -2.8170e-01,\n",
       "          -6.6763e-02,  2.5123e-02,  4.7450e-01, -5.6276e-01,  2.3394e-01,\n",
       "          -8.1048e-01, -3.2569e-01,  3.9438e-01,  3.8914e-01, -4.1720e-01,\n",
       "           1.2684e+00, -2.7415e-01, -1.4549e-01, -3.8214e-01,  7.3684e-01,\n",
       "          -5.7975e-01, -4.2511e-01,  2.5474e-01, -1.0713e+00,  1.1930e+00,\n",
       "           1.1582e+00, -9.4816e-02, -2.6202e-02,  1.6757e-01,  1.2598e+00,\n",
       "           1.4109e+00, -1.1432e-01,  3.8467e-01,  7.7641e-01, -4.8648e-01,\n",
       "          -1.5776e+00,  1.4569e+00, -1.1875e+00, -4.1869e-01],\n",
       "         [ 8.0037e-01, -4.7408e-02,  2.8104e-01,  1.1188e+00,  8.3446e-01,\n",
       "           2.5940e-01,  9.7620e-01,  5.7337e-01, -6.6962e-01,  1.1429e+00,\n",
       "          -2.4446e-01,  9.5140e-02, -1.4171e-02,  1.2323e-01,  1.0760e+00,\n",
       "          -2.1667e-01,  1.6873e-02, -1.1056e+00,  1.0878e-01, -4.6615e-01,\n",
       "           5.1775e-02, -1.8814e-01, -8.1502e-01,  1.2892e+00,  6.8598e-01,\n",
       "          -6.9399e-01, -8.2422e-01,  1.2827e+00, -1.7202e-01, -9.0652e-01,\n",
       "          -5.1168e-01,  2.3676e-02, -5.5461e-01,  1.8561e-01,  1.2486e+00,\n",
       "          -1.3564e+00, -1.0683e+00, -1.8048e-01, -6.9369e-01, -1.1669e+00,\n",
       "           3.0837e-01, -7.8252e-01, -5.0157e-01,  5.8557e-01,  3.7268e-01,\n",
       "           1.0810e-01, -7.0992e-01, -2.0019e-01, -8.4665e-01,  3.9503e-01,\n",
       "           1.3127e+00,  7.7821e-01, -3.2453e-01, -3.2963e-01,  5.7984e-02,\n",
       "           5.0094e-01, -6.5673e-01,  3.3295e-01,  2.0624e+00, -2.1337e-01,\n",
       "          -8.8102e-01,  2.1141e+00, -7.8712e-01, -4.1314e-01],\n",
       "         [ 3.9509e-01,  2.4326e-02,  2.7136e-01,  3.4218e-01,  4.2949e-01,\n",
       "          -2.6020e-01,  2.6019e-02,  1.4206e-01, -7.8709e-01,  1.7605e+00,\n",
       "          -9.7210e-01,  5.0901e-02,  8.7334e-01,  1.1667e-01,  3.9888e-01,\n",
       "           5.3240e-01, -9.8200e-02,  3.7250e-01,  1.5071e-01,  2.3104e-02,\n",
       "          -5.3659e-02, -4.7223e-01, -9.3308e-02,  3.6033e-01,  2.8768e-01,\n",
       "          -7.8007e-02, -2.0820e-01,  1.1453e+00,  8.3367e-03, -6.5847e-01,\n",
       "          -3.4837e-01,  8.0362e-01,  4.8605e-01, -1.8015e-01, -9.6335e-02,\n",
       "          -7.6877e-01, -7.0230e-01, -3.2610e-01,  1.5643e-01, -2.0373e-01,\n",
       "           3.4380e-01, -2.3354e-01,  1.1826e-01, -8.3317e-02,  2.6571e-01,\n",
       "          -9.4326e-02, -6.7173e-01,  7.0114e-01, -1.0516e+00,  8.6428e-01,\n",
       "           9.2522e-01, -2.6216e-01,  1.5300e-01, -4.0996e-01,  8.0283e-01,\n",
       "           1.2263e+00, -4.7724e-01,  9.0756e-01,  9.5900e-01, -2.9031e-01,\n",
       "          -1.2714e+00,  8.0949e-01, -9.3442e-01, -8.9507e-02]],\n",
       "        grad_fn=<AddmmBackward>))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learner(vtask[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing on held out classes\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_273348/2229067109.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlearner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;31m#tetask = test_tasks.sample()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     evaluation_loss, evaluation_accuracy = inner_adapt_delpo(\n\u001b[0m\u001b[1;32m     11\u001b[0m         tetask, reconst_loss, model, n_ways, k_shots, q_shots, 5, device, False, args)\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/Work/TU_Delft/research/implement/learning_to_meta-learn/src/zoo/delpo_utils.py\u001b[0m in \u001b[0;36minner_adapt_delpo\u001b[0;34m(task, reconst_loss, learner, n_ways, k_shots, q_shots, adapt_steps, device, log_data, args)\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0;31m# Inner adapt step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0madapt_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m         reconst_image, logits, mu_l, log_var_l, mu_s, log_var_s = learner(\n\u001b[0m\u001b[1;32m     97\u001b[0m             support)\n\u001b[1;32m     98\u001b[0m         adapt_loss = loss(reconst_loss, reconst_image, support,\n",
      "\u001b[0;32m~/anaconda3/envs/torch/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/torch/lib/python3.8/site-packages/learn2learn/algorithms/maml.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m     def adapt(self,\n",
      "\u001b[0;32m~/anaconda3/envs/torch/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/Work/TU_Delft/research/implement/learning_to_meta-learn/src/zoo/archs.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    639\u001b[0m         \u001b[0mmu_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_var_s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    640\u001b[0m         \u001b[0mz_s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreparameterize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmu_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_var_s\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 641\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mz_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmu_l\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    642\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmu_l\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_var_l\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmu_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_var_s\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    643\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/torch/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/Work/TU_Delft/research/implement/learning_to_meta-learn/src/zoo/archs.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    535\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    536\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 537\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    538\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    539\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/torch/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/torch/lib/python3.8/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    137\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/torch/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/torch/lib/python3.8/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    441\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 443\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    445\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/torch/lib/python3.8/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    437\u001b[0m                             \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    438\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[0;32m--> 439\u001b[0;31m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[0m\u001b[1;32m    440\u001b[0m                         self.padding, self.dilation, self.groups)\n\u001b[1;32m    441\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "## Testing ##\n",
    "print('Testing on held out classes')\n",
    "\n",
    "for i, tetask in enumerate(test_tasks):\n",
    "    # wandb.define_metric(\"accuracies\", summary=\"max\")\n",
    "    # wandb.define_metric(\"accuracies\", summary=\"mean\")\n",
    "\n",
    "    model = learner.clone()\n",
    "    #tetask = test_tasks.sample()\n",
    "    evaluation_loss, evaluation_accuracy = inner_adapt_delpo(\n",
    "        tetask, reconst_loss, model, n_ways, k_shots, q_shots, 5, device, False, args)\n",
    "\n",
    "    # Logging per test-task losses and accuracies\n",
    "    # tmp = [iter, evaluation_accuracy.item()]\n",
    "    # tmp.append(a.item() for a in evaluation_loss.values())\n",
    "    # profiler.log_csv(tmp, 'test')\n",
    "    # # wandb.log(dict({f\"test/{key}\": loss.item() for _, (key, loss) in enumerate(evaluation_loss.items())},\n",
    "    #             **{'test/accuracies': evaluation_accuracy.item(), 'test/task': i}))\n"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "pytorch-gpu.1-8.m71",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-8:m71"
  },
  "interpreter": {
   "hash": "7e3af266dcb7df8b026f0780dbb396b062ee5ca2767a18f50e60e26ee6084121"
  },
  "kernelspec": {
   "display_name": "Python 3.8.11 64-bit ('torch': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
